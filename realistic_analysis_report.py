#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í˜„ì‹¤ì ì¸ í¬ì»¤ ì‹œì¥ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±ê¸°
- ì‹¤ì œ ë°ì´í„° ìƒí™©ì„ ë°˜ì˜í•œ ë¶„ì„
- ë°ì´í„° í•œê³„ ëª…ì‹œ ë° í˜„ì‹¤ì  ì¸ì‚¬ì´íŠ¸ ì œê³µ
"""
import sys
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')

import json
import logging
import sqlite3
from datetime import datetime, timedelta
from collections import defaultdict, Counter
import statistics

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RealisticAnalysisReportGenerator:
    def __init__(self, db_path='poker_insight.db'):
        self.db_path = db_path
        
    def get_db_connection(self):
        """SQLite ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
        return sqlite3.connect(self.db_path)
        
    def generate_realistic_report(self):
        """í˜„ì‹¤ì ì¸ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        logger.info("ğŸ“Š í˜„ì‹¤ì ì¸ í¬ì»¤ ì‹œì¥ ë¶„ì„ ì‹œì‘...")
        
        report_data = {
            'report_metadata': self.get_report_metadata(),
            'data_quality_assessment': self.assess_data_quality(),
            'current_market_snapshot': self.analyze_current_snapshot(),
            'comparative_analysis': self.perform_comparative_analysis(),
            'news_based_insights': self.extract_news_insights(),
            'actionable_recommendations': self.generate_realistic_recommendations(),
            'data_limitations': self.document_data_limitations()
        }
        
        return report_data
        
    def get_report_metadata(self):
        """ë¦¬í¬íŠ¸ ë©”íƒ€ë°ì´í„°"""
        return {
            'generated_at': datetime.now().isoformat(),
            'report_type': 'realistic_market_analysis',
            'data_collection_date': datetime.now().strftime('%Y-%m-%d'),
            'analysis_approach': 'single_snapshot_with_limitations',
            'reliability_level': 'current_state_accurate_trends_limited'
        }
        
    def assess_data_quality(self):
        """ë°ì´í„° í’ˆì§ˆ í‰ê°€"""
        logger.info("  ğŸ” ë°ì´í„° í’ˆì§ˆ í‰ê°€...")
        
        try:
            conn = self.get_db_connection()
            cursor = conn.cursor()
            
            # ì „ì²´ ì‚¬ì´íŠ¸ ë°ì´í„° í’ˆì§ˆ ì²´í¬
            query = """
            SELECT 
                ps.name,
                td.total_players,
                td.seven_day_average,
                CASE WHEN td.seven_day_average > 0 THEN 1 ELSE 0 END as has_historical_data
            FROM poker_sites ps
            JOIN traffic_data td ON ps.id = td.site_id
            WHERE td.total_players > 0
            ORDER BY td.total_players DESC
            """
            
            cursor.execute(query)
            results = cursor.fetchall()
            
            total_sites = len(results)
            sites_with_historical = sum(row[3] for row in results)
            sites_without_historical = total_sites - sites_with_historical
            
            # íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„°ê°€ ìˆëŠ” ì‚¬ì´íŠ¸ë“¤ì˜ í˜„ì‹¤ì„± ì²´í¬
            realistic_data = []
            questionable_data = []
            
            for row in results:
                name, current, avg_7day, has_historical = row
                if has_historical:
                    if avg_7day > 0:
                        growth_rate = ((current - avg_7day) / avg_7day) * 100
                        # í˜„ì‹¤ì ì¸ ì„±ì¥ë¥  ë²”ìœ„ ì²´í¬ (ì¼ì£¼ì¼ì— Â±50% ì´ë‚´)
                        if abs(growth_rate) <= 50:
                            realistic_data.append((name, current, avg_7day, growth_rate))
                        else:
                            questionable_data.append((name, current, avg_7day, growth_rate))
                            
            quality_assessment = {
                'total_active_sites': total_sites,
                'sites_with_historical_data': sites_with_historical,
                'sites_without_historical_data': sites_without_historical,
                'historical_data_coverage': round((sites_with_historical / total_sites) * 100, 1),
                'realistic_trend_data': len(realistic_data),
                'questionable_trend_data': len(questionable_data),
                'data_reliability': self.calculate_data_reliability(sites_with_historical, total_sites, len(realistic_data)),
                'realistic_sites': realistic_data,
                'questionable_sites': questionable_data
            }
            
            conn.close()
            return quality_assessment
            
        except Exception as e:
            logger.error(f"ë°ì´í„° í’ˆì§ˆ í‰ê°€ ì˜¤ë¥˜: {str(e)}")
            return {}
            
    def calculate_data_reliability(self, sites_with_historical, total_sites, realistic_count):
        """ë°ì´í„° ì‹ ë¢°ë„ ê³„ì‚°"""
        if total_sites == 0:
            return "UNKNOWN"
            
        historical_ratio = sites_with_historical / total_sites
        
        if historical_ratio >= 0.8 and realistic_count >= sites_with_historical * 0.8:
            return "HIGH"
        elif historical_ratio >= 0.5 and realistic_count >= sites_with_historical * 0.6:
            return "MEDIUM"
        elif historical_ratio >= 0.2:
            return "LOW"
        else:
            return "VERY_LOW"
            
    def analyze_current_snapshot(self):
        """í˜„ì¬ ì‹œì  ìŠ¤ëƒ…ìƒ· ë¶„ì„"""
        logger.info("  ğŸ“¸ í˜„ì¬ ì‹œì  ìŠ¤ëƒ…ìƒ· ë¶„ì„...")
        
        try:
            conn = self.get_db_connection()
            cursor = conn.cursor()
            
            query = """
            SELECT 
                ps.name,
                ps.url,
                td.total_players,
                td.cash_players,
                td.tournament_players,
                td.rank
            FROM poker_sites ps
            JOIN traffic_data td ON ps.id = td.site_id
            WHERE td.total_players > 0
            ORDER BY td.total_players DESC
            """
            
            cursor.execute(query)
            results = cursor.fetchall()
            
            # í˜„ì¬ ì‹œì¥ êµ¬ì¡° ë¶„ì„
            total_players = sum(row[2] for row in results)
            total_cash = sum(row[3] for row in results)
            total_tournaments = sum(row[4] for row in results)
            
            # ì‹œì¥ ì§‘ì¤‘ë„ (HHI) ê³„ì‚°
            market_shares = [(row[2] / total_players) * 100 for row in results]
            hhi = sum(share ** 2 for share in market_shares)
            
            # ìƒìœ„ ì‚¬ì´íŠ¸ ë¶„ì„
            top_sites = []
            cumulative_share = 0
            
            for i, row in enumerate(results):
                name, url, players, cash, tournaments, rank = row
                market_share = (players / total_players) * 100
                cumulative_share += market_share
                
                # í”Œë ˆì´ì–´ êµ¬ì„± ë¶„ì„
                cash_ratio = (cash / players) * 100 if players > 0 else 0
                tournament_ratio = (tournaments / players) * 100 if players > 0 else 0
                
                site_info = {
                    'rank': i + 1,
                    'name': name,
                    'url': url,
                    'total_players': players,
                    'cash_players': cash,
                    'tournament_players': tournaments,
                    'market_share': round(market_share, 2),
                    'cumulative_share': round(cumulative_share, 2),
                    'cash_ratio': round(cash_ratio, 1),
                    'tournament_ratio': round(tournament_ratio, 1),
                    'size_category': self.categorize_by_size(players),
                    'player_focus': self.determine_player_focus(cash_ratio, tournament_ratio)
                }
                
                top_sites.append(site_info)
                
            snapshot = {
                'capture_timestamp': datetime.now().isoformat(),
                'total_market_players': total_players,
                'total_cash_players': total_cash,
                'total_tournament_players': total_tournaments,
                'active_sites_count': len(results),
                'market_concentration': {
                    'hhi_index': round(hhi, 2),
                    'concentration_level': self.classify_concentration(hhi),
                    'top3_share': round(sum(market_shares[:3]), 1),
                    'top5_share': round(sum(market_shares[:5]), 1),
                    'top10_share': round(sum(market_shares[:10]), 1)
                },
                'player_distribution': {
                    'cash_percentage': round((total_cash / total_players) * 100, 1),
                    'tournament_percentage': round((total_tournaments / total_players) * 100, 1),
                    'cash_vs_tournament_ratio': f"{round((total_cash / total_players) * 100, 1)}% : {round((total_tournaments / total_players) * 100, 1)}%"
                },
                'site_rankings': top_sites
            }
            
            conn.close()
            return snapshot
            
        except Exception as e:
            logger.error(f"ìŠ¤ëƒ…ìƒ· ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {}
            
    def categorize_by_size(self, players):
        """ì‚¬ì´íŠ¸ ê·œëª¨ ë¶„ë¥˜"""
        if players >= 50000:
            return "ë©”ì´ì € ì‚¬ì´íŠ¸"
        elif players >= 10000:
            return "ëŒ€í˜• ì‚¬ì´íŠ¸"
        elif players >= 1000:
            return "ì¤‘í˜• ì‚¬ì´íŠ¸"
        elif players >= 100:
            return "ì†Œí˜• ì‚¬ì´íŠ¸"
        else:
            return "ë§ˆì´í¬ë¡œ ì‚¬ì´íŠ¸"
            
    def determine_player_focus(self, cash_ratio, tournament_ratio):
        """í”Œë ˆì´ì–´ í¬ì»¤ìŠ¤ ê²°ì •"""
        if tournament_ratio > 80:
            return "í† ë„ˆë¨¼íŠ¸ ì¤‘ì‹¬"
        elif cash_ratio > 50:
            return "ìºì‹œê²Œì„ ì¤‘ì‹¬"
        elif 30 <= cash_ratio <= 70:
            return "ê· í˜•í˜•"
        else:
            return "í† ë„ˆë¨¼íŠ¸ ìœ„ì£¼"
            
    def classify_concentration(self, hhi):
        """ì‹œì¥ ì§‘ì¤‘ë„ ë¶„ë¥˜"""
        if hhi < 1500:
            return "ê²½ìŸì  ì‹œì¥"
        elif hhi < 2500:
            return "ì¤‘ê°„ ì§‘ì¤‘ë„"
        else:
            return "ê³ ë„ ì§‘ì¤‘ ì‹œì¥"
            
    def perform_comparative_analysis(self):
        """ë¹„êµ ë¶„ì„ ìˆ˜í–‰"""
        logger.info("  ğŸ“Š ë¹„êµ ë¶„ì„ ìˆ˜í–‰...")
        
        snapshot = self.analyze_current_snapshot()
        sites = snapshot.get('site_rankings', [])
        
        if not sites:
            return {}
            
        # ê·œëª¨ë³„ ê·¸ë£¹ ë¶„ì„
        size_groups = defaultdict(list)
        for site in sites:
            size_groups[site['size_category']].append(site)
            
        # í”Œë ˆì´ì–´ íƒ€ì…ë³„ ë¶„ì„
        focus_groups = defaultdict(list)
        for site in sites:
            focus_groups[site['player_focus']].append(site)
            
        # ë¸Œëœë“œ íŒ¨ë°€ë¦¬ ë¶„ì„
        brand_families = self.analyze_brand_families(sites)
        
        # ë„¤íŠ¸ì›Œí¬ë³„ ì§‘ê³„
        network_analysis = self.analyze_networks(sites)
        
        comparative_analysis = {
            'size_distribution': {
                category: {
                    'count': len(sites_list),
                    'total_players': sum(site['total_players'] for site in sites_list),
                    'market_share': sum(site['market_share'] for site in sites_list),
                    'average_size': round(sum(site['total_players'] for site in sites_list) / len(sites_list), 0) if sites_list else 0
                }
                for category, sites_list in size_groups.items()
            },
            'focus_distribution': {
                focus: {
                    'count': len(sites_list),
                    'total_players': sum(site['total_players'] for site in sites_list),
                    'average_cash_ratio': round(sum(site['cash_ratio'] for site in sites_list) / len(sites_list), 1) if sites_list else 0
                }
                for focus, sites_list in focus_groups.items()
            },
            'brand_families': brand_families,
            'network_analysis': network_analysis,
            'market_leaders': sites[:5],  # ìƒìœ„ 5ê°œ
            'emerging_players': [site for site in sites if 1000 <= site['total_players'] <= 10000]  # ì¤‘í˜• ì‚¬ì´íŠ¸ë“¤
        }
        
        return comparative_analysis
        
    def analyze_brand_families(self, sites):
        """ë¸Œëœë“œ íŒ¨ë°€ë¦¬ ë¶„ì„"""
        brand_families = defaultdict(list)
        
        for site in sites:
            name = site['name']
            
            # ë¸Œëœë“œ ë¶„ë¥˜
            if 'PokerStars' in name:
                brand_families['PokerStars'].append(site)
            elif 'GG' in name:
                brand_families['GGNetwork'].append(site)
            elif 'iPoker' in name:
                brand_families['iPoker'].append(site)
            elif 'WPT' in name:
                brand_families['WPT'].append(site)
            else:
                brand_families['ê¸°íƒ€'].append(site)
                
        # ë¸Œëœë“œë³„ ì§‘ê³„
        brand_summary = {}
        for brand, sites_list in brand_families.items():
            if sites_list:
                brand_summary[brand] = {
                    'site_count': len(sites_list),
                    'total_players': sum(site['total_players'] for site in sites_list),
                    'market_share': sum(site['market_share'] for site in sites_list),
                    'sites': [{'name': site['name'], 'players': site['total_players']} for site in sites_list]
                }
                
        return brand_summary
        
    def analyze_networks(self, sites):
        """ë„¤íŠ¸ì›Œí¬ë³„ ë¶„ì„"""
        # ì£¼ìš” ë„¤íŠ¸ì›Œí¬ ì‹ë³„
        networks = {
            'GGNetwork': [site for site in sites if 'GG' in site['name']],
            'PokerStars': [site for site in sites if 'PokerStars' in site['name']],
            'iPoker': [site for site in sites if 'iPoker' in site['name']],
            'ë…ë¦½í˜•': [site for site in sites if not any(keyword in site['name'] for keyword in ['GG', 'PokerStars', 'iPoker'])]
        }
        
        network_summary = {}
        for network, sites_list in networks.items():
            if sites_list:
                network_summary[network] = {
                    'site_count': len(sites_list),
                    'total_players': sum(site['total_players'] for site in sites_list),
                    'market_share': sum(site['market_share'] for site in sites_list),
                    'largest_site': max(sites_list, key=lambda x: x['total_players'])['name'],
                    'average_site_size': round(sum(site['total_players'] for site in sites_list) / len(sites_list), 0)
                }
                
        return network_summary
        
    def extract_news_insights(self):
        """ë‰´ìŠ¤ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
        logger.info("  ğŸ“° ë‰´ìŠ¤ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ...")
        
        try:
            conn = self.get_db_connection()
            cursor = conn.cursor()
            
            query = """
            SELECT title, content, category, author, published_date, url
            FROM news_items
            ORDER BY scraped_at DESC
            LIMIT 30
            """
            
            cursor.execute(query)
            news_data = cursor.fetchall()
            
            # ì‹¤ì œ ë‰´ìŠ¤ì—ì„œ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ
            insights = {
                'recent_news_count': len(news_data),
                'category_distribution': self.analyze_news_categories(news_data),
                'trending_topics': self.extract_trending_topics(news_data),
                'brand_mentions': self.find_brand_mentions(news_data),
                'market_impact_news': self.identify_market_impact_news(news_data),
                'tournament_calendar': self.extract_tournament_info(news_data)
            }
            
            conn.close()
            return insights
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            return {}
            
    def analyze_news_categories(self, news_data):
        """ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ë¶„ì„"""
        categories = Counter()
        for row in news_data:
            category = row[2] or 'general'
            categories[category] += 1
        return dict(categories.most_common(5))
        
    def extract_trending_topics(self, news_data):
        """íŠ¸ë Œë”© í† í”½ ì¶”ì¶œ"""
        keywords = ['WSOP', 'PokerStars', 'GGPoker', 'tournament', 'bracelet', 'high stakes', 'online poker']
        keyword_counts = Counter()
        
        for row in news_data:
            title = row[0] or ''
            content = row[1] or ''
            text = (title + ' ' + content).lower()
            
            for keyword in keywords:
                if keyword.lower() in text:
                    keyword_counts[keyword] += 1
                    
        return dict(keyword_counts.most_common(5))
        
    def find_brand_mentions(self, news_data):
        """ë¸Œëœë“œ ì–¸ê¸‰ ì°¾ê¸°"""
        brands = ['PokerStars', 'GGPoker', 'WPT Global', '888poker', 'partypoker']
        brand_mentions = defaultdict(list)
        
        for row in news_data:
            title = row[0] or ''
            
            for brand in brands:
                if brand.lower() in title.lower():
                    brand_mentions[brand].append({
                        'title': title,
                        'category': row[2],
                        'published_date': row[4]
                    })
                    
        return dict(brand_mentions)
        
    def identify_market_impact_news(self, news_data):
        """ì‹œì¥ ì„íŒ©íŠ¸ ë‰´ìŠ¤ ì‹ë³„"""
        impact_keywords = ['regulation', 'launch', 'partnership', 'acquisition', 'license', 'ban']
        impact_news = []
        
        for row in news_data:
            title = row[0] or ''
            content = row[1] or ''
            text = (title + ' ' + content).lower()
            
            if any(keyword in text for keyword in impact_keywords):
                impact_news.append({
                    'title': title,
                    'category': row[2],
                    'published_date': row[4],
                    'impact_type': [kw for kw in impact_keywords if kw in text][0]
                })
                
        return impact_news[:5]
        
    def extract_tournament_info(self, news_data):
        """í† ë„ˆë¨¼íŠ¸ ì •ë³´ ì¶”ì¶œ"""
        tournament_news = []
        tournament_keywords = ['WSOP', 'WPT', 'EPT', 'tournament', 'bracelet']
        
        for row in news_data:
            title = row[0] or ''
            
            if any(keyword.lower() in title.lower() for keyword in tournament_keywords):
                tournament_news.append({
                    'title': title,
                    'category': row[2],
                    'published_date': row[4]
                })
                
        return tournament_news[:5]
        
    def generate_realistic_recommendations(self):
        """í˜„ì‹¤ì ì¸ ê¶Œê³ ì‚¬í•­ ìƒì„±"""
        logger.info("  ğŸ’¡ í˜„ì‹¤ì ì¸ ê¶Œê³ ì‚¬í•­ ìƒì„±...")
        
        snapshot = self.analyze_current_snapshot()
        quality = self.assess_data_quality()
        
        recommendations = {
            'data_improvement': [],
            'monitoring_priorities': [],
            'analysis_enhancements': [],
            'broadcast_focus_areas': []
        }
        
        # ë°ì´í„° ê°œì„  ê¶Œê³ 
        if quality.get('historical_data_coverage', 0) < 50:
            recommendations['data_improvement'].append({
                'priority': 'HIGH',
                'action': 'íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ êµ¬ì¶•',
                'description': 'ì •í™•í•œ íŠ¸ë Œë“œ ë¶„ì„ì„ ìœ„í•´ ë§¤ì¼ ë°ì´í„° ìˆ˜ì§‘ í•„ìš”',
                'timeline': '1-2ì£¼'
            })
            
        if quality.get('questionable_trend_data'):
            recommendations['data_improvement'].append({
                'priority': 'MEDIUM',
                'action': 'ë°ì´í„° ê²€ì¦ í”„ë¡œì„¸ìŠ¤ ê°•í™”',
                'description': 'ë¹„í˜„ì‹¤ì ì¸ ì„±ì¥ë¥  ë°ì´í„° í•„í„°ë§ ë¡œì§ ì¶”ê°€',
                'timeline': '1ì£¼'
            })
            
        # ëª¨ë‹ˆí„°ë§ ìš°ì„ ìˆœìœ„
        top_sites = snapshot.get('site_rankings', [])[:5]
        for site in top_sites:
            if site['market_share'] > 20:
                recommendations['monitoring_priorities'].append({
                    'site': site['name'],
                    'reason': f"ì‹œì¥ ë¦¬ë” ({site['market_share']}% ì ìœ )",
                    'focus': 'ì‹œì¥ ì˜í–¥ë ¥ì´ í° ì‚¬ì´íŠ¸ë¡œ ë³€í™” ëª¨ë‹ˆí„°ë§ í•„ìˆ˜'
                })
                
        # ë¶„ì„ ê°•í™” ì˜ì—­
        recommendations['analysis_enhancements'] = [
            {
                'area': 'ë‰´ìŠ¤-íŠ¸ë˜í”½ ìƒê´€ê´€ê³„ ë¶„ì„',
                'description': 'í”„ë¡œëª¨ì…˜, í† ë„ˆë¨¼íŠ¸ ë°œí‘œì™€ íŠ¸ë˜í”½ ë³€í™” íŒ¨í„´ ë¶„ì„',
                'benefit': 'ë§ˆì¼€íŒ… ì´ë²¤íŠ¸ íš¨ê³¼ ì¸¡ì • ê°€ëŠ¥'
            },
            {
                'area': 'ì§€ì—­ë³„ ì‹œì¥ ì„¸ë¶„í™”',
                'description': 'ìœ ëŸ½, ì•„ì‹œì•„, ë¶ë¯¸ ì§€ì—­ë³„ í¬ì»¤ ì‚¬ì´íŠ¸ ì„ í˜¸ë„ ë¶„ì„',
                'benefit': 'ì§€ì—­ë³„ ë§ì¶¤ ì½˜í…ì¸  ì œì‘ ê°€ëŠ¥'
            },
            {
                'area': 'ì‹¤ì‹œê°„ ì•Œë¦¼ ì‹œìŠ¤í…œ',
                'description': 'ê¸‰ê²©í•œ íŠ¸ë˜í”½ ë³€í™” ì‹œ ì¦‰ì‹œ ì•Œë¦¼ ê¸°ëŠ¥',
                'benefit': 'ë¸Œë ˆì´í‚¹ ë‰´ìŠ¤ ë°œêµ´ ë° ì¦‰ì‹œ ëŒ€ì‘ ê°€ëŠ¥'
            }
        ]
        
        # ë°©ì†¡ í¬ì»¤ìŠ¤ ì˜ì—­
        recommendations['broadcast_focus_areas'] = [
            {
                'topic': 'ì‹œì¥ ì§‘ì¤‘ë„ ì´ìŠˆ',
                'angle': f"ìƒìœ„ 3ê°œ ì‚¬ì´íŠ¸ê°€ {snapshot.get('market_concentration', {}).get('top3_share', 0)}% ì ìœ ",
                'talking_points': ['ì‹œì¥ ë‹¤ì–‘ì„±', 'ì‹ ê·œ ì‚¬ì´íŠ¸ ê¸°íšŒ', 'í”Œë ˆì´ì–´ ì„ íƒê¶Œ']
            },
            {
                'topic': 'í† ë„ˆë¨¼íŠ¸ vs ìºì‹œê²Œì„ íŠ¸ë Œë“œ',
                'angle': snapshot.get('player_distribution', {}).get('cash_vs_tournament_ratio', ''),
                'talking_points': ['í”Œë ˆì´ì–´ ì„±í–¥ ë³€í™”', 'ì‚¬ì´íŠ¸ë³„ ì „ëµ', 'ê²Œì„ ìœ í˜•ë³„ ì„±ì¥']
            }
        ]
        
        return recommendations
        
    def document_data_limitations(self):
        """ë°ì´í„° í•œê³„ ë¬¸ì„œí™”"""
        return {
            'current_limitations': [
                'ë‹¨ì¼ ì‹œì  ìŠ¤ëƒ…ìƒ·ìœ¼ë¡œ íŠ¸ë Œë“œ ë¶„ì„ ì œí•œ',
                'ëŒ€ë¶€ë¶„ ì‚¬ì´íŠ¸ì˜ íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ë¶€ì¡±',
                'ì¼ë¶€ ì„±ì¥ë¥  ë°ì´í„°ì˜ í˜„ì‹¤ì„± ì˜ë¬¸',
                'ì‹œê°„ëŒ€ë³„, ìš”ì¼ë³„ íŒ¨í„´ ë¶„ì„ ë¶ˆê°€'
            ],
            'reliability_assessment': {
                'í˜„ì¬ ì‹œì¥ ìƒí™©': 'HIGH - ì‹¤ì‹œê°„ ë°ì´í„° ì •í™•',
                'ì‹œì¥ ì ìœ ìœ¨': 'HIGH - ìƒëŒ€ì  ìˆœìœ„ ì‹ ë¢° ê°€ëŠ¥',
                'ì„±ì¥ íŠ¸ë Œë“œ': 'LOW - íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ë¶€ì¡±',
                'ë‰´ìŠ¤ ì—°ê´€ì„±': 'MEDIUM - ì‹œì  ì°¨ì´ë¡œ ì¸í•œ ì œì•½'
            },
            'improvement_roadmap': [
                '1ì£¼ì°¨: ë§¤ì¼ ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ êµ¬ì¶•',
                '2ì£¼ì°¨: 7ì¼ ì´ë™í‰ê·  íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘',
                '1ê°œì›”ì°¨: ì›”ë³„ ë¹„êµ ë¶„ì„ ê°€ëŠ¥',
                '3ê°œì›”ì°¨: ê³„ì ˆì„± ë° íŒ¨í„´ ë¶„ì„ ê°€ëŠ¥'
            ],
            'alternative_analysis_methods': [
                'í˜„ì¬ ì‹œì  ì‹œì¥ êµ¬ì¡° ë¶„ì„ì— ì§‘ì¤‘',
                'ë‰´ìŠ¤ ê¸°ë°˜ ì •ì„±ì  ë¶„ì„ ê°•í™”',
                'ì‚¬ì´íŠ¸ë³„ íŠ¹ì„± ë° í¬ì§€ì…”ë‹ ë¶„ì„',
                'ë¸Œëœë“œë³„ ì „ëµ ë° ê²½ìŸ êµ¬ë„ ë¶„ì„'
            ]
        }
        
    def save_realistic_report(self, report_data):
        """í˜„ì‹¤ì ì¸ ë¦¬í¬íŠ¸ ì €ì¥"""
        logger.info("ğŸ’¾ í˜„ì‹¤ì ì¸ ë¦¬í¬íŠ¸ ì €ì¥...")
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # JSON ìƒì„¸ ë¦¬í¬íŠ¸
        json_filename = f'realistic_analysis_{timestamp}.json'
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
            
        # ë°©ì†¡ìš© í˜„ì‹¤ì  ë¸Œë¦¬í•‘
        brief_filename = f'realistic_brief_{timestamp}.txt'
        self.save_realistic_brief(report_data, brief_filename)
        
        logger.info(f"ğŸ“Š ìƒì„¸ ë¶„ì„: {json_filename}")
        logger.info(f"ğŸ“º í˜„ì‹¤ì  ë¸Œë¦¬í•‘: {brief_filename}")
        
        return json_filename, brief_filename
        
    def save_realistic_brief(self, report_data, filename):
        """í˜„ì‹¤ì  ë¸Œë¦¬í•‘ ì €ì¥"""
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("ğŸ“Š í˜„ì‹¤ì ì¸ ì˜¨ë¼ì¸ í¬ì»¤ ì‹œì¥ ë¶„ì„ ë¸Œë¦¬í•‘\n")
            f.write(f"ë¶„ì„ ì‹œì : {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„')}\n")
            f.write("=" * 80 + "\n\n")
            
            # ë°ì´í„° ì‹ ë¢°ë„ ê³µì§€
            quality = report_data.get('data_quality_assessment', {})
            f.write("âš ï¸ ë°ì´í„° ì‹ ë¢°ë„ ì•ˆë‚´\n")
            f.write("-" * 40 + "\n")
            f.write(f"í˜„ì¬ ì‹œì  ë°ì´í„°: ì‹ ë¢° ê°€ëŠ¥\n")
            f.write(f"íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ë³´ìœ ìœ¨: {quality.get('historical_data_coverage', 0)}%\n")
            f.write(f"íŠ¸ë Œë“œ ë¶„ì„ ì‹ ë¢°ë„: {quality.get('data_reliability', 'UNKNOWN')}\n\n")
            
            # í˜„ì¬ ì‹œì¥ í˜„í™©
            snapshot = report_data.get('current_market_snapshot', {})
            f.write("ğŸ“Š í˜„ì¬ ì‹œì¥ í˜„í™© (í™•ì‹¤í•œ ë°ì´í„°)\n")
            f.write("-" * 40 + "\n")
            f.write(f"ì´ ì˜¨ë¼ì¸ í”Œë ˆì´ì–´: {snapshot.get('total_market_players', 0):,}ëª…\n")
            f.write(f"í™œì„± í¬ì»¤ ì‚¬ì´íŠ¸: {snapshot.get('active_sites_count', 0)}ê°œ\n")
            concentration = snapshot.get('market_concentration', {})
            f.write(f"ì‹œì¥ ì§‘ì¤‘ë„: {concentration.get('concentration_level', 'N/A')}\n")
            f.write(f"ìƒìœ„ 3ê°œ ì‚¬ì´íŠ¸ ì ìœ ìœ¨: {concentration.get('top3_share', 0)}%\n\n")
            
            # ì‹œì¥ êµ¬ì¡° ë¶„ì„
            comparative = report_data.get('comparative_analysis', {})
            brand_families = comparative.get('brand_families', {})
            f.write("ğŸ¢ ì£¼ìš” ë¸Œëœë“œë³„ í˜„í™©\n")
            f.write("-" * 40 + "\n")
            for brand, data in brand_families.items():
                if data.get('total_players', 0) > 1000:  # 1ì²œëª… ì´ìƒë§Œ í‘œì‹œ
                    f.write(f"{brand}: {data['total_players']:,}ëª… ({data['market_share']:.1f}%, {data['site_count']}ê°œ ì‚¬ì´íŠ¸)\n")
            f.write("\n")
            
            # ë‰´ìŠ¤ ì¸ì‚¬ì´íŠ¸
            news = report_data.get('news_based_insights', {})
            f.write("ğŸ“° ë‰´ìŠ¤ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸\n")
            f.write("-" * 40 + "\n")
            f.write(f"ìµœê·¼ ë¶„ì„ ê¸°ì‚¬: {news.get('recent_news_count', 0)}ê°œ\n")
            
            trending = news.get('trending_topics', {})
            if trending:
                f.write("ì£¼ìš” í‚¤ì›Œë“œ:\n")
                for keyword, count in list(trending.items())[:3]:
                    f.write(f"  â€¢ {keyword}: {count}íšŒ ì–¸ê¸‰\n")
                    
            brand_mentions = news.get('brand_mentions', {})
            if brand_mentions:
                f.write("ë¸Œëœë“œ ë‰´ìŠ¤ ì–¸ê¸‰:\n")
                for brand, mentions in brand_mentions.items():
                    if mentions:
                        f.write(f"  â€¢ {brand}: {len(mentions)}ê°œ ê¸°ì‚¬\n")
            f.write("\n")
            
            # í•œê³„ ë° ì£¼ì˜ì‚¬í•­
            limitations = report_data.get('data_limitations', {})
            f.write("âš ï¸ ë¶„ì„ í•œê³„ ë° ì£¼ì˜ì‚¬í•­\n")
            f.write("-" * 40 + "\n")
            for limitation in limitations.get('current_limitations', []):
                f.write(f"  â€¢ {limitation}\n")
            f.write("\n")
            
            # ê¶Œê³ ì‚¬í•­
            recommendations = report_data.get('actionable_recommendations', {})
            broadcast_areas = recommendations.get('broadcast_focus_areas', [])
            f.write("ğŸ“º ë°©ì†¡ ì¶”ì²œ í¬ì»¤ìŠ¤ ì˜ì—­\n")
            f.write("-" * 40 + "\n")
            for area in broadcast_areas:
                f.write(f"ì£¼ì œ: {area['topic']}\n")
                f.write(f"ì•µê¸€: {area['angle']}\n")
                f.write(f"í¬ì¸íŠ¸: {', '.join(area['talking_points'])}\n\n")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ“Š í˜„ì‹¤ì ì¸ í¬ì»¤ ì‹œì¥ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±ê¸°")
    print("=" * 60)
    
    generator = RealisticAnalysisReportGenerator()
    
    try:
        # í˜„ì‹¤ì ì¸ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
        print("\nğŸ”„ í˜„ì‹¤ì ì¸ ë¶„ì„ ì¤‘...")
        report_data = generator.generate_realistic_report()
        
        # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
        print("\nğŸ“Š ë¶„ì„ ê²°ê³¼:")
        
        quality = report_data.get('data_quality_assessment', {})
        print(f"ë°ì´í„° ì‹ ë¢°ë„: {quality.get('data_reliability', 'UNKNOWN')}")
        print(f"íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ë³´ìœ ìœ¨: {quality.get('historical_data_coverage', 0)}%")
        
        snapshot = report_data.get('current_market_snapshot', {})
        print(f"í˜„ì¬ ì´ í”Œë ˆì´ì–´: {snapshot.get('total_market_players', 0):,}ëª…")
        print(f"í™œì„± ì‚¬ì´íŠ¸: {snapshot.get('active_sites_count', 0)}ê°œ")
        
        # ë‰´ìŠ¤ ì¸ì‚¬ì´íŠ¸
        news = report_data.get('news_based_insights', {})
        print(f"ë¶„ì„ëœ ë‰´ìŠ¤: {news.get('recent_news_count', 0)}ê°œ")
        
        trending = news.get('trending_topics', {})
        if trending:
            top_trend = list(trending.items())[0]
            print(f"ìµœë‹¤ ì–¸ê¸‰ í‚¤ì›Œë“œ: {top_trend[0]} ({top_trend[1]}íšŒ)")
            
        # ë¦¬í¬íŠ¸ ì €ì¥
        print("\nğŸ’¾ ë¦¬í¬íŠ¸ ì €ì¥ ì¤‘...")
        json_file, brief_file = generator.save_realistic_report(report_data)
        
        print(f"\nâœ… í˜„ì‹¤ì ì¸ ë¶„ì„ ë¦¬í¬íŠ¸ ì™„ë£Œ!")
        print(f"ğŸ“Š ìƒì„¸ ë¶„ì„: {json_file}")
        print(f"ğŸ“º í˜„ì‹¤ì  ë¸Œë¦¬í•‘: {brief_file}")
        
        return True
        
    except Exception as e:
        logger.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì˜¤ë¥˜: {str(e)}")
        return False

if __name__ == "__main__":
    success = main()
    if success:
        print(f"\nğŸ¯ í˜„ì‹¤ì ì¸ ë¶„ì„ ì™„ë£Œ! ë°ì´í„° í•œê³„ë¥¼ ê³ ë ¤í•œ ì •í™•í•œ ì¸ì‚¬ì´íŠ¸ ì œê³µ")
    else:
        print(f"\nğŸ’€ ë¶„ì„ ì‹¤íŒ¨ - ë¬¸ì œ í•´ê²° í•„ìš”")