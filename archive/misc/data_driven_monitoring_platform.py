#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GG POKER 직원용 데이터 중심 온라인 포커 모니터링 플랫폼
- 4개 핵심 지표: Players Online, Cash Players, 24h Peak, 7-day Average
- 날짜별 시계열 데이터 수집 및 분석
- 급변 시점의 뉴스 연관성 분석
- 추론 배제, 순수 데이터 기반 분석
"""
import sys
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')

import json
import logging
import sqlite3
from datetime import datetime, timedelta
from collections import defaultdict, Counter
import statistics
import math

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataDrivenMonitoringPlatform:
    def __init__(self, db_path='poker_monitoring.db'):
        self.db_path = db_path
        self.setup_database()
        
        # 급변 감지 임계값 (실제 데이터 기반으로 조정 필요)
        self.SIGNIFICANT_CHANGE_THRESHOLD = 15.0  # 15% 변화
        self.MAJOR_CHANGE_THRESHOLD = 25.0        # 25% 주요 변화
        self.ANOMALY_THRESHOLD = 50.0             # 50% 이상치
        
    def setup_database(self):
        """데이터베이스 스키마 설정"""
        logger.info("📊 데이터베이스 스키마 설정...")
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # 일일 트래픽 데이터 테이블 (시계열)
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS daily_traffic (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                site_name TEXT NOT NULL,
                collection_date DATE NOT NULL,
                collection_time TIME NOT NULL,
                players_online INTEGER NOT NULL,
                cash_players INTEGER NOT NULL,
                peak_24h INTEGER,
                seven_day_avg INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(site_name, collection_date, collection_time)
            )
        ''')
        
        # 변화 감지 로그 테이블
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS change_detection (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                site_name TEXT NOT NULL,\n                detection_date DATE NOT NULL,\n                metric_type TEXT NOT NULL,  -- 'players_online', 'cash_players', 'peak_24h', 'seven_day_avg'\n                previous_value INTEGER,\n                current_value INTEGER,\n                change_percentage REAL,\n                change_magnitude TEXT,  -- 'MINOR', 'SIGNIFICANT', 'MAJOR', 'ANOMALY'\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            )\n        ''')\n        \n        # 뉴스-변화 연관성 테이블\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS news_correlation (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                change_detection_id INTEGER,\n                news_title TEXT,\n                news_url TEXT,\n                news_date DATE,\n                correlation_score REAL,  -- 0-1 스코어\n                correlation_type TEXT,   -- 'DIRECT', 'INDIRECT', 'TEMPORAL'\n                analysis_notes TEXT,\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                FOREIGN KEY (change_detection_id) REFERENCES change_detection (id)\n            )\n        ''')\n        \n        # 사이트 메타데이터 테이블\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS site_metadata (\n                site_name TEXT PRIMARY KEY,\n                site_url TEXT,\n                network_family TEXT,     -- 'GGNetwork', 'PokerStars', 'iPoker', 'Independent'\n                market_tier TEXT,        -- 'Tier1', 'Tier2', 'Tier3'\n                monitoring_priority TEXT, -- 'HIGH', 'MEDIUM', 'LOW'\n                competitor_category TEXT, -- 'DIRECT', 'INDIRECT', 'NICHE'\n                notes TEXT,\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            )\n        ''')\n        \n        conn.commit()\n        conn.close()\n        logger.info("✅ 데이터베이스 스키마 설정 완료")\n        \n    def collect_daily_data(self, site_data_list):\n        """일일 데이터 수집 (실제 환경에서는 크롤러가 호출)"""\n        logger.info("📈 일일 데이터 수집 시작...")\n        \n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        \n        collection_date = datetime.now().strftime('%Y-%m-%d')\n        collection_time = datetime.now().strftime('%H:%M:%S')\n        \n        collected_count = 0\n        \n        for site_data in site_data_list:\n            try:\n                cursor.execute('''\n                    INSERT OR REPLACE INTO daily_traffic \n                    (site_name, collection_date, collection_time, players_online, \n                     cash_players, peak_24h, seven_day_avg)\n                    VALUES (?, ?, ?, ?, ?, ?, ?)\n                ''', (\n                    site_data['site_name'],\n                    collection_date,\n                    collection_time,\n                    site_data['players_online'],\n                    site_data['cash_players'],\n                    site_data.get('peak_24h', None),\n                    site_data.get('seven_day_avg', None)\n                ))\n                \n                collected_count += 1\n                \n            except Exception as e:\n                logger.error(f"데이터 수집 오류 - {site_data['site_name']}: {str(e)}")\n                \n        conn.commit()\n        conn.close()\n        \n        logger.info(f"✅ {collected_count}개 사이트 데이터 수집 완료")\n        return collected_count\n        \n    def detect_significant_changes(self, target_date=None):\n        """유의미한 변화 감지"""\n        if target_date is None:\n            target_date = datetime.now().strftime('%Y-%m-%d')\n            \n        logger.info(f"🔍 {target_date} 변화 감지 시작...")\n        \n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        \n        # 현재 날짜와 이전 날짜 데이터 비교\n        query = '''\n            WITH current_data AS (\n                SELECT site_name, players_online, cash_players, peak_24h, seven_day_avg\n                FROM daily_traffic \n                WHERE collection_date = ?\n                ORDER BY collection_time DESC\n                LIMIT 50\n            ),\n            previous_data AS (\n                SELECT site_name, players_online, cash_players, peak_24h, seven_day_avg\n                FROM daily_traffic \n                WHERE collection_date = date(?, '-1 day')\n                ORDER BY collection_time DESC\n                LIMIT 50\n            )\n            SELECT \n                c.site_name,\n                c.players_online as current_players,\n                p.players_online as previous_players,\n                c.cash_players as current_cash,\n                p.cash_players as previous_cash,\n                c.peak_24h as current_peak,\n                p.peak_24h as previous_peak,\n                c.seven_day_avg as current_7day,\n                p.seven_day_avg as previous_7day\n            FROM current_data c\n            LEFT JOIN previous_data p ON c.site_name = p.site_name\n            WHERE p.site_name IS NOT NULL\n        '''\n        \n        cursor.execute(query, (target_date, target_date))\n        results = cursor.fetchall()\n        \n        detected_changes = []\n        \n        for row in results:\n            site_name = row[0]\n            changes = self.analyze_site_changes(row)\n            \n            for change in changes:\n                if change['magnitude'] in ['SIGNIFICANT', 'MAJOR', 'ANOMALY']:\n                    # 변화 감지 로그에 저장\n                    cursor.execute('''\n                        INSERT INTO change_detection \n                        (site_name, detection_date, metric_type, previous_value, \n                         current_value, change_percentage, change_magnitude)\n                        VALUES (?, ?, ?, ?, ?, ?, ?)\n                    ''', (\n                        site_name,\n                        target_date,\n                        change['metric'],\n                        change['previous_value'],\n                        change['current_value'],\n                        change['change_percentage'],\n                        change['magnitude']\n                    ))\n                    \n                    change['change_id'] = cursor.lastrowid\n                    detected_changes.append(change)\n                    \n        conn.commit()\n        conn.close()\n        \n        logger.info(f"🚨 {len(detected_changes)}개 유의미한 변화 감지")\n        return detected_changes\n        \n    def analyze_site_changes(self, data_row):\n        """사이트별 변화 분석"""\n        site_name, curr_players, prev_players, curr_cash, prev_cash, \\\n        curr_peak, prev_peak, curr_7day, prev_7day = data_row\n        \n        changes = []\n        \n        # 각 지표별 변화율 계산\n        metrics = [\n            ('players_online', curr_players, prev_players),\n            ('cash_players', curr_cash, prev_cash),\n            ('peak_24h', curr_peak, prev_peak),\n            ('seven_day_avg', curr_7day, prev_7day)\n        ]\n        \n        for metric_name, current, previous in metrics:\n            if current is not None and previous is not None and previous > 0:\n                change_percentage = ((current - previous) / previous) * 100\n                magnitude = self.classify_change_magnitude(abs(change_percentage))\n                \n                changes.append({\n                    'site_name': site_name,\n                    'metric': metric_name,\n                    'previous_value': previous,\n                    'current_value': current,\n                    'change_percentage': round(change_percentage, 2),\n                    'magnitude': magnitude,\n                    'direction': 'INCREASE' if change_percentage > 0 else 'DECREASE'\n                })\n                \n        return changes\n        \n    def classify_change_magnitude(self, abs_change_percentage):\n        """변화 크기 분류"""\n        if abs_change_percentage >= self.ANOMALY_THRESHOLD:\n            return 'ANOMALY'\n        elif abs_change_percentage >= self.MAJOR_CHANGE_THRESHOLD:\n            return 'MAJOR'\n        elif abs_change_percentage >= self.SIGNIFICANT_CHANGE_THRESHOLD:\n            return 'SIGNIFICANT'\n        else:\n            return 'MINOR'\n            \n    def analyze_news_correlation(self, detected_changes, news_data):\n        """뉴스 연관성 분석"""\n        logger.info("📰 뉴스 연관성 분석 시작...")\n        \n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        \n        correlations = []\n        \n        for change in detected_changes:\n            site_name = change['site_name']\n            change_date = change.get('detection_date', datetime.now().strftime('%Y-%m-%d'))\n            \n            # 해당 사이트 관련 뉴스 찾기\n            relevant_news = self.find_relevant_news(site_name, change_date, news_data)\n            \n            for news_item in relevant_news:\n                correlation_score = self.calculate_correlation_score(\n                    change, news_item, change_date\n                )\n                \n                if correlation_score >= 0.3:  # 임계값 이상만 저장\n                    cursor.execute('''\n                        INSERT INTO news_correlation \n                        (change_detection_id, news_title, news_url, news_date,\n                         correlation_score, correlation_type, analysis_notes)\n                        VALUES (?, ?, ?, ?, ?, ?, ?)\n                    ''', (\n                        change.get('change_id'),\n                        news_item['title'],\n                        news_item.get('url', ''),\n                        news_item.get('published_date', change_date),\n                        correlation_score,\n                        news_item['correlation_type'],\n                        news_item.get('analysis_notes', '')\n                    ))\n                    \n                    correlations.append({\n                        'change': change,\n                        'news': news_item,\n                        'correlation_score': correlation_score\n                    })\n                    \n        conn.commit()\n        conn.close()\n        \n        logger.info(f"🔗 {len(correlations)}개 뉴스-변화 연관성 발견")\n        return correlations\n        \n    def find_relevant_news(self, site_name, change_date, news_data):\n        """관련 뉴스 찾기"""\n        relevant_news = []\n        \n        # 날짜 범위 설정 (변화 전후 3일)\n        target_date = datetime.strptime(change_date, '%Y-%m-%d')\n        date_range_start = target_date - timedelta(days=3)\n        date_range_end = target_date + timedelta(days=1)\n        \n        # 사이트명 패턴 생성\n        site_patterns = self.generate_site_search_patterns(site_name)\n        \n        for news_item in news_data:\n            news_date_str = news_item.get('published_date', '')\n            \n            # 날짜 필터링\n            try:\n                if news_date_str:\n                    news_date = datetime.strptime(news_date_str[:10], '%Y-%m-%d')\n                    if not (date_range_start <= news_date <= date_range_end):\n                        continue\n            except:\n                continue\n                \n            # 사이트 관련성 체크\n            title = news_item.get('title', '').lower()\n            content = news_item.get('content', '').lower()\n            text = title + ' ' + content\n            \n            correlation_type = None\n            analysis_notes = []\n            \n            # 직접 언급 체크\n            for pattern in site_patterns:\n                if pattern.lower() in text:\n                    correlation_type = 'DIRECT'\n                    analysis_notes.append(f'직접 언급: {pattern}')\n                    break\n                    \n            # 간접 연관성 체크 (직접 언급이 없는 경우)\n            if not correlation_type:\n                indirect_keywords = self.get_indirect_keywords(site_name)\n                found_keywords = []\n                \n                for keyword in indirect_keywords:\n                    if keyword.lower() in text:\n                        found_keywords.append(keyword)\n                        \n                if found_keywords:\n                    correlation_type = 'INDIRECT'\n                    analysis_notes.append(f'간접 연관: {", ".join(found_keywords)}')\n                    \n            # 시간적 연관성 (같은 날짜)\n            if not correlation_type and news_date.date() == target_date.date():\n                market_keywords = ['poker', 'online', 'tournament', 'cash game', 'promotion']\n                if any(keyword in text for keyword in market_keywords):\n                    correlation_type = 'TEMPORAL'\n                    analysis_notes.append('시간적 연관성')\n                    \n            if correlation_type:\n                news_item_copy = news_item.copy()\n                news_item_copy['correlation_type'] = correlation_type\n                news_item_copy['analysis_notes'] = '; '.join(analysis_notes)\n                relevant_news.append(news_item_copy)\n                \n        return relevant_news\n        \n    def generate_site_search_patterns(self, site_name):\n        """사이트 검색 패턴 생성"""\n        patterns = [site_name]\n        \n        # 네트워크별 추가 패턴\n        if 'GG' in site_name.upper():\n            patterns.extend(['GGPoker', 'GG Poker', 'GGNetwork', 'GG Network'])\n        elif 'POKERSTARS' in site_name.upper():\n            patterns.extend(['PokerStars', 'Poker Stars', 'Stars'])\n        elif 'WPT' in site_name.upper():\n            patterns.extend(['WPT Global', 'World Poker Tour'])\n        elif 'IPOKER' in site_name.upper():\n            patterns.extend(['iPoker', 'iPoker Network'])\n            \n        return patterns\n        \n    def get_indirect_keywords(self, site_name):\n        """간접 연관 키워드 획득"""\n        base_keywords = ['tournament', 'promotion', 'bonus', 'update', 'partnership']\n        \n        # 네트워크별 특화 키워드\n        if 'GG' in site_name.upper():\n            base_keywords.extend(['WSOP', 'bracelet', 'GG Masters'])\n        elif 'POKERSTARS' in site_name.upper():\n            base_keywords.extend(['SCOOP', 'WCOOP', 'EPT', 'Sunday Million'])\n        elif 'WPT' in site_name.upper():\n            base_keywords.extend(['WPT', 'World Poker Tour', 'WPT500'])\n            \n        return base_keywords\n        \n    def calculate_correlation_score(self, change, news_item, change_date):\n        """연관성 점수 계산 (0-1)"""\n        score = 0.0\n        \n        # 기본 점수 (연관성 타입별)\n        correlation_type = news_item.get('correlation_type', '')\n        if correlation_type == 'DIRECT':\n            score += 0.6\n        elif correlation_type == 'INDIRECT':\n            score += 0.3\n        elif correlation_type == 'TEMPORAL':\n            score += 0.1\n            \n        # 날짜 근접성 보너스\n        try:\n            news_date_str = news_item.get('published_date', '')\n            if news_date_str:\n                news_date = datetime.strptime(news_date_str[:10], '%Y-%m-%d')\n                change_date_obj = datetime.strptime(change_date, '%Y-%m-%d')\n                days_diff = abs((news_date - change_date_obj).days)\n                \n                if days_diff == 0:\n                    score += 0.3\n                elif days_diff == 1:\n                    score += 0.2\n                elif days_diff <= 2:\n                    score += 0.1\n        except:\n            pass\n            \n        # 변화 크기 보너스\n        change_magnitude = change.get('magnitude', '')\n        if change_magnitude == 'ANOMALY':\n            score += 0.2\n        elif change_magnitude == 'MAJOR':\n            score += 0.1\n            \n        # 키워드 밀도 보너스\n        title = news_item.get('title', '').lower()\n        content = news_item.get('content', '').lower()\n        \n        important_keywords = ['launch', 'new', 'update', 'promotion', 'tournament', 'bonus']\n        keyword_count = sum(1 for keyword in important_keywords if keyword in title + content)\n        score += min(keyword_count * 0.05, 0.15)\n        \n        return min(score, 1.0)\n        \n    def generate_time_series_analysis(self, site_name, days_back=30):\n        \"\"\"시계열 분석 생성\"\"\"\n        logger.info(f\"📊 {site_name} 시계열 분석 ({days_back}일)\")\n        \n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        \n        # 일별 최신 데이터 조회\n        query = '''\n            SELECT \n                collection_date,\n                players_online,\n                cash_players,\n                peak_24h,\n                seven_day_avg\n            FROM daily_traffic \n            WHERE site_name = ? \n            AND collection_date >= date('now', '-' || ? || ' days')\n            GROUP BY collection_date\n            HAVING collection_time = MAX(collection_time)\n            ORDER BY collection_date\n        '''\n        \n        cursor.execute(query, (site_name, days_back))\n        results = cursor.fetchall()\n        \n        if not results:\n            return None\n            \n        # 시계열 데이터 구성\n        time_series = {\n            'site_name': site_name,\n            'period': f'{days_back} days',\n            'data_points': len(results),\n            'dates': [],\n            'players_online': [],\n            'cash_players': [],\n            'peak_24h': [],\n            'seven_day_avg': [],\n            'analytics': {}\n        }\n        \n        for row in results:\n            time_series['dates'].append(row[0])\n            time_series['players_online'].append(row[1])\n            time_series['cash_players'].append(row[2])\n            time_series['peak_24h'].append(row[3] if row[3] is not None else 0)\n            time_series['seven_day_avg'].append(row[4] if row[4] is not None else 0)\n            \n        # 기본 통계 계산\n        time_series['analytics'] = self.calculate_time_series_analytics(time_series)\n        \n        conn.close()\n        return time_series\n        \n    def calculate_time_series_analytics(self, time_series):\n        \"\"\"시계열 기본 통계 계산\"\"\"\n        analytics = {}\n        \n        for metric in ['players_online', 'cash_players', 'peak_24h', 'seven_day_avg']:\n            values = [v for v in time_series[metric] if v is not None and v > 0]\n            \n            if values:\n                analytics[metric] = {\n                    'current': values[-1] if values else 0,\n                    'min': min(values),\n                    'max': max(values),\n                    'mean': round(statistics.mean(values), 1),\n                    'median': round(statistics.median(values), 1),\n                    'std_dev': round(statistics.stdev(values), 1) if len(values) > 1 else 0,\n                    'trend': self.calculate_trend(values),\n                    'volatility': self.calculate_volatility(values),\n                    'recent_change': self.calculate_recent_change(values)\n                }\n            else:\n                analytics[metric] = {'no_data': True}\n                \n        return analytics\n        \n    def calculate_trend(self, values):\n        \"\"\"트렌드 계산 (단순 선형 회귀)\"\"\"\n        if len(values) < 3:\n            return 'INSUFFICIENT_DATA'\n            \n        # 최근 7일 vs 이전 기간 비교\n        if len(values) >= 7:\n            recent_avg = statistics.mean(values[-7:])\n            previous_avg = statistics.mean(values[:-7]) if len(values) > 7 else statistics.mean(values)\n            \n            if recent_avg > previous_avg * 1.05:\n                return 'UPWARD'\n            elif recent_avg < previous_avg * 0.95:\n                return 'DOWNWARD'\n            else:\n                return 'STABLE'\n        else:\n            # 첫값 vs 마지막값\n            if values[-1] > values[0] * 1.1:\n                return 'UPWARD'\n            elif values[-1] < values[0] * 0.9:\n                return 'DOWNWARD'\n            else:\n                return 'STABLE'\n                \n    def calculate_volatility(self, values):\n        \"\"\"변동성 계산\"\"\"\n        if len(values) < 2:\n            return 'LOW'\n            \n        mean_val = statistics.mean(values)\n        std_dev = statistics.stdev(values)\n        cv = (std_dev / mean_val) * 100 if mean_val > 0 else 0\n        \n        if cv >= 20:\n            return 'HIGH'\n        elif cv >= 10:\n            return 'MEDIUM'\n        else:\n            return 'LOW'\n            \n    def calculate_recent_change(self, values):\n        \"\"\"최근 변화율 계산\"\"\"\n        if len(values) < 2:\n            return 0\n            \n        current = values[-1]\n        previous = values[-2]\n        \n        if previous > 0:\n            return round(((current - previous) / previous) * 100, 2)\n        return 0\n        \n    def generate_monitoring_dashboard_data(self, priority_sites=None):\n        \"\"\"모니터링 대시보드 데이터 생성\"\"\"\n        logger.info("📊 모니터링 대시보드 데이터 생성...")\n        \n        dashboard_data = {\n            'timestamp': datetime.now().isoformat(),\n            'summary': {},\n            'priority_sites': [],\n            'recent_changes': [],\n            'alerts': [],\n            'trend_analysis': {}\n        }\n        \n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        \n        # 전체 요약 통계\n        cursor.execute('''\n            SELECT \n                COUNT(DISTINCT site_name) as total_sites,\n                SUM(players_online) as total_players,\n                AVG(players_online) as avg_players,\n                MAX(collection_date) as latest_date\n            FROM daily_traffic \n            WHERE collection_date = (SELECT MAX(collection_date) FROM daily_traffic)\n        ''')\n        \n        summary_result = cursor.fetchone()\n        if summary_result:\n            dashboard_data['summary'] = {\n                'total_sites_monitored': summary_result[0],\n                'total_players_online': summary_result[1] or 0,\n                'average_players_per_site': round(summary_result[2] or 0, 1),\n                'latest_data_date': summary_result[3]\n            }\n            \n        # 우선순위 사이트 데이터\n        if priority_sites:\n            for site_name in priority_sites:\n                site_data = self.get_latest_site_data(site_name)\n                if site_data:\n                    dashboard_data['priority_sites'].append(site_data)\n                    \n        # 최근 변화 (지난 3일)\n        cursor.execute('''\n            SELECT \n                site_name,\n                detection_date,\n                metric_type,\n                change_percentage,\n                change_magnitude\n            FROM change_detection \n            WHERE detection_date >= date('now', '-3 days')\n            AND change_magnitude IN ('SIGNIFICANT', 'MAJOR', 'ANOMALY')\n            ORDER BY detection_date DESC, change_percentage DESC\n            LIMIT 20\n        ''')\n        \n        recent_changes = cursor.fetchall()\n        dashboard_data['recent_changes'] = [\n            {\n                'site_name': row[0],\n                'date': row[1],\n                'metric': row[2],\n                'change_percentage': row[3],\n                'magnitude': row[4]\n            }\n            for row in recent_changes\n        ]\n        \n        # 알림 (ANOMALY 레벨 변화)\n        cursor.execute('''\n            SELECT DISTINCT\n                c.site_name,\n                c.detection_date,\n                c.metric_type,\n                c.change_percentage,\n                COUNT(n.id) as news_correlations\n            FROM change_detection c\n            LEFT JOIN news_correlation n ON c.id = n.change_detection_id\n            WHERE c.detection_date >= date('now', '-1 days')\n            AND c.change_magnitude = 'ANOMALY'\n            GROUP BY c.site_name, c.detection_date, c.metric_type, c.change_percentage\n            ORDER BY c.change_percentage DESC\n        ''')\n        \n        alerts = cursor.fetchall()\n        dashboard_data['alerts'] = [\n            {\n                'site_name': row[0],\n                'date': row[1],\n                'metric': row[2],\n                'change_percentage': row[3],\n                'news_correlations': row[4],\n                'severity': 'CRITICAL'\n            }\n            for row in alerts\n        ]\n        \n        conn.close()\n        return dashboard_data\n        \n    def get_latest_site_data(self, site_name):\n        \"\"\"특정 사이트의 최신 데이터 조회\"\"\"\n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        \n        cursor.execute('''\n            SELECT \n                collection_date,\n                collection_time,\n                players_online,\n                cash_players,\n                peak_24h,\n                seven_day_avg\n            FROM daily_traffic \n            WHERE site_name = ?\n            ORDER BY collection_date DESC, collection_time DESC\n            LIMIT 1\n        ''', (site_name,))\n        \n        result = cursor.fetchone()\n        \n        if result:\n            return {\n                'site_name': site_name,\n                'collection_date': result[0],\n                'collection_time': result[1],\n                'players_online': result[2],\n                'cash_players': result[3],\n                'peak_24h': result[4],\n                'seven_day_avg': result[5],\n                'cash_ratio': round((result[3] / result[2]) * 100, 1) if result[2] > 0 else 0\n            }\n            \n        conn.close()\n        return None\n        \n    def export_analysis_report(self, site_name=None, date_range_days=7):\n        \"\"\"분석 리포트 내보내기\"\"\"\n        logger.info(f\"📋 분석 리포트 생성 ({date_range_days}일간)\")\n        \n        report = {\n            'generated_at': datetime.now().isoformat(),\n            'analysis_period': f'{date_range_days} days',\n            'focus_site': site_name,\n            'executive_summary': {},\n            'detailed_analysis': {},\n            'change_events': [],\n            'news_correlations': [],\n            'recommendations': []\n        }\n        \n        if site_name:\n            # 특정 사이트 집중 분석\n            time_series = self.generate_time_series_analysis(site_name, date_range_days)\n            if time_series:\n                report['detailed_analysis'][site_name] = time_series\n                report['executive_summary'] = self.generate_executive_summary(time_series)\n                \n        else:\n            # 전체 시장 분석\n            dashboard_data = self.generate_monitoring_dashboard_data()\n            report['executive_summary'] = dashboard_data['summary']\n            report['change_events'] = dashboard_data['recent_changes']\n            report['alerts'] = dashboard_data['alerts']\n            \n        # 리포트 저장\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        filename = f'monitoring_report_{site_name or "market"}_{timestamp}.json'\n        \n        with open(filename, 'w', encoding='utf-8') as f:\n            json.dump(report, f, indent=2, ensure_ascii=False)\n            \n        logger.info(f\"📊 리포트 저장: {filename}\")\n        return filename\n        \n    def generate_executive_summary(self, time_series):\n        \"\"\"경영진 요약 생성\"\"\"\n        analytics = time_series.get('analytics', {})\n        \n        summary = {\n            'site_name': time_series['site_name'],\n            'analysis_period': time_series['period'],\n            'data_quality': 'GOOD' if time_series['data_points'] >= 7 else 'LIMITED',\n            'key_metrics': {},\n            'trends': {},\n            'risk_assessment': 'LOW'\n        }\n        \n        # 핵심 지표 추출\n        for metric in ['players_online', 'cash_players']:\n            if metric in analytics and 'no_data' not in analytics[metric]:\n                summary['key_metrics'][metric] = {\n                    'current': analytics[metric]['current'],\n                    'trend': analytics[metric]['trend'],\n                    'volatility': analytics[metric]['volatility'],\n                    'recent_change_pct': analytics[metric]['recent_change']\n                }\n                \n        # 위험도 평가\n        volatility_count = sum(1 for metric in ['players_online', 'cash_players'] \n                              if analytics.get(metric, {}).get('volatility') == 'HIGH')\n        if volatility_count >= 2:\n            summary['risk_assessment'] = 'HIGH'\n        elif volatility_count == 1:\n            summary['risk_assessment'] = 'MEDIUM'\n            \n        return summary\n        \n    def setup_competitor_monitoring(self):\n        \"\"\"경쟁사 모니터링 설정 (GG POKER 직원용)\"\"\"\n        logger.info("🎯 경쟁사 모니터링 설정...")\n        \n        competitor_sites = [\n            # Tier 1 직접 경쟁사\n            {'name': 'PokerStars', 'priority': 'HIGH', 'category': 'DIRECT'},\n            {'name': 'PokerStars Ontario', 'priority': 'HIGH', 'category': 'DIRECT'},\n            {'name': 'PokerStars.it', 'priority': 'MEDIUM', 'category': 'DIRECT'},\n            \n            # Tier 2 간접 경쟁사\n            {'name': '888poker', 'priority': 'MEDIUM', 'category': 'INDIRECT'},\n            {'name': 'partypoker', 'priority': 'MEDIUM', 'category': 'INDIRECT'},\n            {'name': 'WPT Global', 'priority': 'HIGH', 'category': 'INDIRECT'},\n            \n            # Tier 3 틈새 경쟁사\n            {'name': 'iPoker', 'priority': 'LOW', 'category': 'NICHE'},\n            {'name': 'Chico Poker', 'priority': 'LOW', 'category': 'NICHE'},\n        ]\n        \n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        \n        for site in competitor_sites:\n            cursor.execute('''\n                INSERT OR REPLACE INTO site_metadata \n                (site_name, monitoring_priority, competitor_category, notes)\n                VALUES (?, ?, ?, ?)\n            ''', (\n                site['name'],\n                site['priority'],\n                site['category'],\n                f\"GG POKER 경쟁사 모니터링 - {site['category']} 경쟁\"\n            ))\n            \n        conn.commit()\n        conn.close()\n        \n        logger.info(f\"✅ {len(competitor_sites)}개 경쟁사 모니터링 설정 완료\")\n        \n    def get_competitor_analysis(self):\n        \"\"\"경쟁사 분석 리포트\"\"\"\n        logger.info("📊 경쟁사 분석 리포트 생성...")\n        \n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        \n        # 우선순위별 경쟁사 최신 데이터\n        query = '''\n            SELECT \n                sm.site_name,\n                sm.monitoring_priority,\n                sm.competitor_category,\n                dt.players_online,\n                dt.cash_players,\n                dt.peak_24h,\n                dt.seven_day_avg,\n                dt.collection_date\n            FROM site_metadata sm\n            LEFT JOIN daily_traffic dt ON sm.site_name = dt.site_name\n            WHERE dt.collection_date = (SELECT MAX(collection_date) FROM daily_traffic)\n            AND sm.competitor_category IS NOT NULL\n            ORDER BY \n                CASE sm.monitoring_priority \n                    WHEN 'HIGH' THEN 1 \n                    WHEN 'MEDIUM' THEN 2 \n                    ELSE 3 END,\n                dt.players_online DESC\n        '''\n        \n        cursor.execute(query)\n        results = cursor.fetchall()\n        \n        competitor_analysis = {\n            'analysis_date': datetime.now().strftime('%Y-%m-%d'),\n            'high_priority': [],\n            'medium_priority': [],\n            'low_priority': [],\n            'market_summary': {\n                'total_competitors': len(results),\n                'total_players': sum(row[3] for row in results if row[3]),\n                'high_priority_share': 0\n            }\n        }\n        \n        high_priority_players = 0\n        \n        for row in results:\n            site_data = {\n                'site_name': row[0],\n                'category': row[2],\n                'players_online': row[3] or 0,\n                'cash_players': row[4] or 0,\n                'peak_24h': row[5] or 0,\n                'seven_day_avg': row[6] or 0,\n                'cash_ratio': round((row[4] / row[3]) * 100, 1) if row[3] and row[4] else 0,\n                'last_updated': row[7]\n            }\n            \n            priority = row[1]\n            if priority == 'HIGH':\n                competitor_analysis['high_priority'].append(site_data)\n                high_priority_players += site_data['players_online']\n            elif priority == 'MEDIUM':\n                competitor_analysis['medium_priority'].append(site_data)\n            else:\n                competitor_analysis['low_priority'].append(site_data)\n                \n        # 시장 점유율 계산\n        total_players = competitor_analysis['market_summary']['total_players']\n        if total_players > 0:\n            competitor_analysis['market_summary']['high_priority_share'] = \\\n                round((high_priority_players / total_players) * 100, 1)\n                \n        conn.close()\n        return competitor_analysis

def main():\n    \"\"\"메인 실행 함수\"\"\"\n    print(\"🎯 GG POKER 직원용 데이터 중심 모니터링 플랫폼\")\n    print(\"=\" * 70)\n    print(\"📊 4개 핵심 지표: Players Online, Cash Players, 24h Peak, 7-day Avg\")\n    print(\"📈 시계열 분석 및 급변 감지\")\n    print(\"📰 뉴스 연관성 자동 분석\")\n    print(\"🚨 실시간 경쟁사 모니터링\")\n    print(\"=\" * 70)\n    \n    platform = DataDrivenMonitoringPlatform()\n    \n    try:\n        # 1. 경쟁사 모니터링 설정\n        print(\"\\n🔧 경쟁사 모니터링 설정...\")\n        platform.setup_competitor_monitoring()\n        \n        # 2. 샘플 데이터 수집 (실제 환경에서는 크롤러에서 호출)\n        print(\"\\n📈 샘플 데이터 수집...\")\n        sample_data = [\n            {'site_name': 'PokerStars', 'players_online': 55540, 'cash_players': 1397, 'peak_24h': 62000, 'seven_day_avg': 58000},\n            {'site_name': 'GGPoker ON', 'players_online': 4693, 'cash_players': 563, 'peak_24h': 5200, 'seven_day_avg': 4500},\n            {'site_name': 'WPT Global', 'players_online': 2989, 'cash_players': 1596, 'peak_24h': 3500, 'seven_day_avg': 2800},\n            {'site_name': '888poker', 'players_online': 1850, 'cash_players': 420, 'peak_24h': 2100, 'seven_day_avg': 1900},\n        ]\n        \n        collected = platform.collect_daily_data(sample_data)\n        \n        # 3. 변화 감지 테스트를 위한 이전 날짜 데이터 추가\n        print(\"\\n🔄 이전 데이터 시뮬레이션...\")\n        # 실제로는 매일 수집된 데이터가 누적됨\n        \n        # 4. 경쟁사 분석\n        print(\"\\n📊 경쟁사 분석...\")\n        competitor_analysis = platform.get_competitor_analysis()\n        \n        print(f\"\\n✅ 분석 결과:\")\n        print(f\"모니터링 중인 경쟁사: {competitor_analysis['market_summary']['total_competitors']}개\")\n        print(f\"고우선순위 경쟁사 플레이어: {sum(site['players_online'] for site in competitor_analysis['high_priority']):,}명\")\n        \n        for site in competitor_analysis['high_priority']:\n            print(f\"  • {site['site_name']}: {site['players_online']:,}명 (캐시 {site['cash_ratio']}%)\")\n            \n        # 5. 시계열 분석 샘플\n        print(\"\\n📈 시계열 분석 샘플...\")\n        for site_name in ['PokerStars', 'WPT Global']:\n            time_series = platform.generate_time_series_analysis(site_name, 7)\n            if time_series:\n                analytics = time_series['analytics']\n                players_analytics = analytics.get('players_online', {})\n                if 'no_data' not in players_analytics:\n                    print(f\"  {site_name}: 현재 {players_analytics['current']:,}명, 트렌드 {players_analytics['trend']}\")\n                    \n        # 6. 대시보드 데이터 생성\n        print(\"\\n📊 모니터링 대시보드 데이터 생성...\")\n        dashboard = platform.generate_monitoring_dashboard_data(['PokerStars', 'WPT Global'])\n        \n        print(f\"대시보드 요약:\")\n        print(f\"  총 모니터링 사이트: {dashboard['summary']['total_sites_monitored']}개\")\n        print(f\"  총 온라인 플레이어: {dashboard['summary']['total_players_online']:,}명\")\n        print(f\"  우선순위 사이트: {len(dashboard['priority_sites'])}개\")\n        \n        # 7. 리포트 생성\n        print(\"\\n📋 분석 리포트 생성...\")\n        report_file = platform.export_analysis_report()\n        \n        print(f\"\\n🎯 데이터 중심 모니터링 플랫폼 설정 완료!\")\n        print(f\"📊 리포트 파일: {report_file}\")\n        print(f\"\")\n        print(f\"🚀 주요 기능:\")\n        print(f\"  ✅ 4개 핵심 지표 일일 수집\")\n        print(f\"  ✅ 시계열 트렌드 분석\")\n        print(f\"  ✅ 급변 시점 자동 감지\")\n        print(f\"  ✅ 뉴스 연관성 분석\")\n        print(f\"  ✅ 경쟁사 모니터링\")\n        print(f\"  ✅ 실시간 대시보드\")\n        \n        return True\n        \n    except Exception as e:\n        logger.error(f\"플랫폼 실행 오류: {str(e)}\")\n        return False\n\nif __name__ == \"__main__\":\n    success = main()\n    if success:\n        print(f\"\\n🎉 GG POKER용 데이터 모니터링 플랫폼 완성!\")\n        print(f\"매일 자동 수집 → 급변 감지 → 뉴스 분석 → 인사이트 도출\")\n    else:\n        print(f\"\\n💀 플랫폼 설정 실패 - 문제 해결 필요\")