#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GG POKER ì§ì›ìš© ë°ì´í„° ì¤‘ì‹¬ ì˜¨ë¼ì¸ í¬ì»¤ ëª¨ë‹ˆí„°ë§ í”Œë«í¼
- 4ê°œ í•µì‹¬ ì§€í‘œ: Players Online, Cash Players, 24h Peak, 7-day Average
- ë‚ ì§œë³„ ì‹œê³„ì—´ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„
- ê¸‰ë³€ ì‹œì ì˜ ë‰´ìŠ¤ ì—°ê´€ì„± ë¶„ì„
- ì¶”ë¡  ë°°ì œ, ìˆœìˆ˜ ë°ì´í„° ê¸°ë°˜ ë¶„ì„
"""
import sys
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')

import json
import logging
import sqlite3
from datetime import datetime, timedelta
from collections import defaultdict, Counter
import statistics
import math

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataDrivenMonitoringPlatform:
    def __init__(self, db_path='poker_monitoring.db'):
        self.db_path = db_path
        self.setup_database()
        
        # ê¸‰ë³€ ê°ì§€ ì„ê³„ê°’ (ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ì¡°ì • í•„ìš”)
        self.SIGNIFICANT_CHANGE_THRESHOLD = 15.0  # 15% ë³€í™”
        self.MAJOR_CHANGE_THRESHOLD = 25.0        # 25% ì£¼ìš” ë³€í™”
        self.ANOMALY_THRESHOLD = 50.0             # 50% ì´ìƒì¹˜
        
    def setup_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì„¤ì •"""
        logger.info("ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì„¤ì •...")
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ì¼ì¼ íŠ¸ë˜í”½ ë°ì´í„° í…Œì´ë¸” (ì‹œê³„ì—´)
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS daily_traffic (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                site_name TEXT NOT NULL,
                collection_date DATE NOT NULL,
                collection_time TIME NOT NULL,
                players_online INTEGER NOT NULL,
                cash_players INTEGER NOT NULL,
                peak_24h INTEGER,
                seven_day_avg INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(site_name, collection_date, collection_time)
            )
        ''')
        
        # ë³€í™” ê°ì§€ ë¡œê·¸ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS change_detection (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                site_name TEXT NOT NULL,\n                detection_date DATE NOT NULL,\n                metric_type TEXT NOT NULL,  -- 'players_online', 'cash_players', 'peak_24h', 'seven_day_avg'\n                previous_value INTEGER,\n                current_value INTEGER,\n                change_percentage REAL,\n                change_magnitude TEXT,  -- 'MINOR', 'SIGNIFICANT', 'MAJOR', 'ANOMALY'\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            )\n        ''')\n        \n        # ë‰´ìŠ¤-ë³€í™” ì—°ê´€ì„± í…Œì´ë¸”\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS news_correlation (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                change_detection_id INTEGER,\n                news_title TEXT,\n                news_url TEXT,\n                news_date DATE,\n                correlation_score REAL,  -- 0-1 ìŠ¤ì½”ì–´\n                correlation_type TEXT,   -- 'DIRECT', 'INDIRECT', 'TEMPORAL'\n                analysis_notes TEXT,\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                FOREIGN KEY (change_detection_id) REFERENCES change_detection (id)\n            )\n        ''')\n        \n        # ì‚¬ì´íŠ¸ ë©”íƒ€ë°ì´í„° í…Œì´ë¸”\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS site_metadata (\n                site_name TEXT PRIMARY KEY,\n                site_url TEXT,\n                network_family TEXT,     -- 'GGNetwork', 'PokerStars', 'iPoker', 'Independent'\n                market_tier TEXT,        -- 'Tier1', 'Tier2', 'Tier3'\n                monitoring_priority TEXT, -- 'HIGH', 'MEDIUM', 'LOW'\n                competitor_category TEXT, -- 'DIRECT', 'INDIRECT', 'NICHE'\n                notes TEXT,\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            )\n        ''')\n        \n        conn.commit()\n        conn.close()\n        logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì„¤ì • ì™„ë£Œ")\n        \n    def collect_daily_data(self, site_data_list):\n        """ì¼ì¼ ë°ì´í„° ìˆ˜ì§‘ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” í¬ë¡¤ëŸ¬ê°€ í˜¸ì¶œ)"""\n        logger.info("ğŸ“ˆ ì¼ì¼ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")\n        \n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        \n        collection_date = datetime.now().strftime('%Y-%m-%d')\n        collection_time = datetime.now().strftime('%H:%M:%S')\n        \n        collected_count = 0\n        \n        for site_data in site_data_list:\n            try:\n                cursor.execute('''\n                    INSERT OR REPLACE INTO daily_traffic \n                    (site_name, collection_date, collection_time, players_online, \n                     cash_players, peak_24h, seven_day_avg)\n                    VALUES (?, ?, ?, ?, ?, ?, ?)\n                ''', (\n                    site_data['site_name'],\n                    collection_date,\n                    collection_time,\n                    site_data['players_online'],\n                    site_data['cash_players'],\n                    site_data.get('peak_24h', None),\n                    site_data.get('seven_day_avg', None)\n                ))\n                \n                collected_count += 1\n                \n            except Exception as e:\n                logger.error(f"ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜ - {site_data['site_name']}: {str(e)}")\n                \n        conn.commit()\n        conn.close()\n        \n        logger.info(f"âœ… {collected_count}ê°œ ì‚¬ì´íŠ¸ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")\n        return collected_count\n        \n    def detect_significant_changes(self, target_date=None):\n        """ìœ ì˜ë¯¸í•œ ë³€í™” ê°ì§€"""\n        if target_date is None:\n            target_date = datetime.now().strftime('%Y-%m-%d')\n            \n        logger.info(f"ğŸ” {target_date} ë³€í™” ê°ì§€ ì‹œì‘...")\n        \n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        \n        # í˜„ì¬ ë‚ ì§œì™€ ì´ì „ ë‚ ì§œ ë°ì´í„° ë¹„êµ\n        query = '''\n            WITH current_data AS (\n                SELECT site_name, players_online, cash_players, peak_24h, seven_day_avg\n                FROM daily_traffic \n                WHERE collection_date = ?\n                ORDER BY collection_time DESC\n                LIMIT 50\n            ),\n            previous_data AS (\n                SELECT site_name, players_online, cash_players, peak_24h, seven_day_avg\n                FROM daily_traffic \n                WHERE collection_date = date(?, '-1 day')\n                ORDER BY collection_time DESC\n                LIMIT 50\n            )\n            SELECT \n                c.site_name,\n                c.players_online as current_players,\n                p.players_online as previous_players,\n                c.cash_players as current_cash,\n                p.cash_players as previous_cash,\n                c.peak_24h as current_peak,\n                p.peak_24h as previous_peak,\n                c.seven_day_avg as current_7day,\n                p.seven_day_avg as previous_7day\n            FROM current_data c\n            LEFT JOIN previous_data p ON c.site_name = p.site_name\n            WHERE p.site_name IS NOT NULL\n        '''\n        \n        cursor.execute(query, (target_date, target_date))\n        results = cursor.fetchall()\n        \n        detected_changes = []\n        \n        for row in results:\n            site_name = row[0]\n            changes = self.analyze_site_changes(row)\n            \n            for change in changes:\n                if change['magnitude'] in ['SIGNIFICANT', 'MAJOR', 'ANOMALY']:\n                    # ë³€í™” ê°ì§€ ë¡œê·¸ì— ì €ì¥\n                    cursor.execute('''\n                        INSERT INTO change_detection \n                        (site_name, detection_date, metric_type, previous_value, \n                         current_value, change_percentage, change_magnitude)\n                        VALUES (?, ?, ?, ?, ?, ?, ?)\n                    ''', (\n                        site_name,\n                        target_date,\n                        change['metric'],\n                        change['previous_value'],\n                        change['current_value'],\n                        change['change_percentage'],\n                        change['magnitude']\n                    ))\n                    \n                    change['change_id'] = cursor.lastrowid\n                    detected_changes.append(change)\n                    \n        conn.commit()\n        conn.close()\n        \n        logger.info(f"ğŸš¨ {len(detected_changes)}ê°œ ìœ ì˜ë¯¸í•œ ë³€í™” ê°ì§€")\n        return detected_changes\n        \n    def analyze_site_changes(self, data_row):\n        """ì‚¬ì´íŠ¸ë³„ ë³€í™” ë¶„ì„"""\n        site_name, curr_players, prev_players, curr_cash, prev_cash, \\\n        curr_peak, prev_peak, curr_7day, prev_7day = data_row\n        \n        changes = []\n        \n        # ê° ì§€í‘œë³„ ë³€í™”ìœ¨ ê³„ì‚°\n        metrics = [\n            ('players_online', curr_players, prev_players),\n            ('cash_players', curr_cash, prev_cash),\n            ('peak_24h', curr_peak, prev_peak),\n            ('seven_day_avg', curr_7day, prev_7day)\n        ]\n        \n        for metric_name, current, previous in metrics:\n            if current is not None and previous is not None and previous > 0:\n                change_percentage = ((current - previous) / previous) * 100\n                magnitude = self.classify_change_magnitude(abs(change_percentage))\n                \n                changes.append({\n                    'site_name': site_name,\n                    'metric': metric_name,\n                    'previous_value': previous,\n                    'current_value': current,\n                    'change_percentage': round(change_percentage, 2),\n                    'magnitude': magnitude,\n                    'direction': 'INCREASE' if change_percentage > 0 else 'DECREASE'\n                })\n                \n        return changes\n        \n    def classify_change_magnitude(self, abs_change_percentage):\n        """ë³€í™” í¬ê¸° ë¶„ë¥˜"""\n        if abs_change_percentage >= self.ANOMALY_THRESHOLD:\n            return 'ANOMALY'\n        elif abs_change_percentage >= self.MAJOR_CHANGE_THRESHOLD:\n            return 'MAJOR'\n        elif abs_change_percentage >= self.SIGNIFICANT_CHANGE_THRESHOLD:\n            return 'SIGNIFICANT'\n        else:\n            return 'MINOR'\n            \n    def analyze_news_correlation(self, detected_changes, news_data):\n        """ë‰´ìŠ¤ ì—°ê´€ì„± ë¶„ì„"""\n        logger.info("ğŸ“° ë‰´ìŠ¤ ì—°ê´€ì„± ë¶„ì„ ì‹œì‘...")\n        \n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        \n        correlations = []\n        \n        for change in detected_changes:\n            site_name = change['site_name']\n            change_date = change.get('detection_date', datetime.now().strftime('%Y-%m-%d'))\n            \n            # í•´ë‹¹ ì‚¬ì´íŠ¸ ê´€ë ¨ ë‰´ìŠ¤ ì°¾ê¸°\n            relevant_news = self.find_relevant_news(site_name, change_date, news_data)\n            \n            for news_item in relevant_news:\n                correlation_score = self.calculate_correlation_score(\n                    change, news_item, change_date\n                )\n                \n                if correlation_score >= 0.3:  # ì„ê³„ê°’ ì´ìƒë§Œ ì €ì¥\n                    cursor.execute('''\n                        INSERT INTO news_correlation \n                        (change_detection_id, news_title, news_url, news_date,\n                         correlation_score, correlation_type, analysis_notes)\n                        VALUES (?, ?, ?, ?, ?, ?, ?)\n                    ''', (\n                        change.get('change_id'),\n                        news_item['title'],\n                        news_item.get('url', ''),\n                        news_item.get('published_date', change_date),\n                        correlation_score,\n                        news_item['correlation_type'],\n                        news_item.get('analysis_notes', '')\n                    ))\n                    \n                    correlations.append({\n                        'change': change,\n                        'news': news_item,\n                        'correlation_score': correlation_score\n                    })\n                    \n        conn.commit()\n        conn.close()\n        \n        logger.info(f"ğŸ”— {len(correlations)}ê°œ ë‰´ìŠ¤-ë³€í™” ì—°ê´€ì„± ë°œê²¬")\n        return correlations\n        \n    def find_relevant_news(self, site_name, change_date, news_data):\n        """ê´€ë ¨ ë‰´ìŠ¤ ì°¾ê¸°"""\n        relevant_news = []\n        \n        # ë‚ ì§œ ë²”ìœ„ ì„¤ì • (ë³€í™” ì „í›„ 3ì¼)\n        target_date = datetime.strptime(change_date, '%Y-%m-%d')\n        date_range_start = target_date - timedelta(days=3)\n        date_range_end = target_date + timedelta(days=1)\n        \n        # ì‚¬ì´íŠ¸ëª… íŒ¨í„´ ìƒì„±\n        site_patterns = self.generate_site_search_patterns(site_name)\n        \n        for news_item in news_data:\n            news_date_str = news_item.get('published_date', '')\n            \n            # ë‚ ì§œ í•„í„°ë§\n            try:\n                if news_date_str:\n                    news_date = datetime.strptime(news_date_str[:10], '%Y-%m-%d')\n                    if not (date_range_start <= news_date <= date_range_end):\n                        continue\n            except:\n                continue\n                \n            # ì‚¬ì´íŠ¸ ê´€ë ¨ì„± ì²´í¬\n            title = news_item.get('title', '').lower()\n            content = news_item.get('content', '').lower()\n            text = title + ' ' + content\n            \n            correlation_type = None\n            analysis_notes = []\n            \n            # ì§ì ‘ ì–¸ê¸‰ ì²´í¬\n            for pattern in site_patterns:\n                if pattern.lower() in text:\n                    correlation_type = 'DIRECT'\n                    analysis_notes.append(f'ì§ì ‘ ì–¸ê¸‰: {pattern}')\n                    break\n                    \n            # ê°„ì ‘ ì—°ê´€ì„± ì²´í¬ (ì§ì ‘ ì–¸ê¸‰ì´ ì—†ëŠ” ê²½ìš°)\n            if not correlation_type:\n                indirect_keywords = self.get_indirect_keywords(site_name)\n                found_keywords = []\n                \n                for keyword in indirect_keywords:\n                    if keyword.lower() in text:\n                        found_keywords.append(keyword)\n                        \n                if found_keywords:\n                    correlation_type = 'INDIRECT'\n                    analysis_notes.append(f'ê°„ì ‘ ì—°ê´€: {", ".join(found_keywords)}')\n                    \n            # ì‹œê°„ì  ì—°ê´€ì„± (ê°™ì€ ë‚ ì§œ)\n            if not correlation_type and news_date.date() == target_date.date():\n                market_keywords = ['poker', 'online', 'tournament', 'cash game', 'promotion']\n                if any(keyword in text for keyword in market_keywords):\n                    correlation_type = 'TEMPORAL'\n                    analysis_notes.append('ì‹œê°„ì  ì—°ê´€ì„±')\n                    \n            if correlation_type:\n                news_item_copy = news_item.copy()\n                news_item_copy['correlation_type'] = correlation_type\n                news_item_copy['analysis_notes'] = '; '.join(analysis_notes)\n                relevant_news.append(news_item_copy)\n                \n        return relevant_news\n        \n    def generate_site_search_patterns(self, site_name):\n        """ì‚¬ì´íŠ¸ ê²€ìƒ‰ íŒ¨í„´ ìƒì„±"""\n        patterns = [site_name]\n        \n        # ë„¤íŠ¸ì›Œí¬ë³„ ì¶”ê°€ íŒ¨í„´\n        if 'GG' in site_name.upper():\n            patterns.extend(['GGPoker', 'GG Poker', 'GGNetwork', 'GG Network'])\n        elif 'POKERSTARS' in site_name.upper():\n            patterns.extend(['PokerStars', 'Poker Stars', 'Stars'])\n        elif 'WPT' in site_name.upper():\n            patterns.extend(['WPT Global', 'World Poker Tour'])\n        elif 'IPOKER' in site_name.upper():\n            patterns.extend(['iPoker', 'iPoker Network'])\n            \n        return patterns\n        \n    def get_indirect_keywords(self, site_name):\n        """ê°„ì ‘ ì—°ê´€ í‚¤ì›Œë“œ íšë“"""\n        base_keywords = ['tournament', 'promotion', 'bonus', 'update', 'partnership']\n        \n        # ë„¤íŠ¸ì›Œí¬ë³„ íŠ¹í™” í‚¤ì›Œë“œ\n        if 'GG' in site_name.upper():\n            base_keywords.extend(['WSOP', 'bracelet', 'GG Masters'])\n        elif 'POKERSTARS' in site_name.upper():\n            base_keywords.extend(['SCOOP', 'WCOOP', 'EPT', 'Sunday Million'])\n        elif 'WPT' in site_name.upper():\n            base_keywords.extend(['WPT', 'World Poker Tour', 'WPT500'])\n            \n        return base_keywords\n        \n    def calculate_correlation_score(self, change, news_item, change_date):\n        """ì—°ê´€ì„± ì ìˆ˜ ê³„ì‚° (0-1)"""\n        score = 0.0\n        \n        # ê¸°ë³¸ ì ìˆ˜ (ì—°ê´€ì„± íƒ€ì…ë³„)\n        correlation_type = news_item.get('correlation_type', '')\n        if correlation_type == 'DIRECT':\n            score += 0.6\n        elif correlation_type == 'INDIRECT':\n            score += 0.3\n        elif correlation_type == 'TEMPORAL':\n            score += 0.1\n            \n        # ë‚ ì§œ ê·¼ì ‘ì„± ë³´ë„ˆìŠ¤\n        try:\n            news_date_str = news_item.get('published_date', '')\n            if news_date_str:\n                news_date = datetime.strptime(news_date_str[:10], '%Y-%m-%d')\n                change_date_obj = datetime.strptime(change_date, '%Y-%m-%d')\n                days_diff = abs((news_date - change_date_obj).days)\n                \n                if days_diff == 0:\n                    score += 0.3\n                elif days_diff == 1:\n                    score += 0.2\n                elif days_diff <= 2:\n                    score += 0.1\n        except:\n            pass\n            \n        # ë³€í™” í¬ê¸° ë³´ë„ˆìŠ¤\n        change_magnitude = change.get('magnitude', '')\n        if change_magnitude == 'ANOMALY':\n            score += 0.2\n        elif change_magnitude == 'MAJOR':\n            score += 0.1\n            \n        # í‚¤ì›Œë“œ ë°€ë„ ë³´ë„ˆìŠ¤\n        title = news_item.get('title', '').lower()\n        content = news_item.get('content', '').lower()\n        \n        important_keywords = ['launch', 'new', 'update', 'promotion', 'tournament', 'bonus']\n        keyword_count = sum(1 for keyword in important_keywords if keyword in title + content)\n        score += min(keyword_count * 0.05, 0.15)\n        \n        return min(score, 1.0)\n        \n    def generate_time_series_analysis(self, site_name, days_back=30):\n        \"\"\"ì‹œê³„ì—´ ë¶„ì„ ìƒì„±\"\"\"\n        logger.info(f\"ğŸ“Š {site_name} ì‹œê³„ì—´ ë¶„ì„ ({days_back}ì¼)\")\n        \n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        \n        # ì¼ë³„ ìµœì‹  ë°ì´í„° ì¡°íšŒ\n        query = '''\n            SELECT \n                collection_date,\n                players_online,\n                cash_players,\n                peak_24h,\n                seven_day_avg\n            FROM daily_traffic \n            WHERE site_name = ? \n            AND collection_date >= date('now', '-' || ? || ' days')\n            GROUP BY collection_date\n            HAVING collection_time = MAX(collection_time)\n            ORDER BY collection_date\n        '''\n        \n        cursor.execute(query, (site_name, days_back))\n        results = cursor.fetchall()\n        \n        if not results:\n            return None\n            \n        # ì‹œê³„ì—´ ë°ì´í„° êµ¬ì„±\n        time_series = {\n            'site_name': site_name,\n            'period': f'{days_back} days',\n            'data_points': len(results),\n            'dates': [],\n            'players_online': [],\n            'cash_players': [],\n            'peak_24h': [],\n            'seven_day_avg': [],\n            'analytics': {}\n        }\n        \n        for row in results:\n            time_series['dates'].append(row[0])\n            time_series['players_online'].append(row[1])\n            time_series['cash_players'].append(row[2])\n            time_series['peak_24h'].append(row[3] if row[3] is not None else 0)\n            time_series['seven_day_avg'].append(row[4] if row[4] is not None else 0)\n            \n        # ê¸°ë³¸ í†µê³„ ê³„ì‚°\n        time_series['analytics'] = self.calculate_time_series_analytics(time_series)\n        \n        conn.close()\n        return time_series\n        \n    def calculate_time_series_analytics(self, time_series):\n        \"\"\"ì‹œê³„ì—´ ê¸°ë³¸ í†µê³„ ê³„ì‚°\"\"\"\n        analytics = {}\n        \n        for metric in ['players_online', 'cash_players', 'peak_24h', 'seven_day_avg']:\n            values = [v for v in time_series[metric] if v is not None and v > 0]\n            \n            if values:\n                analytics[metric] = {\n                    'current': values[-1] if values else 0,\n                    'min': min(values),\n                    'max': max(values),\n                    'mean': round(statistics.mean(values), 1),\n                    'median': round(statistics.median(values), 1),\n                    'std_dev': round(statistics.stdev(values), 1) if len(values) > 1 else 0,\n                    'trend': self.calculate_trend(values),\n                    'volatility': self.calculate_volatility(values),\n                    'recent_change': self.calculate_recent_change(values)\n                }\n            else:\n                analytics[metric] = {'no_data': True}\n                \n        return analytics\n        \n    def calculate_trend(self, values):\n        \"\"\"íŠ¸ë Œë“œ ê³„ì‚° (ë‹¨ìˆœ ì„ í˜• íšŒê·€)\"\"\"\n        if len(values) < 3:\n            return 'INSUFFICIENT_DATA'\n            \n        # ìµœê·¼ 7ì¼ vs ì´ì „ ê¸°ê°„ ë¹„êµ\n        if len(values) >= 7:\n            recent_avg = statistics.mean(values[-7:])\n            previous_avg = statistics.mean(values[:-7]) if len(values) > 7 else statistics.mean(values)\n            \n            if recent_avg > previous_avg * 1.05:\n                return 'UPWARD'\n            elif recent_avg < previous_avg * 0.95:\n                return 'DOWNWARD'\n            else:\n                return 'STABLE'\n        else:\n            # ì²«ê°’ vs ë§ˆì§€ë§‰ê°’\n            if values[-1] > values[0] * 1.1:\n                return 'UPWARD'\n            elif values[-1] < values[0] * 0.9:\n                return 'DOWNWARD'\n            else:\n                return 'STABLE'\n                \n    def calculate_volatility(self, values):\n        \"\"\"ë³€ë™ì„± ê³„ì‚°\"\"\"\n        if len(values) < 2:\n            return 'LOW'\n            \n        mean_val = statistics.mean(values)\n        std_dev = statistics.stdev(values)\n        cv = (std_dev / mean_val) * 100 if mean_val > 0 else 0\n        \n        if cv >= 20:\n            return 'HIGH'\n        elif cv >= 10:\n            return 'MEDIUM'\n        else:\n            return 'LOW'\n            \n    def calculate_recent_change(self, values):\n        \"\"\"ìµœê·¼ ë³€í™”ìœ¨ ê³„ì‚°\"\"\"\n        if len(values) < 2:\n            return 0\n            \n        current = values[-1]\n        previous = values[-2]\n        \n        if previous > 0:\n            return round(((current - previous) / previous) * 100, 2)\n        return 0\n        \n    def generate_monitoring_dashboard_data(self, priority_sites=None):\n        \"\"\"ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„±\"\"\"\n        logger.info("ğŸ“Š ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„±...")\n        \n        dashboard_data = {\n            'timestamp': datetime.now().isoformat(),\n            'summary': {},\n            'priority_sites': [],\n            'recent_changes': [],\n            'alerts': [],\n            'trend_analysis': {}\n        }\n        \n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        \n        # ì „ì²´ ìš”ì•½ í†µê³„\n        cursor.execute('''\n            SELECT \n                COUNT(DISTINCT site_name) as total_sites,\n                SUM(players_online) as total_players,\n                AVG(players_online) as avg_players,\n                MAX(collection_date) as latest_date\n            FROM daily_traffic \n            WHERE collection_date = (SELECT MAX(collection_date) FROM daily_traffic)\n        ''')\n        \n        summary_result = cursor.fetchone()\n        if summary_result:\n            dashboard_data['summary'] = {\n                'total_sites_monitored': summary_result[0],\n                'total_players_online': summary_result[1] or 0,\n                'average_players_per_site': round(summary_result[2] or 0, 1),\n                'latest_data_date': summary_result[3]\n            }\n            \n        # ìš°ì„ ìˆœìœ„ ì‚¬ì´íŠ¸ ë°ì´í„°\n        if priority_sites:\n            for site_name in priority_sites:\n                site_data = self.get_latest_site_data(site_name)\n                if site_data:\n                    dashboard_data['priority_sites'].append(site_data)\n                    \n        # ìµœê·¼ ë³€í™” (ì§€ë‚œ 3ì¼)\n        cursor.execute('''\n            SELECT \n                site_name,\n                detection_date,\n                metric_type,\n                change_percentage,\n                change_magnitude\n            FROM change_detection \n            WHERE detection_date >= date('now', '-3 days')\n            AND change_magnitude IN ('SIGNIFICANT', 'MAJOR', 'ANOMALY')\n            ORDER BY detection_date DESC, change_percentage DESC\n            LIMIT 20\n        ''')\n        \n        recent_changes = cursor.fetchall()\n        dashboard_data['recent_changes'] = [\n            {\n                'site_name': row[0],\n                'date': row[1],\n                'metric': row[2],\n                'change_percentage': row[3],\n                'magnitude': row[4]\n            }\n            for row in recent_changes\n        ]\n        \n        # ì•Œë¦¼ (ANOMALY ë ˆë²¨ ë³€í™”)\n        cursor.execute('''\n            SELECT DISTINCT\n                c.site_name,\n                c.detection_date,\n                c.metric_type,\n                c.change_percentage,\n                COUNT(n.id) as news_correlations\n            FROM change_detection c\n            LEFT JOIN news_correlation n ON c.id = n.change_detection_id\n            WHERE c.detection_date >= date('now', '-1 days')\n            AND c.change_magnitude = 'ANOMALY'\n            GROUP BY c.site_name, c.detection_date, c.metric_type, c.change_percentage\n            ORDER BY c.change_percentage DESC\n        ''')\n        \n        alerts = cursor.fetchall()\n        dashboard_data['alerts'] = [\n            {\n                'site_name': row[0],\n                'date': row[1],\n                'metric': row[2],\n                'change_percentage': row[3],\n                'news_correlations': row[4],\n                'severity': 'CRITICAL'\n            }\n            for row in alerts\n        ]\n        \n        conn.close()\n        return dashboard_data\n        \n    def get_latest_site_data(self, site_name):\n        \"\"\"íŠ¹ì • ì‚¬ì´íŠ¸ì˜ ìµœì‹  ë°ì´í„° ì¡°íšŒ\"\"\"\n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        \n        cursor.execute('''\n            SELECT \n                collection_date,\n                collection_time,\n                players_online,\n                cash_players,\n                peak_24h,\n                seven_day_avg\n            FROM daily_traffic \n            WHERE site_name = ?\n            ORDER BY collection_date DESC, collection_time DESC\n            LIMIT 1\n        ''', (site_name,))\n        \n        result = cursor.fetchone()\n        \n        if result:\n            return {\n                'site_name': site_name,\n                'collection_date': result[0],\n                'collection_time': result[1],\n                'players_online': result[2],\n                'cash_players': result[3],\n                'peak_24h': result[4],\n                'seven_day_avg': result[5],\n                'cash_ratio': round((result[3] / result[2]) * 100, 1) if result[2] > 0 else 0\n            }\n            \n        conn.close()\n        return None\n        \n    def export_analysis_report(self, site_name=None, date_range_days=7):\n        \"\"\"ë¶„ì„ ë¦¬í¬íŠ¸ ë‚´ë³´ë‚´ê¸°\"\"\"\n        logger.info(f\"ğŸ“‹ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ({date_range_days}ì¼ê°„)\")\n        \n        report = {\n            'generated_at': datetime.now().isoformat(),\n            'analysis_period': f'{date_range_days} days',\n            'focus_site': site_name,\n            'executive_summary': {},\n            'detailed_analysis': {},\n            'change_events': [],\n            'news_correlations': [],\n            'recommendations': []\n        }\n        \n        if site_name:\n            # íŠ¹ì • ì‚¬ì´íŠ¸ ì§‘ì¤‘ ë¶„ì„\n            time_series = self.generate_time_series_analysis(site_name, date_range_days)\n            if time_series:\n                report['detailed_analysis'][site_name] = time_series\n                report['executive_summary'] = self.generate_executive_summary(time_series)\n                \n        else:\n            # ì „ì²´ ì‹œì¥ ë¶„ì„\n            dashboard_data = self.generate_monitoring_dashboard_data()\n            report['executive_summary'] = dashboard_data['summary']\n            report['change_events'] = dashboard_data['recent_changes']\n            report['alerts'] = dashboard_data['alerts']\n            \n        # ë¦¬í¬íŠ¸ ì €ì¥\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        filename = f'monitoring_report_{site_name or "market"}_{timestamp}.json'\n        \n        with open(filename, 'w', encoding='utf-8') as f:\n            json.dump(report, f, indent=2, ensure_ascii=False)\n            \n        logger.info(f\"ğŸ“Š ë¦¬í¬íŠ¸ ì €ì¥: {filename}\")\n        return filename\n        \n    def generate_executive_summary(self, time_series):\n        \"\"\"ê²½ì˜ì§„ ìš”ì•½ ìƒì„±\"\"\"\n        analytics = time_series.get('analytics', {})\n        \n        summary = {\n            'site_name': time_series['site_name'],\n            'analysis_period': time_series['period'],\n            'data_quality': 'GOOD' if time_series['data_points'] >= 7 else 'LIMITED',\n            'key_metrics': {},\n            'trends': {},\n            'risk_assessment': 'LOW'\n        }\n        \n        # í•µì‹¬ ì§€í‘œ ì¶”ì¶œ\n        for metric in ['players_online', 'cash_players']:\n            if metric in analytics and 'no_data' not in analytics[metric]:\n                summary['key_metrics'][metric] = {\n                    'current': analytics[metric]['current'],\n                    'trend': analytics[metric]['trend'],\n                    'volatility': analytics[metric]['volatility'],\n                    'recent_change_pct': analytics[metric]['recent_change']\n                }\n                \n        # ìœ„í—˜ë„ í‰ê°€\n        volatility_count = sum(1 for metric in ['players_online', 'cash_players'] \n                              if analytics.get(metric, {}).get('volatility') == 'HIGH')\n        if volatility_count >= 2:\n            summary['risk_assessment'] = 'HIGH'\n        elif volatility_count == 1:\n            summary['risk_assessment'] = 'MEDIUM'\n            \n        return summary\n        \n    def setup_competitor_monitoring(self):\n        \"\"\"ê²½ìŸì‚¬ ëª¨ë‹ˆí„°ë§ ì„¤ì • (GG POKER ì§ì›ìš©)\"\"\"\n        logger.info("ğŸ¯ ê²½ìŸì‚¬ ëª¨ë‹ˆí„°ë§ ì„¤ì •...")\n        \n        competitor_sites = [\n            # Tier 1 ì§ì ‘ ê²½ìŸì‚¬\n            {'name': 'PokerStars', 'priority': 'HIGH', 'category': 'DIRECT'},\n            {'name': 'PokerStars Ontario', 'priority': 'HIGH', 'category': 'DIRECT'},\n            {'name': 'PokerStars.it', 'priority': 'MEDIUM', 'category': 'DIRECT'},\n            \n            # Tier 2 ê°„ì ‘ ê²½ìŸì‚¬\n            {'name': '888poker', 'priority': 'MEDIUM', 'category': 'INDIRECT'},\n            {'name': 'partypoker', 'priority': 'MEDIUM', 'category': 'INDIRECT'},\n            {'name': 'WPT Global', 'priority': 'HIGH', 'category': 'INDIRECT'},\n            \n            # Tier 3 í‹ˆìƒˆ ê²½ìŸì‚¬\n            {'name': 'iPoker', 'priority': 'LOW', 'category': 'NICHE'},\n            {'name': 'Chico Poker', 'priority': 'LOW', 'category': 'NICHE'},\n        ]\n        \n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        \n        for site in competitor_sites:\n            cursor.execute('''\n                INSERT OR REPLACE INTO site_metadata \n                (site_name, monitoring_priority, competitor_category, notes)\n                VALUES (?, ?, ?, ?)\n            ''', (\n                site['name'],\n                site['priority'],\n                site['category'],\n                f\"GG POKER ê²½ìŸì‚¬ ëª¨ë‹ˆí„°ë§ - {site['category']} ê²½ìŸ\"\n            ))\n            \n        conn.commit()\n        conn.close()\n        \n        logger.info(f\"âœ… {len(competitor_sites)}ê°œ ê²½ìŸì‚¬ ëª¨ë‹ˆí„°ë§ ì„¤ì • ì™„ë£Œ\")\n        \n    def get_competitor_analysis(self):\n        \"\"\"ê²½ìŸì‚¬ ë¶„ì„ ë¦¬í¬íŠ¸\"\"\"\n        logger.info("ğŸ“Š ê²½ìŸì‚¬ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±...")\n        \n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        \n        # ìš°ì„ ìˆœìœ„ë³„ ê²½ìŸì‚¬ ìµœì‹  ë°ì´í„°\n        query = '''\n            SELECT \n                sm.site_name,\n                sm.monitoring_priority,\n                sm.competitor_category,\n                dt.players_online,\n                dt.cash_players,\n                dt.peak_24h,\n                dt.seven_day_avg,\n                dt.collection_date\n            FROM site_metadata sm\n            LEFT JOIN daily_traffic dt ON sm.site_name = dt.site_name\n            WHERE dt.collection_date = (SELECT MAX(collection_date) FROM daily_traffic)\n            AND sm.competitor_category IS NOT NULL\n            ORDER BY \n                CASE sm.monitoring_priority \n                    WHEN 'HIGH' THEN 1 \n                    WHEN 'MEDIUM' THEN 2 \n                    ELSE 3 END,\n                dt.players_online DESC\n        '''\n        \n        cursor.execute(query)\n        results = cursor.fetchall()\n        \n        competitor_analysis = {\n            'analysis_date': datetime.now().strftime('%Y-%m-%d'),\n            'high_priority': [],\n            'medium_priority': [],\n            'low_priority': [],\n            'market_summary': {\n                'total_competitors': len(results),\n                'total_players': sum(row[3] for row in results if row[3]),\n                'high_priority_share': 0\n            }\n        }\n        \n        high_priority_players = 0\n        \n        for row in results:\n            site_data = {\n                'site_name': row[0],\n                'category': row[2],\n                'players_online': row[3] or 0,\n                'cash_players': row[4] or 0,\n                'peak_24h': row[5] or 0,\n                'seven_day_avg': row[6] or 0,\n                'cash_ratio': round((row[4] / row[3]) * 100, 1) if row[3] and row[4] else 0,\n                'last_updated': row[7]\n            }\n            \n            priority = row[1]\n            if priority == 'HIGH':\n                competitor_analysis['high_priority'].append(site_data)\n                high_priority_players += site_data['players_online']\n            elif priority == 'MEDIUM':\n                competitor_analysis['medium_priority'].append(site_data)\n            else:\n                competitor_analysis['low_priority'].append(site_data)\n                \n        # ì‹œì¥ ì ìœ ìœ¨ ê³„ì‚°\n        total_players = competitor_analysis['market_summary']['total_players']\n        if total_players > 0:\n            competitor_analysis['market_summary']['high_priority_share'] = \\\n                round((high_priority_players / total_players) * 100, 1)\n                \n        conn.close()\n        return competitor_analysis

def main():\n    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\n    print(\"ğŸ¯ GG POKER ì§ì›ìš© ë°ì´í„° ì¤‘ì‹¬ ëª¨ë‹ˆí„°ë§ í”Œë«í¼\")\n    print(\"=\" * 70)\n    print(\"ğŸ“Š 4ê°œ í•µì‹¬ ì§€í‘œ: Players Online, Cash Players, 24h Peak, 7-day Avg\")\n    print(\"ğŸ“ˆ ì‹œê³„ì—´ ë¶„ì„ ë° ê¸‰ë³€ ê°ì§€\")\n    print(\"ğŸ“° ë‰´ìŠ¤ ì—°ê´€ì„± ìë™ ë¶„ì„\")\n    print(\"ğŸš¨ ì‹¤ì‹œê°„ ê²½ìŸì‚¬ ëª¨ë‹ˆí„°ë§\")\n    print(\"=\" * 70)\n    \n    platform = DataDrivenMonitoringPlatform()\n    \n    try:\n        # 1. ê²½ìŸì‚¬ ëª¨ë‹ˆí„°ë§ ì„¤ì •\n        print(\"\\nğŸ”§ ê²½ìŸì‚¬ ëª¨ë‹ˆí„°ë§ ì„¤ì •...\")\n        platform.setup_competitor_monitoring()\n        \n        # 2. ìƒ˜í”Œ ë°ì´í„° ìˆ˜ì§‘ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” í¬ë¡¤ëŸ¬ì—ì„œ í˜¸ì¶œ)\n        print(\"\\nğŸ“ˆ ìƒ˜í”Œ ë°ì´í„° ìˆ˜ì§‘...\")\n        sample_data = [\n            {'site_name': 'PokerStars', 'players_online': 55540, 'cash_players': 1397, 'peak_24h': 62000, 'seven_day_avg': 58000},\n            {'site_name': 'GGPoker ON', 'players_online': 4693, 'cash_players': 563, 'peak_24h': 5200, 'seven_day_avg': 4500},\n            {'site_name': 'WPT Global', 'players_online': 2989, 'cash_players': 1596, 'peak_24h': 3500, 'seven_day_avg': 2800},\n            {'site_name': '888poker', 'players_online': 1850, 'cash_players': 420, 'peak_24h': 2100, 'seven_day_avg': 1900},\n        ]\n        \n        collected = platform.collect_daily_data(sample_data)\n        \n        # 3. ë³€í™” ê°ì§€ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì´ì „ ë‚ ì§œ ë°ì´í„° ì¶”ê°€\n        print(\"\\nğŸ”„ ì´ì „ ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜...\")\n        # ì‹¤ì œë¡œëŠ” ë§¤ì¼ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ëˆ„ì ë¨\n        \n        # 4. ê²½ìŸì‚¬ ë¶„ì„\n        print(\"\\nğŸ“Š ê²½ìŸì‚¬ ë¶„ì„...\")\n        competitor_analysis = platform.get_competitor_analysis()\n        \n        print(f\"\\nâœ… ë¶„ì„ ê²°ê³¼:\")\n        print(f\"ëª¨ë‹ˆí„°ë§ ì¤‘ì¸ ê²½ìŸì‚¬: {competitor_analysis['market_summary']['total_competitors']}ê°œ\")\n        print(f\"ê³ ìš°ì„ ìˆœìœ„ ê²½ìŸì‚¬ í”Œë ˆì´ì–´: {sum(site['players_online'] for site in competitor_analysis['high_priority']):,}ëª…\")\n        \n        for site in competitor_analysis['high_priority']:\n            print(f\"  â€¢ {site['site_name']}: {site['players_online']:,}ëª… (ìºì‹œ {site['cash_ratio']}%)\")\n            \n        # 5. ì‹œê³„ì—´ ë¶„ì„ ìƒ˜í”Œ\n        print(\"\\nğŸ“ˆ ì‹œê³„ì—´ ë¶„ì„ ìƒ˜í”Œ...\")\n        for site_name in ['PokerStars', 'WPT Global']:\n            time_series = platform.generate_time_series_analysis(site_name, 7)\n            if time_series:\n                analytics = time_series['analytics']\n                players_analytics = analytics.get('players_online', {})\n                if 'no_data' not in players_analytics:\n                    print(f\"  {site_name}: í˜„ì¬ {players_analytics['current']:,}ëª…, íŠ¸ë Œë“œ {players_analytics['trend']}\")\n                    \n        # 6. ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„±\n        print(\"\\nğŸ“Š ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„±...\")\n        dashboard = platform.generate_monitoring_dashboard_data(['PokerStars', 'WPT Global'])\n        \n        print(f\"ëŒ€ì‹œë³´ë“œ ìš”ì•½:\")\n        print(f\"  ì´ ëª¨ë‹ˆí„°ë§ ì‚¬ì´íŠ¸: {dashboard['summary']['total_sites_monitored']}ê°œ\")\n        print(f\"  ì´ ì˜¨ë¼ì¸ í”Œë ˆì´ì–´: {dashboard['summary']['total_players_online']:,}ëª…\")\n        print(f\"  ìš°ì„ ìˆœìœ„ ì‚¬ì´íŠ¸: {len(dashboard['priority_sites'])}ê°œ\")\n        \n        # 7. ë¦¬í¬íŠ¸ ìƒì„±\n        print(\"\\nğŸ“‹ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±...\")\n        report_file = platform.export_analysis_report()\n        \n        print(f\"\\nğŸ¯ ë°ì´í„° ì¤‘ì‹¬ ëª¨ë‹ˆí„°ë§ í”Œë«í¼ ì„¤ì • ì™„ë£Œ!\")\n        print(f\"ğŸ“Š ë¦¬í¬íŠ¸ íŒŒì¼: {report_file}\")\n        print(f\"\")\n        print(f\"ğŸš€ ì£¼ìš” ê¸°ëŠ¥:\")\n        print(f\"  âœ… 4ê°œ í•µì‹¬ ì§€í‘œ ì¼ì¼ ìˆ˜ì§‘\")\n        print(f\"  âœ… ì‹œê³„ì—´ íŠ¸ë Œë“œ ë¶„ì„\")\n        print(f\"  âœ… ê¸‰ë³€ ì‹œì  ìë™ ê°ì§€\")\n        print(f\"  âœ… ë‰´ìŠ¤ ì—°ê´€ì„± ë¶„ì„\")\n        print(f\"  âœ… ê²½ìŸì‚¬ ëª¨ë‹ˆí„°ë§\")\n        print(f\"  âœ… ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ\")\n        \n        return True\n        \n    except Exception as e:\n        logger.error(f\"í”Œë«í¼ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}\")\n        return False\n\nif __name__ == \"__main__\":\n    success = main()\n    if success:\n        print(f\"\\nğŸ‰ GG POKERìš© ë°ì´í„° ëª¨ë‹ˆí„°ë§ í”Œë«í¼ ì™„ì„±!\")\n        print(f\"ë§¤ì¼ ìë™ ìˆ˜ì§‘ â†’ ê¸‰ë³€ ê°ì§€ â†’ ë‰´ìŠ¤ ë¶„ì„ â†’ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ\")\n    else:\n        print(f\"\\nğŸ’€ í”Œë«í¼ ì„¤ì • ì‹¤íŒ¨ - ë¬¸ì œ í•´ê²° í•„ìš”\")