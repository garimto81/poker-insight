#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë°ì´í„°ë² ì´ìŠ¤ í†µí•© - ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ DBì— ì €ì¥
"""
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Windowsì—ì„œ ìœ ë‹ˆì½”ë“œ ì¶œë ¥ ë¬¸ì œ í•´ê²°
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')

import json
import logging
from datetime import datetime
from database.models import SessionLocal, PokerSite, TrafficData, NewsItem, CrawlLog
from config.settings import Config

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataIntegrator:
    def __init__(self):
        self.session = SessionLocal()
        
    def load_json_data(self, file_path):
        """JSON íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Error loading {file_path}: {str(e)}")
            return None
            
    def integrate_pokerscout_data(self):
        """PokerScout ë°ì´í„° í†µí•©"""
        logger.info("ğŸ¯ PokerScout ë°ì´í„° í†µí•© ì‹œì‘...")
        
        data = self.load_json_data('pokerscout_perfect_data.json')
        if not data:
            logger.error("PokerScout ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
            
        sites_added = 0
        traffic_records = 0
        
        try:
            for site_data in data['data']:
                # í¬ì»¤ ì‚¬ì´íŠ¸ ì €ì¥/ì—…ë°ì´íŠ¸
                site = self.session.query(PokerSite).filter_by(name=site_data['name']).first()
                if not site:
                    site = PokerSite(
                        name=site_data['name'],
                        network=site_data.get('network', 'Unknown')
                    )
                    self.session.add(site)
                    self.session.flush()  # ID ìƒì„±ì„ ìœ„í•´
                    sites_added += 1
                    logger.info(f"  ìƒˆ ì‚¬ì´íŠ¸ ì¶”ê°€: {site.name}")
                    
                # íŠ¸ë˜í”½ ë°ì´í„° ì €ì¥
                traffic_data = TrafficData(
                    site_id=site.id,
                    cash_players=site_data.get('cash_players', 0),
                    tournament_players=site_data.get('tournament_players', 0),
                    total_players=site_data.get('players_online', 0),
                    seven_day_average=site_data.get('7_day_average', 0),
                    peak_players=site_data.get('peak_24h', 0),
                    rank=site_data.get('rank', 0),
                    timestamp=datetime.fromisoformat(data['timestamp'].replace('Z', '+00:00'))
                )
                self.session.add(traffic_data)
                traffic_records += 1
                
            # í¬ë¡¤ ë¡œê·¸ ì €ì¥
            crawl_log = CrawlLog(
                source='pokerscout',
                status='success',
                items_scraped=len(data['data']),
                started_at=datetime.fromisoformat(data['timestamp'].replace('Z', '+00:00')),
                completed_at=datetime.utcnow()
            )
            self.session.add(crawl_log)
            
            self.session.commit()
            
            logger.info(f"âœ… PokerScout ë°ì´í„° í†µí•© ì™„ë£Œ!")
            logger.info(f"  - ìƒˆ ì‚¬ì´íŠ¸: {sites_added}ê°œ")
            logger.info(f"  - íŠ¸ë˜í”½ ë ˆì½”ë“œ: {traffic_records}ê°œ")
            
            return True
            
        except Exception as e:
            logger.error(f"PokerScout ë°ì´í„° í†µí•© ì˜¤ë¥˜: {str(e)}")
            self.session.rollback()
            return False
            
    def integrate_pokernews_data(self):
        """PokerNews ë°ì´í„° í†µí•©"""
        logger.info("ğŸ“° PokerNews ë°ì´í„° í†µí•© ì‹œì‘...")
        
        data = self.load_json_data('pokernews_final_data.json')
        if not data:
            logger.error("PokerNews ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
            
        news_added = 0
        
        try:
            for article_data in data['data']:
                # ì¤‘ë³µ ì²´í¬
                existing = self.session.query(NewsItem).filter_by(url=article_data['url']).first()
                if existing:
                    continue
                    
                # ê´€ë ¨ ì‚¬ì´íŠ¸ ì°¾ê¸°
                site_id = None
                site_name = self.extract_site_from_title(article_data['title'])
                if site_name:
                    site = self.session.query(PokerSite).filter_by(name=site_name).first()
                    if site:
                        site_id = site.id
                        
                # ë‰´ìŠ¤ ì•„ì´í…œ ì €ì¥
                news_item = NewsItem(
                    site_id=site_id,
                    title=article_data['title'],
                    url=article_data['url'],
                    content=article_data.get('summary', ''),
                    author=self.clean_author_name(article_data.get('author', '')),
                    published_date=self.parse_news_date(article_data.get('date')),
                    category=article_data.get('category', 'general'),
                    scraped_at=datetime.fromisoformat(data['timestamp'].replace('Z', '+00:00'))
                )
                self.session.add(news_item)
                news_added += 1
                
            # í¬ë¡¤ ë¡œê·¸ ì €ì¥
            crawl_log = CrawlLog(
                source='pokernews',
                status='success',
                items_scraped=len(data['data']),
                started_at=datetime.fromisoformat(data['timestamp'].replace('Z', '+00:00')),
                completed_at=datetime.utcnow()
            )
            self.session.add(crawl_log)
            
            self.session.commit()
            
            logger.info(f"âœ… PokerNews ë°ì´í„° í†µí•© ì™„ë£Œ!")
            logger.info(f"  - ìƒˆ ë‰´ìŠ¤: {news_added}ê°œ")
            
            return True
            
        except Exception as e:
            logger.error(f"PokerNews ë°ì´í„° í†µí•© ì˜¤ë¥˜: {str(e)}")
            self.session.rollback()
            return False
            
    def extract_site_from_title(self, title):
        """ì œëª©ì—ì„œ í¬ì»¤ ì‚¬ì´íŠ¸ëª… ì¶”ì¶œ"""
        poker_sites = {
            'PokerStars': ['pokerstars', 'stars'],
            'GGNetwork': ['ggpoker', 'ggnetwork', 'gg poker'],
            '888poker': ['888poker', '888'],
            'partypoker': ['partypoker', 'party poker'],
            'WPT Global': ['wpt', 'world poker tour'],
            'WSOP': ['wsop', 'world series']
        }
        
        title_lower = title.lower()
        for site_name, keywords in poker_sites.items():
            if any(keyword in title_lower for keyword in keywords):
                return site_name
                
        return None
        
    def clean_author_name(self, author_text):
        """ì €ìëª… ì •ë¦¬"""
        if not author_text:
            return ''
            
        # ì²« ë²ˆì§¸ ì¤„ë§Œ ê°€ì ¸ì˜¤ê¸° (ì´ë¦„ ë¶€ë¶„)
        lines = author_text.split('\n')
        if lines:
            return lines[0].strip()
            
        return author_text.strip()
        
    def parse_news_date(self, date_str):
        """ë‰´ìŠ¤ ë‚ ì§œ íŒŒì‹±"""
        try:
            if date_str:
                return datetime.strptime(date_str, '%Y-%m-%d')
        except:
            pass
        return datetime.utcnow()
        
    def create_summary_report(self):
        """í†µí•© í›„ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
        logger.info("ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ í†µí•© ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±...")
        
        try:
            # í¬ì»¤ ì‚¬ì´íŠ¸ í†µê³„
            total_sites = self.session.query(PokerSite).count()
            
            # íŠ¸ë˜í”½ ë°ì´í„° í†µê³„
            total_traffic_records = self.session.query(TrafficData).count()
            latest_traffic = self.session.query(TrafficData).order_by(TrafficData.timestamp.desc()).first()
            
            # ë‰´ìŠ¤ ë°ì´í„° í†µê³„
            total_news = self.session.query(NewsItem).count()
            latest_news = self.session.query(NewsItem).order_by(NewsItem.scraped_at.desc()).first()
            
            # í¬ë¡¤ ë¡œê·¸ í†µê³„
            total_crawls = self.session.query(CrawlLog).count()
            successful_crawls = self.session.query(CrawlLog).filter_by(status='success').count()
            
            report = {
                'integration_summary': {
                    'timestamp': datetime.utcnow().isoformat(),
                    'poker_sites': total_sites,
                    'traffic_records': total_traffic_records,
                    'news_articles': total_news,
                    'total_crawls': total_crawls,
                    'successful_crawls': successful_crawls,
                    'success_rate': f"{(successful_crawls/total_crawls*100):.1f}%" if total_crawls > 0 else "0%"
                },
                'latest_data': {
                    'latest_traffic_update': latest_traffic.timestamp.isoformat() if latest_traffic else None,
                    'latest_news_update': latest_news.scraped_at.isoformat() if latest_news else None
                }
            }
            
            # ë¦¬í¬íŠ¸ ì €ì¥
            with open('integration_report.json', 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
                
            logger.info("ğŸ“„ í†µí•© ë¦¬í¬íŠ¸:")
            logger.info(f"  í¬ì»¤ ì‚¬ì´íŠ¸: {total_sites}ê°œ")
            logger.info(f"  íŠ¸ë˜í”½ ë ˆì½”ë“œ: {total_traffic_records}ê°œ")
            logger.info(f"  ë‰´ìŠ¤ ê¸°ì‚¬: {total_news}ê°œ")
            logger.info(f"  í¬ë¡¤ë§ ì„±ê³µë¥ : {report['integration_summary']['success_rate']}")
            
            return report
            
        except Exception as e:
            logger.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return None
            
    def close(self):
        """ì„¸ì…˜ ì¢…ë£Œ"""
        self.session.close()
        
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”„ ë°ì´í„°ë² ì´ìŠ¤ í†µí•© ì‹œì‘")
    print("="*50)
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” í™•ì¸
    try:
        from database.models import init_db
        init_db()
        print("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        print("SQLite ëª¨ë“œë¡œ ì§„í–‰í•©ë‹ˆë‹¤...")
        
        # SQLite ëª¨ë“œë¡œ ì „í™˜
        import tempfile
        temp_db = tempfile.NamedTemporaryFile(delete=False, suffix='.db')
        temp_db.close()
        
        from database.models import Base, engine
        Base.metadata.create_all(bind=engine)
    
    integrator = DataIntegrator()
    
    try:
        # 1. PokerScout ë°ì´í„° í†µí•©
        pokerscout_success = integrator.integrate_pokerscout_data()
        
        # 2. PokerNews ë°ì´í„° í†µí•©
        pokernews_success = integrator.integrate_pokernews_data()
        
        # 3. ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
        report = integrator.create_summary_report()
        
        if pokerscout_success and pokernews_success:
            print(f"\nğŸ‰ ë°ì´í„°ë² ì´ìŠ¤ í†µí•© ì™„ë£Œ!")
            print(f"âœ… PokerScout ë°ì´í„° í†µí•© ì„±ê³µ")
            print(f"âœ… PokerNews ë°ì´í„° í†µí•© ì„±ê³µ")
            print(f"ğŸ“Š í†µí•© ë¦¬í¬íŠ¸: integration_report.json")
            return True
        else:
            print(f"\nâš ï¸ ì¼ë¶€ ë°ì´í„° í†µí•© ì‹¤íŒ¨")
            return False
            
    except Exception as e:
        logger.error(f"í†µí•© í”„ë¡œì„¸ìŠ¤ ì˜¤ë¥˜: {str(e)}")
        return False
    finally:
        integrator.close()

if __name__ == "__main__":
    success = main()
    if success:
        print(f"\nğŸš€ ë‹¤ìŒ ë‹¨ê³„: í¬ì»¤ íŠ¸ë˜í”½ ë¶„ì„ ê¸°ëŠ¥ ê°œë°œ ì¤€ë¹„ ì™„ë£Œ!")
    else:
        print(f"\nğŸ’€ ë°ì´í„°ë² ì´ìŠ¤ í†µí•© ì‹¤íŒ¨ - ë¬¸ì œ í•´ê²° í•„ìš”")