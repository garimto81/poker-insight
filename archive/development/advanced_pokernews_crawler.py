#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PokerNews ê³ ê¸‰ í¬ë¡¤ëŸ¬ - ì‹¤ì œ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘
"""
import sys
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')

import cloudscraper
from bs4 import BeautifulSoup
import json
import re
from datetime import datetime, timedelta
import time

class AdvancedPokerNewsCrawler:
    def __init__(self):
        self.scraper = cloudscraper.create_scraper()
        
    def get_latest_news_articles(self):
        """ìµœì‹  ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘"""
        print("ğŸ” PokerNews ìµœì‹  ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘...")
        
        # ë‹¤ì–‘í•œ ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ ì‹œë„
        categories = [
            ('Latest News', '/news/'),
            ('PokerStars News', '/news/pokerstars/'),
            ('Casino News', '/casino/news/'),
            ('Tournament News', '/news/tournaments/'),
            ('Strategy', '/strategy/'),
        ]
        
        all_articles = []
        
        for category_name, url_path in categories:
            try:
                print(f"\nğŸ“‚ {category_name} ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§...")
                articles = self.crawl_category_page(url_path, category_name)
                if articles:
                    all_articles.extend(articles)
                    print(f"  âœ… {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
                else:
                    print(f"  âŒ ê¸°ì‚¬ ì—†ìŒ")
                    
                time.sleep(1)  # ìš”ì²­ ê°„ ë”œë ˆì´
                
            except Exception as e:
                print(f"  âŒ ì˜¤ë¥˜: {str(e)}")
                
        # ì¤‘ë³µ ì œê±°
        unique_articles = self.remove_duplicates(all_articles)
        print(f"\nğŸ“Š ì´ {len(unique_articles)}ê°œ ê³ ìœ  ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
        
        return unique_articles
        
    def crawl_category_page(self, url_path, category):
        """ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ í¬ë¡¤ë§"""
        full_url = f"https://www.pokernews.com{url_path}"
        
        try:
            response = self.scraper.get(full_url, timeout=15)
            if response.status_code != 200:
                return None
                
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ê¸°ì‚¬ ì¶”ì¶œ ì‹œë„
            articles = []
            
            # ë°©ë²• 1: íŠ¹ì • ì„ íƒìë“¤ ì‹œë„
            selectors = [
                'a[href*="/news/"][href*=".htm"]',  # ë‰´ìŠ¤ ë§í¬
                'a[href*="/strategy/"][href*=".htm"]',  # ì „ëµ ê¸°ì‚¬ ë§í¬
                'a[href*="/casino/"][href*=".htm"]',  # ì¹´ì§€ë…¸ ê¸°ì‚¬ ë§í¬
                '.article-list a[href*=".htm"]',  # ê¸°ì‚¬ ëª©ë¡ ë§í¬
                '.news-list a[href*=".htm"]',  # ë‰´ìŠ¤ ëª©ë¡ ë§í¬
            ]
            
            for selector in selectors:
                links = soup.select(selector)
                for link in links:
                    article = self.extract_article_info(link, category)
                    if article:
                        articles.append(article)
                        
            # ë°©ë²• 2: ëª¨ë“  .htm ë§í¬ì—ì„œ í•„í„°ë§
            if len(articles) < 5:
                all_links = soup.find_all('a', href=re.compile(r'\.htm'))
                for link in all_links:
                    href = link.get('href', '')
                    # ë‰´ìŠ¤/ê¸°ì‚¬ ê´€ë ¨ URLë§Œ í•„í„°ë§
                    if any(keyword in href for keyword in ['/news/', '/strategy/', '/casino/', '/poker/']):
                        article = self.extract_article_info(link, category)
                        if article:
                            articles.append(article)
                            
            return articles[:20]  # ìµœëŒ€ 20ê°œë¡œ ì œí•œ
            
        except Exception as e:
            print(f"    í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
            return None
            
    def extract_article_info(self, link_elem, category):
        """ë§í¬ ìš”ì†Œì—ì„œ ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ"""
        try:
            href = link_elem.get('href', '')
            if not href:
                return None
                
            # ì „ì²´ URL ìƒì„±
            if href.startswith('/'):
                url = f"https://www.pokernews.com{href}"
            elif href.startswith('http'):
                url = href
            else:
                return None
                
            # ì œëª© ì¶”ì¶œ
            title = link_elem.get_text().strip()
            if not title or len(title) < 10:
                return None
                
            # ë‚´ë¶€ ë§í¬ë§Œ í—ˆìš©
            if 'pokernews.com' not in url:
                return None
                
            # ë¶ˆí•„ìš”í•œ ë§í¬ í•„í„°ë§
            exclude_patterns = [
                '/authors/', '/tags/', '/tournaments/calendar/',
                '/poker-rooms/', '/bonus/', '/reviews/', '/live-reporting/'
            ]
            
            if any(pattern in url for pattern in exclude_patterns):
                return None
                
            # ë‚ ì§œ ì¶”ì¶œ ì‹œë„ (URLì—ì„œ)
            date_match = re.search(r'/(\d{4})/(\d{2})/', url)
            if date_match:
                year, month = date_match.groups()
                estimated_date = f"{year}-{month}-01"
            else:
                estimated_date = datetime.now().strftime('%Y-%m-%d')
                
            return {
                'title': title,
                'url': url,
                'category': category,
                'date': estimated_date,
                'summary': '',
                'source': 'PokerNews'
            }
            
        except Exception:
            return None
            
    def enhance_articles_with_content(self, articles):
        """ê¸°ì‚¬ ë‚´ìš©ìœ¼ë¡œ ì •ë³´ ë³´ê°•"""
        print(f"\nğŸ” ìƒìœ„ {min(10, len(articles))}ê°œ ê¸°ì‚¬ ë‚´ìš© ë¶„ì„ ì¤‘...")
        
        enhanced_articles = []
        
        for i, article in enumerate(articles[:10]):  # ìƒìœ„ 10ê°œë§Œ
            try:
                print(f"  ğŸ“„ {i+1}/10: {article['title'][:50]}...")
                
                response = self.scraper.get(article['url'], timeout=10)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
                    date_elem = soup.find('time') or soup.find(class_=re.compile(r'date|publish'))
                    if date_elem:
                        date_text = date_elem.get_text().strip()
                        parsed_date = self.parse_date(date_text)
                        if parsed_date:
                            article['date'] = parsed_date
                            
                    # ìš”ì•½ ì •ë³´ ì¶”ì¶œ
                    summary_elem = soup.find('meta', {'name': 'description'}) or soup.find('meta', {'property': 'og:description'})
                    if summary_elem:
                        article['summary'] = summary_elem.get('content', '')[:300]
                        
                    # ì €ì ì •ë³´ ì¶”ì¶œ
                    author_elem = soup.find(class_=re.compile(r'author|byline'))
                    if author_elem:
                        article['author'] = author_elem.get_text().strip()
                        
                enhanced_articles.append(article)
                time.sleep(0.5)  # ìš”ì²­ ê°„ ë”œë ˆì´
                
            except Exception as e:
                print(f"    âš ï¸ ë‚´ìš© ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
                enhanced_articles.append(article)  # ì›ë³¸ ì •ë³´ë¼ë„ ìœ ì§€
                
        # ë‚˜ë¨¸ì§€ ê¸°ì‚¬ë“¤ë„ ì¶”ê°€ (ë‚´ìš© ë¶„ì„ ì—†ì´)
        enhanced_articles.extend(articles[10:])
        
        return enhanced_articles
        
    def parse_date(self, date_text):
        """ë‚ ì§œ í…ìŠ¤íŠ¸ íŒŒì‹±"""
        try:
            # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
            if 'ago' in date_text.lower():
                if 'hour' in date_text:
                    hours = int(re.search(r'(\d+)', date_text).group(1))
                    date = datetime.now() - timedelta(hours=hours)
                    return date.strftime('%Y-%m-%d')
                elif 'day' in date_text:
                    days = int(re.search(r'(\d+)', date_text).group(1))
                    date = datetime.now() - timedelta(days=days)
                    return date.strftime('%Y-%m-%d')
                    
            # ISO ë‚ ì§œ í˜•ì‹
            date_match = re.search(r'(\d{4}-\d{2}-\d{2})', date_text)
            if date_match:
                return date_match.group(1)
                
            # ê¸°íƒ€ í˜•ì‹ë“¤
            date_match = re.search(r'(\d{1,2})[/\-](\d{1,2})[/\-](\d{4})', date_text)
            if date_match:
                month, day, year = date_match.groups()
                return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                
        except:
            pass
            
        return datetime.now().strftime('%Y-%m-%d')
        
    def remove_duplicates(self, articles):
        """ì¤‘ë³µ ê¸°ì‚¬ ì œê±°"""
        seen_urls = set()
        unique_articles = []
        
        for article in articles:
            if article['url'] not in seen_urls:
                seen_urls.add(article['url'])
                unique_articles.append(article)
                
        return unique_articles
        
    def save_data(self, articles):
        """ë°ì´í„° ì €ì¥"""
        if not articles:
            return False
            
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        category_stats = {}
        for article in articles:
            category = article['category']
            category_stats[category] = category_stats.get(category, 0) + 1
            
        output = {
            'source': 'PokerNews.com',
            'method': 'Advanced Multi-Category Crawling',
            'timestamp': datetime.now().isoformat(),
            'total_articles': len(articles),
            'category_breakdown': category_stats,
            'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC'),
            'data': articles
        }
        
        with open('pokernews_final_data.json', 'w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
            
        return True
        
    def display_results(self, articles):
        """ê²°ê³¼ ì¶œë ¥"""
        if not articles:
            print("âŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        print(f"\nğŸ“° PokerNews ê¸°ì‚¬ ìˆ˜ì§‘ ê²°ê³¼")
        print("="*80)
        print(f"ğŸ“Š ì´ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
        by_category = {}
        for article in articles:
            category = article['category']
            if category not in by_category:
                by_category[category] = []
            by_category[category].append(article)
            
        print(f"\nğŸ“‚ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")
        for category, arts in by_category.items():
            print(f"  â€¢ {category}: {len(arts)}ê°œ")
            
        print(f"\nğŸ”¥ ìµœì‹  ê¸°ì‚¬ TOP 15:")
        print("-"*80)
        
        for i, article in enumerate(articles[:15], 1):
            print(f"{i:2d}. {article['title']}")
            print(f"    ğŸ“‚ {article['category']} | ğŸ“… {article['date']}")
            if article.get('author'):
                print(f"    âœï¸ {article['author']}")
            if article['summary']:
                print(f"    ğŸ“ {article['summary'][:100]}...")
            print(f"    ğŸ”— {article['url']}")
            print()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ PokerNews ê³ ê¸‰ í¬ë¡¤ë§ ì‹œì‘!")
    print("="*60)
    
    crawler = AdvancedPokerNewsCrawler()
    
    # 1. ê¸°ì‚¬ ìˆ˜ì§‘
    articles = crawler.get_latest_news_articles()
    
    if articles and len(articles) > 5:
        # 2. ìƒìœ„ ê¸°ì‚¬ë“¤ ë‚´ìš© ë³´ê°•
        enhanced_articles = crawler.enhance_articles_with_content(articles)
        
        # 3. ê²°ê³¼ ì¶œë ¥
        crawler.display_results(enhanced_articles)
        
        # 4. ë°ì´í„° ì €ì¥
        success = crawler.save_data(enhanced_articles)
        
        if success:
            print("ğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ: pokernews_final_data.json")
            print(f"\nâœ… PokerNews í¬ë¡¤ë§ ì„±ê³µ!")
            print(f"ğŸ“Š ìˆ˜ì§‘ ê¸°ì‚¬: {len(enhanced_articles)}ê°œ")
            print(f"ğŸ“ˆ ë°ì´í„° í’ˆì§ˆ: ìš°ìˆ˜")
            return True
        else:
            print("âŒ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨")
            
    else:
        print(f"\nâŒ ì¶©ë¶„í•œ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        print(f"ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìˆ˜: {len(articles) if articles else 0}")
        
    return False

if __name__ == "__main__":
    success = main()
    
    if success:
        print(f"\nğŸ‰ PokerNews ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
        print(f"í¬ì»¤ ë°©ì†¡ í”„ë¡œë•ì…˜ìš© ë‰´ìŠ¤ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!")
    else:
        print(f"\nâš ï¸ PokerNews í¬ë¡¤ë§ ê°œì„  í•„ìš”")