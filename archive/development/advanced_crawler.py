#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PokerScout ë°ì´í„° ìˆ˜ì§‘ì„ ìœ„í•œ ë‹¤ì–‘í•œ í¬ë¡¤ë§ ë°©ë²•
"""
import sys
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')

import requests
import cloudscraper
from bs4 import BeautifulSoup
import time
import json
from datetime import datetime
import random
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

class AdvancedPokerScoutCrawler:
    def __init__(self):
        self.results = []
        
    def parse_table_data(self, soup):
        """í…Œì´ë¸” ë°ì´í„° íŒŒì‹±"""
        table = soup.find('table', {'class': 'ranktable'})
        if not table:
            return None
            
        rows = table.find_all('tr')[1:]  # í—¤ë” ì œì™¸
        results = []
        
        for row in rows:
            cols = row.find_all('td')
            if len(cols) >= 6:
                data = {
                    'rank': cols[0].text.strip(),
                    'name': cols[1].text.strip(),
                    'network': self.extract_network(cols[1]),
                    'cash_players': self.parse_number(cols[2].text.strip()),
                    'tournament_players': self.parse_number(cols[3].text.strip()),
                    'total_players': self.parse_number(cols[4].text.strip()),
                    '7_day_average': self.parse_number(cols[5].text.strip())
                }
                results.append(data)
                
        return results
        
    def parse_number(self, text):
        """ìˆ«ì íŒŒì‹± (1,234 -> 1234)"""
        if not text or text == '-':
            return 0
        try:
            return int(text.replace(',', '').replace(' ', ''))
        except:
            return 0
            
    def extract_network(self, cell):
        """ë„¤íŠ¸ì›Œí¬ ì •ë³´ ì¶”ì¶œ"""
        title = cell.get('title', '')
        if title:
            return title
        return 'Unknown'
        
    def method1_cloudscraper(self):
        """ë°©ë²• 1: CloudScraper ì‚¬ìš© (Cloudflare ìš°íšŒ)"""
        print("\n=== ë°©ë²• 1: CloudScraper ì‚¬ìš© ===")
        try:
            scraper = cloudscraper.create_scraper()
            response = scraper.get('https://www.pokerscout.com', timeout=30)
            
            if response.status_code == 200:
                print(f"âœ“ ì„±ê³µ! ì‘ë‹µ í¬ê¸°: {len(response.content)} bytes")
                soup = BeautifulSoup(response.content, 'html.parser')
                results = self.parse_table_data(soup)
                if results:
                    print(f"âœ“ {len(results)}ê°œ ì‚¬ì´íŠ¸ ë°ì´í„° ìˆ˜ì§‘ ì„±ê³µ!")
                    return results
                else:
                    print("âœ— í…Œì´ë¸” íŒŒì‹± ì‹¤íŒ¨")
            else:
                print(f"âœ— ì‹¤íŒ¨: HTTP {response.status_code}")
                
        except Exception as e:
            print(f"âœ— ì˜¤ë¥˜: {str(e)}")
        return None
        
    def method2_undetected_chrome(self):
        """ë°©ë²• 2: Undetected ChromeDriver ì‚¬ìš©"""
        print("\n=== ë°©ë²• 2: Undetected ChromeDriver ì‚¬ìš© ===")
        driver = None
        try:
            print("Chrome ë“œë¼ì´ë²„ ì‹œì‘...")
            options = uc.ChromeOptions()
            options.add_argument('--disable-blink-features=AutomationControlled')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            
            driver = uc.Chrome(options=options)
            
            print("PokerScout ì ‘ì† ì¤‘...")
            driver.get('https://www.pokerscout.com')
            
            # Cloudflare í†µê³¼ ëŒ€ê¸°
            print("Cloudflare ì²´í¬ ëŒ€ê¸°...")
            time.sleep(15)
            
            # í…Œì´ë¸” í™•ì¸
            try:
                table = WebDriverWait(driver, 30).until(
                    EC.presence_of_element_located((By.CLASS_NAME, "ranktable"))
                )
                print("âœ“ í…Œì´ë¸” ë°œê²¬!")
                
                soup = BeautifulSoup(driver.page_source, 'html.parser')
                results = self.parse_table_data(soup)
                if results:
                    print(f"âœ“ {len(results)}ê°œ ì‚¬ì´íŠ¸ ë°ì´í„° ìˆ˜ì§‘ ì„±ê³µ!")
                    return results
                    
            except Exception as e:
                print(f"âœ— í…Œì´ë¸” ëŒ€ê¸° ì‹¤íŒ¨: {str(e)}")
                
        except Exception as e:
            print(f"âœ— ì˜¤ë¥˜: {str(e)}")
        finally:
            if driver:
                driver.quit()
        return None
        
    def method3_mobile_version(self):
        """ë°©ë²• 3: ëª¨ë°”ì¼ ë²„ì „ ì‹œë„"""
        print("\n=== ë°©ë²• 3: ëª¨ë°”ì¼ ë²„ì „ ì‹œë„ ===")
        mobile_headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_7_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Mobile/15E148 Safari/604.1',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        urls = [
            'https://m.pokerscout.com',
            'https://mobile.pokerscout.com',
            'https://www.pokerscout.com/mobile',
            'https://www.pokerscout.com/?mobile=1'
        ]
        
        for url in urls:
            try:
                print(f"ì‹œë„ ì¤‘: {url}")
                response = requests.get(url, headers=mobile_headers, timeout=10)
                if response.status_code == 200:
                    print(f"âœ“ ì„±ê³µ! {url}")
                    soup = BeautifulSoup(response.content, 'html.parser')
                    results = self.parse_table_data(soup)
                    if results:
                        return results
            except Exception as e:
                print(f"  âœ— ì‹¤íŒ¨: {str(e)}")
                
        return None
        
    def method4_api_endpoints(self):
        """ë°©ë²• 4: API ì—”ë“œí¬ì¸íŠ¸ íƒìƒ‰"""
        print("\n=== ë°©ë²• 4: API ì—”ë“œí¬ì¸íŠ¸ íƒìƒ‰ ===")
        api_endpoints = [
            'https://www.pokerscout.com/api/data',
            'https://www.pokerscout.com/api/rankings',
            'https://www.pokerscout.com/data.json',
            'https://www.pokerscout.com/rankings.json',
            'https://api.pokerscout.com/rankings',
            'https://data.pokerscout.com/current'
        ]
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/json, text/plain, */*',
            'X-Requested-With': 'XMLHttpRequest'
        }
        
        for endpoint in api_endpoints:
            try:
                print(f"ì‹œë„ ì¤‘: {endpoint}")
                response = requests.get(endpoint, headers=headers, timeout=10)
                if response.status_code == 200:
                    print(f"âœ“ ë°œê²¬! {endpoint}")
                    try:
                        data = response.json()
                        print(f"JSON ë°ì´í„° í¬ê¸°: {len(str(data))}")
                        return self.parse_api_data(data)
                    except:
                        print("JSONì´ ì•„ë‹˜, HTML íŒŒì‹± ì‹œë„...")
                        soup = BeautifulSoup(response.content, 'html.parser')
                        return self.parse_table_data(soup)
            except Exception as e:
                print(f"  âœ— {response.status_code if 'response' in locals() else 'Connection error'}")
                
        return None
        
    def parse_api_data(self, data):
        """API ë°ì´í„° íŒŒì‹±"""
        # API ì‘ë‹µ êµ¬ì¡°ì— ë”°ë¼ íŒŒì‹± ë¡œì§ êµ¬í˜„
        if isinstance(data, list):
            return data
        elif isinstance(data, dict) and 'sites' in data:
            return data['sites']
        return None
        
    def method5_cached_pages(self):
        """ë°©ë²• 5: ìºì‹œëœ í˜ì´ì§€ ì ‘ê·¼"""
        print("\n=== ë°©ë²• 5: ìºì‹œëœ í˜ì´ì§€ ì ‘ê·¼ ===")
        
        # Google Cache
        google_cache_url = "https://webcache.googleusercontent.com/search?q=cache:pokerscout.com"
        
        try:
            print("Google Cache ì‹œë„...")
            response = requests.get(google_cache_url, timeout=10)
            if response.status_code == 200:
                print("âœ“ Google Cache ì ‘ê·¼ ì„±ê³µ!")
                soup = BeautifulSoup(response.content, 'html.parser')
                results = self.parse_table_data(soup)
                if results:
                    return results
        except Exception as e:
            print(f"âœ— Google Cache ì‹¤íŒ¨: {str(e)}")
            
        # Wayback Machine
        try:
            print("Wayback Machine ìµœì‹  ìŠ¤ëƒ…ìƒ· í™•ì¸...")
            # ìµœì‹  ìŠ¤ëƒ…ìƒ· URL ê°€ì ¸ì˜¤ê¸°
            wayback_api = "http://archive.org/wayback/available?url=pokerscout.com"
            response = requests.get(wayback_api, timeout=10)
            if response.status_code == 200:
                data = response.json()
                if 'archived_snapshots' in data and 'closest' in data['archived_snapshots']:
                    snapshot_url = data['archived_snapshots']['closest']['url']
                    print(f"âœ“ ìŠ¤ëƒ…ìƒ· ë°œê²¬: {snapshot_url}")
                    
                    snapshot_response = requests.get(snapshot_url, timeout=10)
                    if snapshot_response.status_code == 200:
                        soup = BeautifulSoup(snapshot_response.content, 'html.parser')
                        results = self.parse_table_data(soup)
                        if results:
                            print("âœ“ Wayback Machineì—ì„œ ë°ì´í„° ìˆ˜ì§‘ ì„±ê³µ!")
                            return results
        except Exception as e:
            print(f"âœ— Wayback Machine ì‹¤íŒ¨: {str(e)}")
            
        return None
        
    def run_all_methods(self):
        """ëª¨ë“  ë°©ë²• ìˆœì°¨ ì‹¤í–‰"""
        print("PokerScout ë°ì´í„° ìˆ˜ì§‘ì„ ìœ„í•œ ë‹¤ì–‘í•œ ë°©ë²• ì‹œë„ ì¤‘...")
        
        methods = [
            ('CloudScraper', self.method1_cloudscraper),
            ('Mobile Version', self.method3_mobile_version),
            ('Cached Pages', self.method5_cached_pages),
            ('API Endpoints', self.method4_api_endpoints),
            ('Undetected Chrome', self.method2_undetected_chrome)
        ]
        
        for method_name, method_func in methods:
            print(f"\n{'='*50}")
            print(f"ì‹œë„ ì¤‘: {method_name}")
            print(f"{'='*50}")
            
            try:
                results = method_func()
                if results and len(results) > 0:
                    print(f"\nğŸ‰ ì„±ê³µ! {method_name}ìœ¼ë¡œ {len(results)}ê°œ ì‚¬ì´íŠ¸ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
                    
                    # ê²°ê³¼ ì €ì¥
                    output = {
                        'source': 'PokerScout.com',
                        'method': method_name,
                        'timestamp': datetime.now().isoformat(),
                        'site_count': len(results),
                        'data': results
                    }
                    
                    with open('pokerscout_success.json', 'w', encoding='utf-8') as f:
                        json.dump(output, f, indent=2, ensure_ascii=False)
                    
                    print(f"âœ“ ë°ì´í„°ê°€ 'pokerscout_success.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    
                    # ìƒìœ„ 5ê°œ ì‚¬ì´íŠ¸ ì¶œë ¥
                    print("\nìƒìœ„ 5ê°œ í¬ì»¤ ì‚¬ì´íŠ¸:")
                    for i, site in enumerate(results[:5]):
                        print(f"{site['rank']}. {site['name']} - {site['total_players']:,} players")
                    
                    return True
                else:
                    print(f"âœ— {method_name} ì‹¤íŒ¨ - ë°ì´í„° ì—†ìŒ")
                    
            except Exception as e:
                print(f"âœ— {method_name} ì˜¤ë¥˜: {str(e)}")
                
            # ë°©ë²• ê°„ ë”œë ˆì´
            time.sleep(2)
            
        print("\nğŸ’€ ëª¨ë“  ë°©ë²• ì‹¤íŒ¨...")
        return False

if __name__ == "__main__":
    crawler = AdvancedPokerScoutCrawler()
    success = crawler.run_all_methods()
    
    if not success:
        print("\nâš ï¸  ëª¨ë“  í¬ë¡¤ë§ ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        print("ê¶Œì¥ ì‚¬í•­:")
        print("1. VPN ì‚¬ìš©í•˜ì—¬ ë‹¤ë¥¸ ì§€ì—­ì—ì„œ ì ‘ì†")
        print("2. PokerScoutì— ì§ì ‘ ë¬¸ì˜í•˜ì—¬ API ì•¡ì„¸ìŠ¤ ìš”ì²­")
        print("3. ëŒ€ì²´ ë°ì´í„° ì†ŒìŠ¤ ì‚¬ìš© ê³ ë ¤")
    else:
        print("\nâœ… í”„ë¡œì íŠ¸ ê³„ì† ì§„í–‰ ê°€ëŠ¥!")