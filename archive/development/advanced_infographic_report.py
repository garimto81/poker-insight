#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ê³ ê¸‰ ì¸í¬ê·¸ë˜í”½ í˜•íƒœ í¬ì»¤ ì‹œì¥ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±ê¸°
- ëª¨ë“  í™œì„± ì‚¬ì´íŠ¸ ìˆ˜ì¹˜ ë¶„ì„
- íŠ¹ì´ ë³€í™” ê°ì§€ ë° ë‰´ìŠ¤ ì—°ê´€ì„± ë¶„ì„
- ì˜¨ë¼ì¸ í¬ì»¤ ì‚¬ì´íŠ¸ ë°ì´í„° ì§‘ì¤‘ ë¶„ì„
"""
import sys
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')

import json
import logging
import sqlite3
from datetime import datetime, timedelta
from collections import defaultdict, Counter
import statistics
import re

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AdvancedInfographicReportGenerator:
    def __init__(self, db_path='poker_insight.db'):
        self.db_path = db_path
        
        # ë¶„ì„ ì„ê³„ê°’ ì„¤ì •
        self.SIGNIFICANT_CHANGE_THRESHOLD = 15.0  # 15% ì´ìƒ ë³€í™”
        self.MAJOR_CHANGE_THRESHOLD = 25.0        # 25% ì´ìƒ ì£¼ìš” ë³€í™”
        self.ANOMALY_THRESHOLD = 50.0             # 50% ì´ìƒ ì´ìƒ ì§•í›„
        
    def get_db_connection(self):
        """SQLite ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
        return sqlite3.connect(self.db_path)
        
    def generate_comprehensive_infographic_report(self):
        """ì¢…í•© ì¸í¬ê·¸ë˜í”½ ë¦¬í¬íŠ¸ ìƒì„±"""
        logger.info("ğŸ¨ ê³ ê¸‰ ì¸í¬ê·¸ë˜í”½ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘...")
        
        report_data = {
            'report_metadata': self.get_report_metadata(),
            'market_overview': self.analyze_complete_market_overview(),
            'all_sites_analysis': self.analyze_all_active_sites(),
            'anomaly_detection': self.detect_market_anomalies(),
            'news_correlation': self.analyze_news_correlations(),
            'infographic_sections': self.generate_infographic_sections(),
            'executive_summary': {},  # ë§ˆì§€ë§‰ì— ìƒì„±
            'actionable_insights': self.generate_actionable_insights()
        }
        
        # ê²½ì˜ì§„ ìš”ì•½ ìƒì„±
        report_data['executive_summary'] = self.generate_executive_summary(report_data)
        
        return report_data
        
    def get_report_metadata(self):
        """ë¦¬í¬íŠ¸ ë©”íƒ€ë°ì´í„°"""
        return {
            'generated_at': datetime.now().isoformat(),
            'report_type': 'comprehensive_infographic_analysis',
            'version': '2.0_advanced',
            'focus': 'online_poker_sites_data_analysis',
            'analysis_scope': 'all_active_sites_with_news_correlation'
        }
        
    def analyze_complete_market_overview(self):
        """ì™„ì „í•œ ì‹œì¥ ê°œìš” ë¶„ì„"""
        logger.info("  ğŸ“Š ì™„ì „í•œ ì‹œì¥ ê°œìš” ë¶„ì„...")
        
        try:
            conn = self.get_db_connection()
            cursor = conn.cursor()
            
            # ëª¨ë“  ì‚¬ì´íŠ¸ ë°ì´í„° ì¡°íšŒ
            query = """
            SELECT 
                ps.name,
                ps.url,
                td.total_players,
                td.cash_players,
                td.tournament_players,
                td.seven_day_average,
                td.rank
            FROM poker_sites ps
            JOIN traffic_data td ON ps.id = td.site_id
            ORDER BY td.total_players DESC
            """
            
            cursor.execute(query)
            all_sites_data = cursor.fetchall()
            
            # ì „ì²´ ì‹œì¥ í†µê³„ ê³„ì‚°
            total_players = sum(row[2] for row in all_sites_data)
            total_cash = sum(row[3] for row in all_sites_data)
            total_tournaments = sum(row[4] for row in all_sites_data)
            
            # í™œì„± ì‚¬ì´íŠ¸ ë¶„ë¥˜
            active_sites = [row for row in all_sites_data if row[2] > 0]
            large_sites = [row for row in active_sites if row[2] > 10000]  # 1ë§Œëª… ì´ìƒ
            medium_sites = [row for row in active_sites if 1000 < row[2] <= 10000]  # 1ì²œ-1ë§Œëª…
            small_sites = [row for row in active_sites if 100 < row[2] <= 1000]    # 100-1ì²œëª…
            micro_sites = [row for row in active_sites if row[2] <= 100]           # 100ëª… ì´í•˜
            
            # ì‹œì¥ ì§‘ì¤‘ë„ ê³„ì‚° (HHI)
            hhi = sum(((site[2] / total_players) * 100) ** 2 for site in active_sites) if total_players > 0 else 0
            
            # ìƒìœ„ ì‚¬ì´íŠ¸ë“¤ì˜ ì ìœ ìœ¨
            top5_share = sum(row[2] for row in active_sites[:5]) / total_players * 100 if total_players > 0 else 0
            top10_share = sum(row[2] for row in active_sites[:10]) / total_players * 100 if total_players > 0 else 0
            
            overview = {
                'total_market_size': total_players,
                'total_cash_players': total_cash,
                'total_tournament_players': total_tournaments,
                'market_segmentation': {
                    'large_sites_count': len(large_sites),
                    'medium_sites_count': len(medium_sites),
                    'small_sites_count': len(small_sites),
                    'micro_sites_count': len(micro_sites),
                    'large_sites_share': round(sum(site[2] for site in large_sites) / total_players * 100, 1) if total_players > 0 else 0,
                    'medium_sites_share': round(sum(site[2] for site in medium_sites) / total_players * 100, 1) if total_players > 0 else 0
                },
                'market_concentration': {
                    'hhi_index': round(hhi, 2),
                    'top5_market_share': round(top5_share, 1),
                    'top10_market_share': round(top10_share, 1),
                    'concentration_level': self.classify_market_concentration(hhi)
                },
                'player_distribution': {
                    'cash_vs_tournament_ratio': f"{round(total_cash/total_players*100, 1)}% : {round(total_tournaments/total_players*100, 1)}%" if total_players > 0 else "0% : 0%",
                    'average_players_per_site': round(total_players / len(active_sites), 0) if active_sites else 0
                },
                'total_active_sites': len(active_sites),
                'total_tracked_sites': len(all_sites_data)
            }
            
            conn.close()
            return overview
            
        except Exception as e:
            logger.error(f"ì‹œì¥ ê°œìš” ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {}
            
    def classify_market_concentration(self, hhi):
        """ì‹œì¥ ì§‘ì¤‘ë„ ë¶„ë¥˜"""
        if hhi < 1500:
            return "ê²½ìŸì  ì‹œì¥"
        elif hhi < 2500:
            return "ì¤‘ê°„ ì§‘ì¤‘ë„"
        else:
            return "ê³ ë„ ì§‘ì¤‘ ì‹œì¥"
            
    def analyze_all_active_sites(self):
        """ëª¨ë“  í™œì„± ì‚¬ì´íŠ¸ ìƒì„¸ ë¶„ì„"""
        logger.info("  ğŸ” ëª¨ë“  í™œì„± ì‚¬ì´íŠ¸ ìƒì„¸ ë¶„ì„...")
        
        try:
            conn = self.get_db_connection()
            cursor = conn.cursor()
            
            query = """
            SELECT 
                ps.name,
                ps.url,
                td.total_players,
                td.cash_players,
                td.tournament_players,
                td.seven_day_average,
                td.rank
            FROM poker_sites ps
            JOIN traffic_data td ON ps.id = td.site_id
            WHERE td.total_players > 0
            ORDER BY td.total_players DESC
            """
            
            cursor.execute(query)
            sites_data = cursor.fetchall()
            
            analyzed_sites = []
            total_market = sum(row[2] for row in sites_data)
            
            for i, row in enumerate(sites_data):
                name, url, total_players, cash_players, tournament_players, seven_day_avg, rank = row
                
                # ì„±ì¥ë¥  ê³„ì‚°
                growth_rate = 0
                if seven_day_avg and seven_day_avg > 0:
                    growth_rate = ((total_players - seven_day_avg) / seven_day_avg) * 100
                
                # ì‚¬ì´íŠ¸ ë¶„ë¥˜
                site_category = self.categorize_site_size(total_players)
                
                # í”Œë ˆì´ì–´ êµ¬ì„± ë¶„ì„
                cash_ratio = (cash_players / total_players * 100) if total_players > 0 else 0
                tournament_ratio = (tournament_players / total_players * 100) if total_players > 0 else 0
                
                # ì‹œì¥ ì ìœ ìœ¨
                market_share = (total_players / total_market * 100) if total_market > 0 else 0
                
                # íŠ¹ì´ì‚¬í•­ ê°ì§€
                anomaly_flags = self.detect_site_anomalies(total_players, growth_rate, cash_ratio)
                
                site_analysis = {
                    'rank': rank or (i + 1),
                    'name': name,
                    'url': url,
                    'metrics': {
                        'total_players': total_players,
                        'cash_players': cash_players,
                        'tournament_players': tournament_players,
                        'seven_day_average': seven_day_avg or 0
                    },
                    'percentages': {
                        'market_share': round(market_share, 2),
                        'cash_ratio': round(cash_ratio, 1),
                        'tournament_ratio': round(tournament_ratio, 1),
                        'growth_rate': round(growth_rate, 1)
                    },
                    'classification': {
                        'size_category': site_category,
                        'growth_trend': self.classify_growth_trend(growth_rate),
                        'player_type_focus': 'tournament' if tournament_ratio > 70 else 'cash' if cash_ratio > 70 else 'balanced'
                    },
                    'anomaly_flags': anomaly_flags,
                    'notable_features': self.identify_notable_features(name, total_players, growth_rate, cash_ratio)
                }
                
                analyzed_sites.append(site_analysis)
                
            conn.close()
            
            # ì‚¬ì´íŠ¸ë³„ í†µê³„ ìš”ì•½
            site_stats = self.calculate_site_statistics(analyzed_sites)
            
            return {
                'sites_analysis': analyzed_sites,
                'statistics_summary': site_stats,
                'total_sites_analyzed': len(analyzed_sites)
            }
            
        except Exception as e:
            logger.error(f"ì‚¬ì´íŠ¸ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {}
            
    def categorize_site_size(self, total_players):
        """ì‚¬ì´íŠ¸ ê·œëª¨ ë¶„ë¥˜"""
        if total_players >= 50000:
            return "ë©”ì´ì € ì‚¬ì´íŠ¸"
        elif total_players >= 10000:
            return "ëŒ€í˜• ì‚¬ì´íŠ¸"
        elif total_players >= 1000:
            return "ì¤‘í˜• ì‚¬ì´íŠ¸"
        elif total_players >= 100:
            return "ì†Œí˜• ì‚¬ì´íŠ¸"
        else:
            return "ë§ˆì´í¬ë¡œ ì‚¬ì´íŠ¸"
            
    def classify_growth_trend(self, growth_rate):
        """ì„±ì¥ íŠ¸ë Œë“œ ë¶„ë¥˜"""
        if growth_rate >= 25:
            return "ê¸‰ì„±ì¥"
        elif growth_rate >= 10:
            return "ì„±ì¥"
        elif growth_rate >= -10:
            return "ì•ˆì •"
        elif growth_rate >= -25:
            return "í•˜ë½"
        else:
            return "ê¸‰ë½"
            
    def detect_site_anomalies(self, total_players, growth_rate, cash_ratio):
        """ì‚¬ì´íŠ¸ë³„ ì´ìƒ ì§•í›„ ê°ì§€"""
        anomalies = []
        
        # ê¸‰ê²©í•œ ì„±ì¥/í•˜ë½
        if abs(growth_rate) > self.ANOMALY_THRESHOLD:
            anomalies.append(f"ê·¹ë„ì˜ {'ì„±ì¥' if growth_rate > 0 else 'í•˜ë½'} ({growth_rate:+.1f}%)")
            
        # ë¹„ì •ìƒì ì¸ í”Œë ˆì´ì–´ êµ¬ì„±
        if cash_ratio > 90:
            anomalies.append("ìºì‹œê²Œì„ ê³¼ë„ ì§‘ì¤‘")
        elif cash_ratio < 5 and total_players > 1000:
            anomalies.append("í† ë„ˆë¨¼íŠ¸ ê³¼ë„ ì§‘ì¤‘")
            
        # í¬ê¸° ëŒ€ë¹„ ë¹„ì •ìƒì  ì„±ì¥
        if total_players < 1000 and growth_rate > 100:
            anomalies.append("ì†Œê·œëª¨ ì‚¬ì´íŠ¸ í­ë°œì  ì„±ì¥")
            
        return anomalies
        
    def identify_notable_features(self, name, total_players, growth_rate, cash_ratio):
        """ì£¼ëª©í•  ë§Œí•œ íŠ¹ì§• ì‹ë³„"""
        features = []
        
        # ë¸Œëœë“œ íŠ¹ì„± ë¶„ì„
        if 'PokerStars' in name:
            features.append("ê¸€ë¡œë²Œ ë¸Œëœë“œ")
        if 'GG' in name:
            features.append("GGNetwork ê³„ì—´")
        if 'WPT' in name:
            features.append("WPT ë¸Œëœë“œ")
            
        # ì„±ê³¼ íŠ¹ì„±
        if growth_rate > 20:
            features.append("ê³ ì„±ì¥ ì‚¬ì´íŠ¸")
        if total_players > 50000:
            features.append("ë©”ì´ì € í”Œë ˆì´ì–´")
        if 30 <= cash_ratio <= 70:
            features.append("ê· í˜•ì¡íŒ ê²Œì„ êµ¬ì„±")
            
        return features
        
    def calculate_site_statistics(self, analyzed_sites):
        """ì‚¬ì´íŠ¸ë³„ í†µê³„ ê³„ì‚°"""
        if not analyzed_sites:
            return {}
            
        growth_rates = [site['percentages']['growth_rate'] for site in analyzed_sites]
        market_shares = [site['percentages']['market_share'] for site in analyzed_sites]
        total_players = [site['metrics']['total_players'] for site in analyzed_sites]
        
        return {
            'growth_statistics': {
                'average_growth_rate': round(statistics.mean(growth_rates), 1),
                'median_growth_rate': round(statistics.median(growth_rates), 1),
                'growth_rate_std': round(statistics.stdev(growth_rates), 1) if len(growth_rates) > 1 else 0,
                'positive_growth_sites': len([r for r in growth_rates if r > 0]),
                'negative_growth_sites': len([r for r in growth_rates if r < 0])
            },
            'market_distribution': {
                'top_10_share': round(sum(market_shares[:10]), 1),
                'market_share_std': round(statistics.stdev(market_shares), 2) if len(market_shares) > 1 else 0,
                'largest_site_share': round(max(market_shares), 1) if market_shares else 0
            },
            'size_distribution': {
                'average_site_size': round(statistics.mean(total_players), 0),
                'median_site_size': round(statistics.median(total_players), 0),
                'size_variance': round(statistics.variance(total_players), 0) if len(total_players) > 1 else 0
            }
        }
        
    def detect_market_anomalies(self):
        """ì‹œì¥ ì´ìƒ ì§•í›„ ê°ì§€"""
        logger.info("  ğŸš¨ ì‹œì¥ ì´ìƒ ì§•í›„ ê°ì§€...")
        
        try:
            sites_analysis = self.analyze_all_active_sites()
            sites = sites_analysis.get('sites_analysis', [])
            
            anomalies = {
                'significant_changes': [],
                'market_disruptions': [],
                'unusual_patterns': [],
                'risk_factors': []
            }
            
            for site in sites:
                growth_rate = site['percentages']['growth_rate']
                total_players = site['metrics']['total_players']
                anomaly_flags = site['anomaly_flags']
                
                # ì¤‘ìš”í•œ ë³€í™” ê°ì§€
                if abs(growth_rate) > self.SIGNIFICANT_CHANGE_THRESHOLD:
                    severity = "HIGH" if abs(growth_rate) > self.MAJOR_CHANGE_THRESHOLD else "MEDIUM"
                    change_type = "ê¸‰ë“±" if growth_rate > 0 else "ê¸‰ë½"
                    
                    anomalies['significant_changes'].append({
                        'site': site['name'],
                        'change_type': change_type,
                        'growth_rate': growth_rate,
                        'severity': severity,
                        'current_players': total_players,
                        'impact_assessment': self.assess_market_impact(site, growth_rate)
                    })
                    
                # ì‹œì¥ êµë€ ìš”ì†Œ ê°ì§€
                if total_players > 10000 and abs(growth_rate) > 30:
                    anomalies['market_disruptions'].append({
                        'site': site['name'],
                        'disruption_type': "ëŒ€í˜• ì‚¬ì´íŠ¸ ê¸‰ë³€",
                        'description': f"{site['name']} ({total_players:,}ëª…)ì—ì„œ {growth_rate:+.1f}% ë³€í™”",
                        'market_share': site['percentages']['market_share']
                    })
                    
                # ë¹„ì •ìƒ íŒ¨í„´
                if anomaly_flags:
                    anomalies['unusual_patterns'].append({
                        'site': site['name'],
                        'patterns': anomaly_flags,
                        'total_players': total_players
                    })
                    
            # ë¦¬ìŠ¤í¬ ìš”ì¸ ë¶„ì„
            anomalies['risk_factors'] = self.analyze_risk_factors(sites)
            
            return anomalies
            
        except Exception as e:
            logger.error(f"ì´ìƒ ì§•í›„ ê°ì§€ ì˜¤ë¥˜: {str(e)}")
            return {}
            
    def assess_market_impact(self, site, growth_rate):
        """ì‹œì¥ ì˜í–¥ í‰ê°€"""
        market_share = site['percentages']['market_share']
        
        if market_share > 20:
            return "HIGH - ì‹œì¥ ë¦¬ë”ì˜ ë³€í™”ë¡œ ì „ì²´ ì‹œì¥ì— í° ì˜í–¥"
        elif market_share > 10:
            return "MEDIUM - ì£¼ìš” í”Œë ˆì´ì–´ ë³€í™”ë¡œ ì‹œì¥ ë™í–¥ì— ì˜í–¥"
        elif market_share > 5:
            return "LOW-MEDIUM - ì¤‘í˜• ì‚¬ì´íŠ¸ ë³€í™”ë¡œ í•´ë‹¹ ì„¸ê·¸ë¨¼íŠ¸ì— ì˜í–¥"
        else:
            return "LOW - ì†Œê·œëª¨ ë³€í™”ë¡œ ì œí•œì  ì˜í–¥"
            
    def analyze_risk_factors(self, sites):
        """ë¦¬ìŠ¤í¬ ìš”ì¸ ë¶„ì„"""
        risk_factors = []
        
        # ì‹œì¥ ì§‘ì¤‘ë„ ë¦¬ìŠ¤í¬
        top3_share = sum(site['percentages']['market_share'] for site in sites[:3])
        if top3_share > 70:
            risk_factors.append({
                'type': 'ì‹œì¥ ì§‘ì¤‘ë„ ë¦¬ìŠ¤í¬',
                'description': f'ìƒìœ„ 3ê°œ ì‚¬ì´íŠ¸ê°€ {top3_share:.1f}% ì ìœ ',
                'level': 'HIGH'
            })
            
        # ê¸‰ê²©í•œ ë³€í™” ì‚¬ì´íŠ¸ ìˆ˜
        rapid_change_sites = len([s for s in sites if abs(s['percentages']['growth_rate']) > 25])
        if rapid_change_sites > 3:
            risk_factors.append({
                'type': 'ì‹œì¥ ë¶ˆì•ˆì •ì„±',
                'description': f'{rapid_change_sites}ê°œ ì‚¬ì´íŠ¸ì—ì„œ ê¸‰ê²©í•œ ë³€í™”',
                'level': 'MEDIUM'
            })
            
        return risk_factors
        
    def analyze_news_correlations(self):
        """ë‰´ìŠ¤-ë°ì´í„° ì—°ê´€ì„± ë¶„ì„"""
        logger.info("  ğŸ“° ë‰´ìŠ¤-ë°ì´í„° ì—°ê´€ì„± ë¶„ì„...")
        
        try:
            # ì´ìƒ ì§•í›„ ì‚¬ì´íŠ¸ë“¤ ì‹ë³„
            anomalies = self.detect_market_anomalies()
            significant_sites = [change['site'] for change in anomalies.get('significant_changes', [])]
            
            # ë‰´ìŠ¤ ë°ì´í„° ì¡°íšŒ
            conn = self.get_db_connection()
            cursor = conn.cursor()
            
            query = """
            SELECT title, content, category, author, published_date, url
            FROM news_items
            ORDER BY scraped_at DESC
            LIMIT 50
            """
            
            cursor.execute(query)
            news_data = cursor.fetchall()
            
            correlations = {
                'direct_mentions': [],
                'indirect_correlations': [],
                'market_trend_news': [],
                'regulatory_impact': [],
                'tournament_effects': []
            }
            
            # ì‚¬ì´íŠ¸ë³„ ë‰´ìŠ¤ ì—°ê´€ì„± ë¶„ì„
            for site_name in significant_sites:
                site_mentions = self.find_site_news_mentions(site_name, news_data)
                if site_mentions:
                    correlations['direct_mentions'].extend(site_mentions)
                    
            # ê°„ì ‘ ì—°ê´€ì„± ë¶„ì„
            correlations['indirect_correlations'] = self.analyze_indirect_correlations(news_data, anomalies)
            
            # ì‹œì¥ íŠ¸ë Œë“œ ê´€ë ¨ ë‰´ìŠ¤
            correlations['market_trend_news'] = self.identify_market_trend_news(news_data)
            
            # ê·œì œ ì˜í–¥ ë¶„ì„
            correlations['regulatory_impact'] = self.analyze_regulatory_news_impact(news_data)
            
            # í† ë„ˆë¨¼íŠ¸ íš¨ê³¼ ë¶„ì„
            correlations['tournament_effects'] = self.analyze_tournament_effects(news_data)
            
            conn.close()
            return correlations
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ì—°ê´€ì„± ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {}
            
    def find_site_news_mentions(self, site_name, news_data):
        """ì‚¬ì´íŠ¸ë³„ ë‰´ìŠ¤ ì–¸ê¸‰ ì°¾ê¸°"""
        mentions = []
        
        # ì‚¬ì´íŠ¸ëª… ë³€í˜• íŒ¨í„´
        site_patterns = self.generate_site_search_patterns(site_name)
        
        for row in news_data:
            title, content, category, author, pub_date, url = row
            text = (title + ' ' + (content or '')).lower()
            
            for pattern in site_patterns:
                if pattern.lower() in text:
                    mentions.append({
                        'site': site_name,
                        'news_title': title,
                        'category': category,
                        'published_date': pub_date,
                        'url': url,
                        'mention_context': self.extract_mention_context(text, pattern),
                        'relevance_score': self.calculate_relevance_score(title, content, pattern)
                    })
                    break  # ì¤‘ë³µ ë°©ì§€
                    
        return mentions
        
    def generate_site_search_patterns(self, site_name):
        """ì‚¬ì´íŠ¸ ê²€ìƒ‰ íŒ¨í„´ ìƒì„±"""
        patterns = [site_name]
        
        # ë¸Œëœë“œë³„ ì¶”ê°€ íŒ¨í„´
        if 'PokerStars' in site_name:
            patterns.extend(['pokerstars', 'stars', 'ps'])
        elif 'GGPoker' in site_name or 'GGNetwork' in site_name:
            patterns.extend(['ggpoker', 'ggnetwork', 'gg poker'])
        elif 'WPT' in site_name:
            patterns.extend(['wpt global', 'world poker tour'])
        elif 'iPoker' in site_name:
            patterns.extend(['ipoker network'])
            
        return patterns
        
    def extract_mention_context(self, text, pattern):
        """ì–¸ê¸‰ ë§¥ë½ ì¶”ì¶œ"""
        pattern_pos = text.find(pattern.lower())
        if pattern_pos == -1:
            return ""
            
        # ì•ë’¤ 50ìì”© ì¶”ì¶œ
        start = max(0, pattern_pos - 50)
        end = min(len(text), pattern_pos + len(pattern) + 50)
        context = text[start:end].strip()
        
        return context
        
    def calculate_relevance_score(self, title, content, pattern):
        """ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        score = 0
        
        # ì œëª©ì— ìˆìœ¼ë©´ ë†’ì€ ì ìˆ˜
        if pattern.lower() in title.lower():
            score += 3
            
        # ë‚´ìš©ì— ìˆìœ¼ë©´ ì ìˆ˜ ì¶”ê°€
        if content and pattern.lower() in content.lower():
            score += 1
            
        # ì˜¨ë¼ì¸ í¬ì»¤ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì¶”ê°€ ì ìˆ˜
        poker_keywords = ['online poker', 'tournament', 'cash game', 'promotion', 'bonus']
        text = (title + ' ' + (content or '')).lower()
        
        for keyword in poker_keywords:
            if keyword in text:
                score += 0.5
                
        return round(score, 1)
        
    def analyze_indirect_correlations(self, news_data, anomalies):
        """ê°„ì ‘ ì—°ê´€ì„± ë¶„ì„"""
        correlations = []
        
        # ì£¼ìš” í‚¤ì›Œë“œì™€ ì´ìƒ ì§•í›„ ë§¤í•‘
        correlation_keywords = {
            'promotion': 'í”„ë¡œëª¨ì…˜ íš¨ê³¼',
            'bonus': 'ë³´ë„ˆìŠ¤ ì´ë²¤íŠ¸ ì˜í–¥',
            'tournament': 'í† ë„ˆë¨¼íŠ¸ ì‹œì¦Œ íš¨ê³¼',
            'regulation': 'ê·œì œ ë³€í™” ì˜í–¥',
            'partnership': 'íŒŒíŠ¸ë„ˆì‹­ ë°œí‘œ íš¨ê³¼',
            'update': 'í”Œë«í¼ ì—…ë°ì´íŠ¸ ì˜í–¥'
        }
        
        for row in news_data:
            title, content, category, author, pub_date, url = row
            text = (title + ' ' + (content or '')).lower()
            
            for keyword, effect_type in correlation_keywords.items():
                if keyword in text:
                    correlations.append({
                        'news_title': title,
                        'effect_type': effect_type,
                        'published_date': pub_date,
                        'potential_impact': self.assess_potential_impact(keyword, text),
                        'affected_segments': self.identify_affected_segments(keyword)
                    })
                    
        return correlations[:10]  # ìƒìœ„ 10ê°œë§Œ
        
    def assess_potential_impact(self, keyword, text):
        """ì ì¬ì  ì˜í–¥ í‰ê°€"""
        impact_indicators = {
            'major': ['major', 'significant', 'huge', 'massive'],
            'moderate': ['new', 'launch', 'introduce'],
            'minor': ['small', 'minor', 'slight']
        }
        
        for level, indicators in impact_indicators.items():
            if any(indicator in text for indicator in indicators):
                return level.upper()
                
        return "UNKNOWN"
        
    def identify_affected_segments(self, keyword):
        """ì˜í–¥ë°›ëŠ” ì„¸ê·¸ë¨¼íŠ¸ ì‹ë³„"""
        segment_mapping = {
            'promotion': ['ëª¨ë“  ì‚¬ì´íŠ¸', 'ìºì‹œê²Œì„', 'í† ë„ˆë¨¼íŠ¸'],
            'bonus': ['ì‹ ê·œ ê°€ì…ì', 'ê¸°ì¡´ í”Œë ˆì´ì–´'],
            'tournament': ['í† ë„ˆë¨¼íŠ¸ ì¤‘ì‹¬ ì‚¬ì´íŠ¸', 'ë©”ì´ì € ì‚¬ì´íŠ¸'],
            'regulation': ['ì „ì²´ ì‹œì¥', 'íŠ¹ì • ì§€ì—­'],
            'partnership': ['ê´€ë ¨ ë¸Œëœë“œ', 'ì œíœ´ ì‚¬ì´íŠ¸'],
            'update': ['í”Œë«í¼ ì‚¬ìš©ì', 'ëª¨ë°”ì¼ í”Œë ˆì´ì–´']
        }
        
        return segment_mapping.get(keyword, ['ì „ì²´ ì‹œì¥'])
        
    def identify_market_trend_news(self, news_data):
        """ì‹œì¥ íŠ¸ë Œë“œ ê´€ë ¨ ë‰´ìŠ¤ ì‹ë³„"""
        trend_news = []
        
        trend_keywords = ['market', 'growth', 'decline', 'trend', 'industry', 'revenue', 'player count']
        
        for row in news_data:
            title, content, category, author, pub_date, url = row
            text = (title + ' ' + (content or '')).lower()
            
            if any(keyword in text for keyword in trend_keywords):
                trend_news.append({
                    'title': title,
                    'category': category,
                    'published_date': pub_date,
                    'trend_indicators': [kw for kw in trend_keywords if kw in text],
                    'market_relevance': 'HIGH' if 'online poker' in text else 'MEDIUM'
                })
                
        return trend_news[:5]
        
    def analyze_regulatory_news_impact(self, news_data):
        """ê·œì œ ë‰´ìŠ¤ ì˜í–¥ ë¶„ì„"""
        regulatory_news = []
        
        regulatory_keywords = ['regulation', 'legal', 'law', 'government', 'license', 'ban', 'approval']
        
        for row in news_data:
            title, content, category, author, pub_date, url = row
            text = (title + ' ' + (content or '')).lower()
            
            if any(keyword in text for keyword in regulatory_keywords):
                regulatory_news.append({
                    'title': title,
                    'regulatory_type': [kw for kw in regulatory_keywords if kw in text],
                    'published_date': pub_date,
                    'impact_assessment': 'POSITIVE' if any(word in text for word in ['approval', 'legal', 'license']) else 'NEGATIVE' if any(word in text for word in ['ban', 'illegal']) else 'NEUTRAL'
                })
                
        return regulatory_news[:3]
        
    def analyze_tournament_effects(self, news_data):
        """í† ë„ˆë¨¼íŠ¸ íš¨ê³¼ ë¶„ì„"""
        tournament_news = []
        
        tournament_keywords = ['wsop', 'wpt', 'ept', 'tournament', 'bracelet', 'main event']
        
        for row in news_data:
            title, content, category, author, pub_date, url = row
            text = (title + ' ' + (content or '')).lower()
            
            if any(keyword in text for keyword in tournament_keywords):
                tournament_news.append({
                    'title': title,
                    'tournament_type': [kw for kw in tournament_keywords if kw in text],
                    'published_date': pub_date,
                    'expected_impact': 'HIGH' if 'wsop' in text else 'MEDIUM'
                })
                
        return tournament_news[:5]
        
    def generate_infographic_sections(self):
        """ì¸í¬ê·¸ë˜í”½ ì„¹ì…˜ ìƒì„±"""
        logger.info("  ğŸ¨ ì¸í¬ê·¸ë˜í”½ ì„¹ì…˜ ìƒì„±...")
        
        market_overview = self.analyze_complete_market_overview()
        sites_analysis = self.analyze_all_active_sites()
        anomalies = self.detect_market_anomalies()
        
        sections = {
            'header_stats': self.create_header_statistics(market_overview),
            'market_composition': self.create_market_composition_chart(market_overview),
            'top_performers_grid': self.create_top_performers_grid(sites_analysis),
            'anomaly_alerts': self.create_anomaly_alerts_section(anomalies),
            'trend_indicators': self.create_trend_indicators(sites_analysis),
            'risk_dashboard': self.create_risk_dashboard(anomalies),
            'news_impact_summary': self.create_news_impact_summary()
        }
        
        return sections
        
    def create_header_statistics(self, market_overview):
        """í—¤ë” í†µê³„ ìƒì„±"""
        return {
            'primary_metrics': [
                {
                    'label': 'ì „ì²´ ì˜¨ë¼ì¸ í”Œë ˆì´ì–´',
                    'value': f"{market_overview.get('total_market_size', 0):,}ëª…",
                    'icon': 'ğŸ‘¥',
                    'trend': 'stable'
                },
                {
                    'label': 'í™œì„± í¬ì»¤ ì‚¬ì´íŠ¸',
                    'value': f"{market_overview.get('total_active_sites', 0)}ê°œ",
                    'icon': 'ğŸ¢',
                    'trend': 'stable'
                },
                {
                    'label': 'ì‹œì¥ ì§‘ì¤‘ë„',
                    'value': market_overview.get('market_concentration', {}).get('concentration_level', 'N/A'),
                    'icon': 'ğŸ“Š',
                    'trend': 'neutral'
                }
            ],
            'secondary_metrics': [
                {
                    'label': 'ìºì‹œê²Œì„ í”Œë ˆì´ì–´',
                    'value': f"{market_overview.get('total_cash_players', 0):,}ëª…",
                    'percentage': market_overview.get('player_distribution', {}).get('cash_vs_tournament_ratio', '').split(' : ')[0] if ' : ' in market_overview.get('player_distribution', {}).get('cash_vs_tournament_ratio', '') else '0%'
                },
                {
                    'label': 'í† ë„ˆë¨¼íŠ¸ í”Œë ˆì´ì–´',
                    'value': f"{market_overview.get('total_tournament_players', 0):,}ëª…",
                    'percentage': market_overview.get('player_distribution', {}).get('cash_vs_tournament_ratio', '').split(' : ')[1] if ' : ' in market_overview.get('player_distribution', {}).get('cash_vs_tournament_ratio', '') else '0%'
                }
            ]
        }
        
    def create_market_composition_chart(self, market_overview):
        """ì‹œì¥ êµ¬ì„± ì°¨íŠ¸ ë°ì´í„°"""
        segmentation = market_overview.get('market_segmentation', {})
        
        return {
            'chart_type': 'donut_chart',
            'title': 'ì‚¬ì´íŠ¸ ê·œëª¨ë³„ ì‹œì¥ êµ¬ì„±',
            'data': [
                {
                    'category': 'ëŒ€í˜• ì‚¬ì´íŠ¸ (1ë§Œëª…+)',
                    'count': segmentation.get('large_sites_count', 0),
                    'market_share': segmentation.get('large_sites_share', 0),
                    'color': '#FF6B6B'
                },
                {
                    'category': 'ì¤‘í˜• ì‚¬ì´íŠ¸ (1ì²œ-1ë§Œëª…)',
                    'count': segmentation.get('medium_sites_count', 0),
                    'market_share': segmentation.get('medium_sites_share', 0),
                    'color': '#4ECDC4'
                },
                {
                    'category': 'ì†Œí˜• ì‚¬ì´íŠ¸ (1ì²œëª… ë¯¸ë§Œ)',
                    'count': segmentation.get('small_sites_count', 0) + segmentation.get('micro_sites_count', 0),
                    'market_share': 100 - segmentation.get('large_sites_share', 0) - segmentation.get('medium_sites_share', 0),
                    'color': '#45B7D1'
                }
            ]
        }
        
    def create_top_performers_grid(self, sites_analysis):
        """ìƒìœ„ ì„±ê³¼ì ê·¸ë¦¬ë“œ"""
        sites = sites_analysis.get('sites_analysis', [])[:12]  # ìƒìœ„ 12ê°œ
        
        grid_data = []
        for site in sites:
            grid_data.append({
                'rank': site['rank'],
                'name': site['name'],
                'players': site['metrics']['total_players'],
                'market_share': site['percentages']['market_share'],
                'growth_rate': site['percentages']['growth_rate'],
                'trend_indicator': self.get_trend_indicator(site['percentages']['growth_rate']),
                'size_category': site['classification']['size_category'],
                'notable_features': site['notable_features'][:2],  # ìƒìœ„ 2ê°œ íŠ¹ì§•ë§Œ
                'alert_level': self.get_alert_level(site['anomaly_flags'])
            })
            
        return {
            'grid_type': 'performance_grid',
            'title': 'TOP 12 í¬ì»¤ ì‚¬ì´íŠ¸ í˜„í™©',
            'data': grid_data
        }
        
    def get_trend_indicator(self, growth_rate):
        """íŠ¸ë Œë“œ ì§€ì‹œì"""
        if growth_rate > 10:
            return {'icon': 'ğŸ“ˆ', 'color': 'green', 'label': 'ìƒìŠ¹'}
        elif growth_rate < -10:
            return {'icon': 'ğŸ“‰', 'color': 'red', 'label': 'í•˜ë½'}
        else:
            return {'icon': 'â¡ï¸', 'color': 'blue', 'label': 'ì•ˆì •'}
            
    def get_alert_level(self, anomaly_flags):
        """ì•Œë¦¼ ë ˆë²¨ ê²°ì •"""
        if not anomaly_flags:
            return 'normal'
        elif len(anomaly_flags) >= 2:
            return 'high'
        else:
            return 'medium'
            
    def create_anomaly_alerts_section(self, anomalies):
        """ì´ìƒ ì§•í›„ ì•Œë¦¼ ì„¹ì…˜"""
        significant_changes = anomalies.get('significant_changes', [])
        
        alerts = []
        for change in significant_changes[:5]:  # ìƒìœ„ 5ê°œ
            alerts.append({
                'site': change['site'],
                'alert_type': change['change_type'],
                'severity': change['severity'],
                'change_value': f"{change['growth_rate']:+.1f}%",
                'current_players': change['current_players'],
                'impact_assessment': change['impact_assessment'],
                'alert_color': 'red' if change['severity'] == 'HIGH' else 'orange'
            })
            
        return {
            'section_type': 'alert_dashboard',
            'title': 'ğŸš¨ ì¤‘ìš” ë³€í™” ê°ì§€',
            'alerts': alerts,
            'summary': f"{len(significant_changes)}ê°œ ì‚¬ì´íŠ¸ì—ì„œ ì¤‘ìš”í•œ ë³€í™” ê°ì§€"
        }
        
    def create_trend_indicators(self, sites_analysis):
        """íŠ¸ë Œë“œ ì§€ì‹œì ìƒì„±"""
        stats = sites_analysis.get('statistics_summary', {})
        growth_stats = stats.get('growth_statistics', {})
        
        return {
            'section_type': 'trend_metrics',
            'title': 'ğŸ“Š ì‹œì¥ íŠ¸ë Œë“œ ì§€í‘œ',
            'metrics': [
                {
                    'label': 'í‰ê·  ì„±ì¥ë¥ ',
                    'value': f"{growth_stats.get('average_growth_rate', 0):+.1f}%",
                    'trend': 'positive' if growth_stats.get('average_growth_rate', 0) > 0 else 'negative'
                },
                {
                    'label': 'ì„±ì¥ ì‚¬ì´íŠ¸ ìˆ˜',
                    'value': f"{growth_stats.get('positive_growth_sites', 0)}ê°œ",
                    'total': growth_stats.get('positive_growth_sites', 0) + growth_stats.get('negative_growth_sites', 0)
                },
                {
                    'label': 'ì‹œì¥ ë³€ë™ì„±',
                    'value': f"{growth_stats.get('growth_rate_std', 0):.1f}%",
                    'level': 'high' if growth_stats.get('growth_rate_std', 0) > 20 else 'normal'
                }
            ]
        }
        
    def create_risk_dashboard(self, anomalies):
        """ë¦¬ìŠ¤í¬ ëŒ€ì‹œë³´ë“œ"""
        risk_factors = anomalies.get('risk_factors', [])
        
        return {
            'section_type': 'risk_assessment',
            'title': 'âš ï¸ ë¦¬ìŠ¤í¬ í‰ê°€',
            'risk_factors': risk_factors,
            'overall_risk_level': self.calculate_overall_risk_level(risk_factors),
            'recommendations': self.generate_risk_recommendations(risk_factors)
        }
        
    def calculate_overall_risk_level(self, risk_factors):
        """ì „ì²´ ë¦¬ìŠ¤í¬ ë ˆë²¨ ê³„ì‚°"""
        if not risk_factors:
            return 'LOW'
            
        high_risks = len([r for r in risk_factors if r.get('level') == 'HIGH'])
        medium_risks = len([r for r in risk_factors if r.get('level') == 'MEDIUM'])
        
        if high_risks >= 2:
            return 'HIGH'
        elif high_risks >= 1 or medium_risks >= 3:
            return 'MEDIUM'
        else:
            return 'LOW'
            
    def generate_risk_recommendations(self, risk_factors):
        """ë¦¬ìŠ¤í¬ ê¶Œê³ ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        for risk in risk_factors:
            if risk.get('level') == 'HIGH':
                recommendations.append(f"ì¦‰ì‹œ ëª¨ë‹ˆí„°ë§ í•„ìš”: {risk.get('description')}")
            elif risk.get('level') == 'MEDIUM':
                recommendations.append(f"ì£¼ì˜ ê¹Šê²Œ ê´€ì°°: {risk.get('description')}")
                
        return recommendations[:3]  # ìƒìœ„ 3ê°œ ê¶Œê³ ì‚¬í•­
        
    def create_news_impact_summary(self):
        """ë‰´ìŠ¤ ì˜í–¥ ìš”ì•½"""
        correlations = self.analyze_news_correlations()
        
        return {
            'section_type': 'news_impact',
            'title': 'ğŸ“° ë‰´ìŠ¤ ì˜í–¥ ë¶„ì„',
            'direct_mentions_count': len(correlations.get('direct_mentions', [])),
            'indirect_correlations_count': len(correlations.get('indirect_correlations', [])),
            'regulatory_impact_count': len(correlations.get('regulatory_impact', [])),
            'tournament_effects_count': len(correlations.get('tournament_effects', [])),
            'key_insights': self.extract_key_news_insights(correlations)
        }
        
    def extract_key_news_insights(self, correlations):
        """ì£¼ìš” ë‰´ìŠ¤ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
        insights = []
        
        # ì§ì ‘ ì–¸ê¸‰ì´ ìˆëŠ” ì‚¬ì´íŠ¸
        direct_sites = set(mention['site'] for mention in correlations.get('direct_mentions', []))
        if direct_sites:
            insights.append(f"{len(direct_sites)}ê°œ ì‚¬ì´íŠ¸ê°€ ë‰´ìŠ¤ì— ì§ì ‘ ì–¸ê¸‰ë¨")
            
        # í† ë„ˆë¨¼íŠ¸ íš¨ê³¼
        tournament_count = len(correlations.get('tournament_effects', []))
        if tournament_count > 0:
            insights.append(f"{tournament_count}ê°œ í† ë„ˆë¨¼íŠ¸ ê´€ë ¨ ë‰´ìŠ¤ ê°ì§€")
            
        # ê·œì œ ì˜í–¥
        regulatory_count = len(correlations.get('regulatory_impact', []))
        if regulatory_count > 0:
            insights.append(f"{regulatory_count}ê°œ ê·œì œ ê´€ë ¨ ë‰´ìŠ¤ í™•ì¸")
            
        return insights[:3]
        
    def generate_actionable_insights(self):
        """ì‹¤í–‰ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        logger.info("  ğŸ’¡ ì‹¤í–‰ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ ìƒì„±...")
        
        market_overview = self.analyze_complete_market_overview()
        sites_analysis = self.analyze_all_active_sites()
        anomalies = self.detect_market_anomalies()
        
        insights = {
            'immediate_actions': [],
            'monitoring_priorities': [],
            'market_opportunities': [],
            'broadcast_talking_points': []
        }
        
        # ì¦‰ì‹œ ì¡°ì¹˜ ì‚¬í•­
        significant_changes = anomalies.get('significant_changes', [])
        for change in significant_changes[:3]:
            if change['severity'] == 'HIGH':
                insights['immediate_actions'].append({
                    'priority': 'HIGH',
                    'action': f"{change['site']} ê¸‰ë³€ ì›ì¸ ì¡°ì‚¬",
                    'description': f"{change['growth_rate']:+.1f}% ë³€í™” ì›ì¸ ë¶„ì„ í•„ìš”"
                })
                
        # ëª¨ë‹ˆí„°ë§ ìš°ì„ ìˆœìœ„
        risk_factors = anomalies.get('risk_factors', [])
        for risk in risk_factors:
            insights['monitoring_priorities'].append({
                'priority': risk.get('level', 'MEDIUM'),
                'focus_area': risk.get('type'),
                'description': risk.get('description')
            })
            
        # ì‹œì¥ ê¸°íšŒ
        growth_sites = [site for site in sites_analysis.get('sites_analysis', []) 
                       if site['percentages']['growth_rate'] > 15]
        for site in growth_sites[:3]:
            insights['market_opportunities'].append({
                'opportunity': f"{site['name']} ì„±ì¥ ëª¨ë©˜í…€",
                'description': f"{site['percentages']['growth_rate']:+.1f}% ì„±ì¥ ì¤‘",
                'potential': 'ë†’ìŒ'
            })
            
        # ë°©ì†¡ìš© í•µì‹¬ í¬ì¸íŠ¸
        insights['broadcast_talking_points'] = self.generate_broadcast_talking_points(
            market_overview, significant_changes
        )
        
        return insights
        
    def generate_broadcast_talking_points(self, market_overview, significant_changes):
        """ë°©ì†¡ìš© í•µì‹¬ í¬ì¸íŠ¸ ìƒì„±"""
        talking_points = []
        
        # ì‹œì¥ ê·œëª¨ í¬ì¸íŠ¸
        total_players = market_overview.get('total_market_size', 0)
        talking_points.append({
            'category': 'ì‹œì¥ í˜„í™©',
            'point': f"í˜„ì¬ ì „ ì„¸ê³„ ì˜¨ë¼ì¸ í¬ì»¤ ì‹œì¥ì—ëŠ” {total_players:,}ëª…ì´ ë™ì‹œ ì ‘ì† ì¤‘",
            'visual_cue': 'ğŸ“Š ì „ì²´ í”Œë ˆì´ì–´ ìˆ˜ ê·¸ë˜í”„'
        })
        
        # ì£¼ìš” ë³€í™” í¬ì¸íŠ¸
        if significant_changes:
            biggest_change = max(significant_changes, key=lambda x: abs(x['growth_rate']))
            talking_points.append({
                'category': 'ì£¼ìš” ë³€í™”',
                'point': f"{biggest_change['site']}ì—ì„œ {biggest_change['growth_rate']:+.1f}% ê¸‰ë³€ ê°ì§€",
                'visual_cue': f"ğŸš¨ {biggest_change['site']} ë³€í™” ì°¨íŠ¸"
            })
            
        # ì‹œì¥ ì§‘ì¤‘ë„ í¬ì¸íŠ¸
        concentration = market_overview.get('market_concentration', {})
        top5_share = concentration.get('top5_market_share', 0)
        talking_points.append({
            'category': 'ì‹œì¥ êµ¬ì¡°',
            'point': f"ìƒìœ„ 5ê°œ ì‚¬ì´íŠ¸ê°€ ì „ì²´ ì‹œì¥ì˜ {top5_share:.1f}% ì ìœ ",
            'visual_cue': 'ğŸ† ì‹œì¥ ì ìœ ìœ¨ íŒŒì´ ì°¨íŠ¸'
        })
        
        return talking_points
        
    def generate_executive_summary(self, report_data):
        """ê²½ì˜ì§„ ìš”ì•½ ìƒì„±"""
        logger.info("  ğŸ“‹ ê²½ì˜ì§„ ìš”ì•½ ìƒì„±...")
        
        market_overview = report_data.get('market_overview', {})
        anomalies = report_data.get('anomaly_detection', {})
        
        return {
            'key_findings': [
                f"ì´ {market_overview.get('total_active_sites', 0)}ê°œ í™œì„± ì‚¬ì´íŠ¸ì—ì„œ {market_overview.get('total_market_size', 0):,}ëª… ë™ì‹œ ì ‘ì†",
                f"{len(anomalies.get('significant_changes', []))}ê°œ ì‚¬ì´íŠ¸ì—ì„œ ì¤‘ìš”í•œ ë³€í™” ê°ì§€",
                f"ì‹œì¥ ì§‘ì¤‘ë„: {market_overview.get('market_concentration', {}).get('concentration_level', 'N/A')}"
            ],
            'critical_alerts': [change['site'] + ': ' + f"{change['growth_rate']:+.1f}%" 
                              for change in anomalies.get('significant_changes', [])[:3]],
            'market_health': self.assess_market_health(market_overview, anomalies),
            'recommended_actions': [insight['action'] for insight in 
                                  report_data.get('actionable_insights', {}).get('immediate_actions', [])[:3]]
        }
        
    def assess_market_health(self, market_overview, anomalies):
        """ì‹œì¥ ê±´ì „ì„± í‰ê°€"""
        health_factors = []
        
        # ì‹œì¥ ì§‘ì¤‘ë„ í‰ê°€
        hhi = market_overview.get('market_concentration', {}).get('hhi_index', 0)
        if hhi < 1500:
            health_factors.append('ê²½ìŸì ')
        elif hhi < 2500:
            health_factors.append('ë³´í†µ ì§‘ì¤‘')
        else:
            health_factors.append('ê³ ë„ ì§‘ì¤‘')
            
        # ë³€í™” ì•ˆì •ì„± í‰ê°€
        significant_changes = len(anomalies.get('significant_changes', []))
        if significant_changes < 3:
            health_factors.append('ì•ˆì •ì ')
        elif significant_changes < 6:
            health_factors.append('ë³€ë™ì ')
        else:
            health_factors.append('ë¶ˆì•ˆì •')
            
        return ' + '.join(health_factors)
        
    def save_infographic_report(self, report_data):
        """ì¸í¬ê·¸ë˜í”½ ë¦¬í¬íŠ¸ ì €ì¥"""
        logger.info("ğŸ’¾ ì¸í¬ê·¸ë˜í”½ ë¦¬í¬íŠ¸ ì €ì¥...")
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # JSON ìƒì„¸ ë¦¬í¬íŠ¸
        json_filename = f'infographic_report_{timestamp}.json'
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
            
        # ë°©ì†¡ìš© ìš”ì•½ ë¦¬í¬íŠ¸
        summary_filename = f'broadcast_summary_{timestamp}.txt'
        self.save_broadcast_summary(report_data, summary_filename)
        
        # ì¸í¬ê·¸ë˜í”½ ë°ì´í„° íŒŒì¼
        infographic_filename = f'infographic_data_{timestamp}.json'
        self.save_infographic_data(report_data, infographic_filename)
        
        logger.info(f"ğŸ“Š ìƒì„¸ ë¶„ì„: {json_filename}")
        logger.info(f"ğŸ“º ë°©ì†¡ ìš”ì•½: {summary_filename}")
        logger.info(f"ğŸ¨ ì¸í¬ê·¸ë˜í”½ ë°ì´í„°: {infographic_filename}")
        
        return json_filename, summary_filename, infographic_filename
        
    def save_broadcast_summary(self, report_data, filename):
        """ë°©ì†¡ìš© ìš”ì•½ ì €ì¥"""
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("ğŸ“º ì˜¨ë¼ì¸ í¬ì»¤ ì‹œì¥ ì¸í¬ê·¸ë˜í”½ ë¸Œë¦¬í•‘\n")
            f.write(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„')}\n")
            f.write("=" * 80 + "\n\n")
            
            # ê²½ì˜ì§„ ìš”ì•½
            executive = report_data.get('executive_summary', {})
            f.write("ğŸ¯ í•µì‹¬ ìš”ì•½\n")
            f.write("-" * 40 + "\n")
            for finding in executive.get('key_findings', []):
                f.write(f"  â€¢ {finding}\n")
            f.write(f"\nì‹œì¥ ê±´ì „ì„±: {executive.get('market_health', 'N/A')}\n\n")
            
            # ì¤‘ìš” ì•Œë¦¼
            if executive.get('critical_alerts'):
                f.write("ğŸš¨ ê¸´ê¸‰ ì•Œë¦¼\n")
                f.write("-" * 40 + "\n")
                for alert in executive.get('critical_alerts', []):
                    f.write(f"  âš ï¸ {alert}\n")
                f.write("\n")
                
            # ë°©ì†¡ìš© í•µì‹¬ í¬ì¸íŠ¸
            talking_points = report_data.get('actionable_insights', {}).get('broadcast_talking_points', [])
            f.write("ğŸ“º ë°©ì†¡ í•µì‹¬ í¬ì¸íŠ¸\n")
            f.write("-" * 40 + "\n")
            for point in talking_points:
                f.write(f"  {point['category']}: {point['point']}\n")
                f.write(f"     ì‹œê° ìë£Œ: {point['visual_cue']}\n\n")
                
            # ê¶Œì¥ ì¡°ì¹˜
            if executive.get('recommended_actions'):
                f.write("ğŸ’¡ ê¶Œì¥ ì¡°ì¹˜ì‚¬í•­\n")
                f.write("-" * 40 + "\n")
                for action in executive.get('recommended_actions', []):
                    f.write(f"  â€¢ {action}\n")
                    
    def save_infographic_data(self, report_data, filename):
        """ì¸í¬ê·¸ë˜í”½ ë°ì´í„° ì €ì¥"""
        infographic_data = {
            'metadata': report_data.get('report_metadata', {}),
            'sections': report_data.get('infographic_sections', {}),
            'data_points': self.extract_key_data_points(report_data)
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(infographic_data, f, indent=2, ensure_ascii=False)
            
    def extract_key_data_points(self, report_data):
        """ì£¼ìš” ë°ì´í„° í¬ì¸íŠ¸ ì¶”ì¶œ"""
        market_overview = report_data.get('market_overview', {})
        anomalies = report_data.get('anomaly_detection', {})
        
        return {
            'total_players': market_overview.get('total_market_size', 0),
            'active_sites': market_overview.get('total_active_sites', 0),
            'significant_changes': len(anomalies.get('significant_changes', [])),
            'market_concentration': market_overview.get('market_concentration', {}).get('hhi_index', 0),
            'top3_share': market_overview.get('market_concentration', {}).get('top3_market_share', 0)
        }

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¨ ê³ ê¸‰ ì¸í¬ê·¸ë˜í”½ í¬ì»¤ ì‹œì¥ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±ê¸°")
    print("=" * 70)
    
    generator = AdvancedInfographicReportGenerator()
    
    try:
        # ì¢…í•© ì¸í¬ê·¸ë˜í”½ ë¦¬í¬íŠ¸ ìƒì„±
        print("\nğŸ”„ ì¢…í•© ë¶„ì„ ì¤‘...")
        report_data = generator.generate_comprehensive_infographic_report()
        
        # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
        print("\nğŸ“Š ë¶„ì„ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°:")
        executive = report_data.get('executive_summary', {})
        
        print("í•µì‹¬ ë°œê²¬ì‚¬í•­:")
        for finding in executive.get('key_findings', [])[:3]:
            print(f"  â€¢ {finding}")
            
        critical_alerts = executive.get('critical_alerts', [])
        if critical_alerts:
            print(f"\nğŸš¨ ê¸´ê¸‰ ì•Œë¦¼: {len(critical_alerts)}ê±´")
            for alert in critical_alerts[:2]:
                print(f"  âš ï¸ {alert}")
                
        print(f"\nì‹œì¥ ê±´ì „ì„±: {executive.get('market_health', 'N/A')}")
        
        # ë‰´ìŠ¤ ì—°ê´€ì„±
        correlations = report_data.get('news_correlation', {})
        direct_mentions = len(correlations.get('direct_mentions', []))
        if direct_mentions > 0:
            print(f"ë‰´ìŠ¤ ì—°ê´€ì„±: {direct_mentions}ê°œ ì‚¬ì´íŠ¸ ì§ì ‘ ì–¸ê¸‰ ë°œê²¬")
            
        # ë¦¬í¬íŠ¸ ì €ì¥
        print("\nğŸ’¾ ë¦¬í¬íŠ¸ ì €ì¥ ì¤‘...")
        json_file, summary_file, infographic_file = generator.save_infographic_report(report_data)
        
        print(f"\nâœ… ê³ ê¸‰ ì¸í¬ê·¸ë˜í”½ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ!")
        print(f"ğŸ“Š ìƒì„¸ ë¶„ì„: {json_file}")
        print(f"ğŸ“º ë°©ì†¡ ìš”ì•½: {summary_file}")
        print(f"ğŸ¨ ì¸í¬ê·¸ë˜í”½ ë°ì´í„°: {infographic_file}")
        
        return True
        
    except Exception as e:
        logger.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì˜¤ë¥˜: {str(e)}")
        return False

if __name__ == "__main__":
    success = main()
    if success:
        print(f"\nğŸš€ ì¸í¬ê·¸ë˜í”½ ë¦¬í¬íŠ¸ ì™„ë£Œ! ëª¨ë“  í™œì„± ì‚¬ì´íŠ¸ ìˆ˜ì¹˜ ë¶„ì„ ë° ë‰´ìŠ¤ ì—°ê´€ì„± ì™„ë£Œ")
    else:
        print(f"\nğŸ’€ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨ - ë¬¸ì œ í•´ê²° í•„ìš”")