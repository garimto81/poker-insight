#!/usr/bin/env python3
"""
SQLite ë°ì´í„°ë¥¼ Supabaseë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜
ê¸°ì¡´ ë¡œì»¬ ë°ì´í„°ë² ì´ìŠ¤ì˜ ë°ì´í„°ë¥¼ Supabaseë¡œ ì´ì „í•©ë‹ˆë‹¤.
"""

import os
import sqlite3
import json
import logging
from datetime import datetime
from typing import List, Dict
from supabase_config import SupabaseClient

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataMigrator:
    """ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.supabase_client = SupabaseClient(use_service_key=True)
        self.sqlite_dbs = [
            'github_actions_fallback.db',
            'poker_insight.db',
            'gg_poker_monitoring.db',
            'online_poker_data.db'
        ]
    
    def get_sqlite_data(self, db_path: str) -> List[Dict]:
        """SQLiteì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
        data = []
        
        if not os.path.exists(db_path):
            logger.warning(f"âš ï¸ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ì—†ìŒ: {db_path}")
            return data
        
        try:
            conn = sqlite3.connect(db_path)
            conn.row_factory = sqlite3.Row  # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜
            cursor = conn.cursor()
            
            # daily_traffic í…Œì´ë¸” í™•ì¸
            cursor.execute("""
                SELECT name FROM sqlite_master 
                WHERE type='table' AND name='daily_traffic'
            """)
            
            if cursor.fetchone():
                cursor.execute("""
                    SELECT 
                        site_name,
                        collection_date,
                        collection_time,
                        players_online,
                        cash_players,
                        peak_24h,
                        seven_day_avg,
                        created_at
                    FROM daily_traffic
                    ORDER BY collection_date DESC, collection_time DESC
                """)
                
                rows = cursor.fetchall()
                for row in rows:
                    data.append({
                        'site_name': row['site_name'],
                        'collection_date': row['collection_date'],
                        'collection_time': row['collection_time'],
                        'players_online': row['players_online'] or 0,
                        'cash_players': row['cash_players'] or 0,
                        'peak_24h': row['peak_24h'] or 0,
                        'seven_day_avg': row['seven_day_avg'] or 0,
                        'created_at': row.get('created_at', datetime.now().isoformat())
                    })
                
                logger.info(f"âœ… {db_path}ì—ì„œ {len(data)}ê°œ ë ˆì½”ë“œ ì¶”ì¶œ")
            
            conn.close()
            
        except Exception as e:
            logger.error(f"âŒ {db_path} ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return data
    
    def merge_and_deduplicate(self, all_data: List[List[Dict]]) -> List[Dict]:
        """ì—¬ëŸ¬ ë°ì´í„°ë² ì´ìŠ¤ì˜ ë°ì´í„°ë¥¼ ë³‘í•©í•˜ê³  ì¤‘ë³µ ì œê±°"""
        merged = {}
        
        for data_list in all_data:
            for record in data_list:
                # ê³ ìœ  í‚¤ ìƒì„± (ì‚¬ì´íŠ¸ëª… + ë‚ ì§œ + ì‹œê°„)
                key = f"{record['site_name']}_{record['collection_date']}_{record['collection_time']}"
                
                # ì¤‘ë³µì´ë©´ ë” ìµœì‹  ë°ì´í„° ìœ ì§€
                if key not in merged:
                    merged[key] = record
                elif record.get('created_at', '') > merged[key].get('created_at', ''):
                    merged[key] = record
        
        result = list(merged.values())
        logger.info(f"âœ… ì¤‘ë³µ ì œê±° í›„ {len(result)}ê°œ ë ˆì½”ë“œ")
        return result
    
    def batch_insert(self, data: List[Dict], batch_size: int = 100) -> bool:
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„° ì‚½ì…"""
        total_inserted = 0
        
        for i in range(0, len(data), batch_size):
            batch = data[i:i + batch_size]
            
            try:
                if self.supabase_client.insert_daily_traffic(batch):
                    total_inserted += len(batch)
                    logger.info(f"âœ… ë°°ì¹˜ {i//batch_size + 1}: {len(batch)}ê°œ ì‚½ì… ì™„ë£Œ")
                else:
                    logger.error(f"âŒ ë°°ì¹˜ {i//batch_size + 1} ì‚½ì… ì‹¤íŒ¨")
                    
            except Exception as e:
                logger.error(f"âŒ ë°°ì¹˜ ì‚½ì… ì˜¤ë¥˜: {e}")
        
        logger.info(f"ğŸ¯ ì´ {total_inserted}ê°œ ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ")
        return total_inserted > 0
    
    def run_migration(self) -> bool:
        """ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰"""
        logger.info("ğŸš€ SQLite â†’ Supabase ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")
        
        # 1. Supabase ì—°ê²° í…ŒìŠ¤íŠ¸
        if not self.supabase_client.test_connection():
            logger.error("âŒ Supabase ì—°ê²° ì‹¤íŒ¨")
            return False
        
        # 2. í…Œì´ë¸” ìƒì„±
        if not self.supabase_client.create_tables():
            logger.error("âŒ í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨")
            return False
        
        # 3. ëª¨ë“  SQLite ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„° ì¶”ì¶œ
        all_data = []
        for db_name in self.sqlite_dbs:
            db_path = os.path.join('.', db_name)
            data = self.get_sqlite_data(db_path)
            if data:
                all_data.append(data)
        
        if not all_data:
            logger.warning("âš ï¸ ë§ˆì´ê·¸ë ˆì´ì…˜í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        # 4. ë°ì´í„° ë³‘í•© ë° ì¤‘ë³µ ì œê±°
        merged_data = self.merge_and_deduplicate(all_data)
        
        if not merged_data:
            logger.warning("âš ï¸ ë³‘í•©í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        # 5. Supabaseì— ë°ì´í„° ì‚½ì…
        success = self.batch_insert(merged_data)
        
        if success:
            logger.info("âœ… ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ")
            
            # 6. ë§ˆì´ê·¸ë ˆì´ì…˜ ê²°ê³¼ ê²€ì¦
            dashboard_data = self.supabase_client.get_dashboard_data()
            if dashboard_data:
                logger.info("ğŸ“Š ë§ˆì´ê·¸ë ˆì´ì…˜ ê²°ê³¼:")
                logger.info(f"   - ì‚¬ì´íŠ¸ ìˆ˜: {dashboard_data['summary']['total_sites']}")
                logger.info(f"   - ì´ í”Œë ˆì´ì–´: {dashboard_data['summary']['latest_total_players']:,}")
                logger.info(f"   - ë°ì´í„° í¬ì¸íŠ¸: {dashboard_data['summary']['data_points']}")
        
        return success

def create_env_template():
    """í™˜ê²½ë³€ìˆ˜ í…œí”Œë¦¿ íŒŒì¼ ìƒì„±"""
    env_template = """# Supabase ì„¤ì •
SUPABASE_URL=https://your-project-id.supabase.co
SUPABASE_ANON_KEY=your-anon-key-here
SUPABASE_SERVICE_KEY=your-service-role-key-here

# ì‚¬ìš©ë²•:
# 1. Supabase í”„ë¡œì íŠ¸ë¥¼ ìƒì„±í•˜ì„¸ìš”
# 2. Settings > APIì—ì„œ URLê³¼ í‚¤ë“¤ì„ ë³µì‚¬í•˜ì„¸ìš”
# 3. ì´ íŒŒì¼ì„ .envë¡œ ì €ì¥í•˜ê³  ì‹¤ì œ ê°’ìœ¼ë¡œ êµì²´í•˜ì„¸ìš”
# 4. python migrate_to_supabase.py ì‹¤í–‰í•˜ì„¸ìš”
"""
    
    with open('.env.template', 'w', encoding='utf-8') as f:
        f.write(env_template)
    
    logger.info("ğŸ“„ .env.template íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ SQLite â†’ Supabase ë§ˆì´ê·¸ë ˆì´ì…˜ ë„êµ¬")
    print("=" * 50)
    
    # í™˜ê²½ë³€ìˆ˜ í™•ì¸
    supabase_url = os.getenv('SUPABASE_URL', '')
    if not supabase_url or supabase_url == 'https://your-project.supabase.co':
        print("âŒ í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¼í•˜ì„¸ìš”:")
        print("   1. Supabase í”„ë¡œì íŠ¸ ìƒì„±")
        print("   2. .env íŒŒì¼ì— URLê³¼ í‚¤ ì„¤ì •")
        print("   3. ìŠ¤í¬ë¦½íŠ¸ ì¬ì‹¤í–‰")
        
        create_env_template()
        return False
    
    # ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
    migrator = DataMigrator()
    success = migrator.run_migration()
    
    if success:
        print("\nğŸ‰ ë§ˆì´ê·¸ë ˆì´ì…˜ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„: í”„ë¡ íŠ¸ì—”ë“œì—ì„œ Supabase API ì—°ë™")
    else:
        print("\nâŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨")
    
    return success

if __name__ == "__main__":
    main()