#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í¬ì»¤ íŠ¸ë Œë“œ íŒ¨í„´ ë¶„ì„ê¸°
"""
import sys
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')

import json
import logging
from datetime import datetime, timedelta
from collections import defaultdict, Counter
import statistics
import re
from sqlalchemy import create_engine, func
from sqlalchemy.orm import sessionmaker

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PokerTrendAnalyzer:
    def __init__(self, db_path='poker_insight.db'):
        self.engine = create_engine(f'sqlite:///{db_path}')
        SessionLocal = sessionmaker(bind=self.engine)
        self.session = SessionLocal()
        
        # íŠ¸ë Œë“œ ë¶„ë¥˜ ì„ê³„ê°’
        self.GROWTH_THRESHOLDS = {
            'rapid_growth': 20,    # 20% ì´ìƒ ê¸‰ì„±ì¥
            'growth': 5,           # 5% ì´ìƒ ì„±ì¥
            'stable': -5,          # -5% ~ 5% ì•ˆì •
            'decline': -15,        # -15% ~ -5% í•˜ë½
            'rapid_decline': -100  # -15% ì´í•˜ ê¸‰ë½
        }
        
    def analyze_traffic_trends(self):
        """íŠ¸ë˜í”½ íŠ¸ë Œë“œ ë¶„ì„"""
        logger.info("ğŸ“ˆ í¬ì»¤ íŠ¸ë˜í”½ íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘...")
        
        # í˜„ì¬ ë°ì´í„°ëŠ” ë‹¨ì¼ ì‹œì ì´ë¯€ë¡œ ëª¨ì˜ íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ìƒì„±
        trends = self.simulate_historical_trends()
        
        # ì‹¤ì œ í˜„ì¬ ë°ì´í„° ë¶„ì„
        current_analysis = self.analyze_current_market()
        
        # íŠ¸ë Œë“œ ê²°í•©
        trends['current_analysis'] = current_analysis
        
        return trends
        
    def simulate_historical_trends(self):
        """íˆìŠ¤í† ë¦¬ì»¬ íŠ¸ë Œë“œ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì‹¤ì œ ë°ì´í„° ì‚¬ìš©)"""
        logger.info("  ğŸ“Š íˆìŠ¤í† ë¦¬ì»¬ íŠ¸ë Œë“œ ì‹œë®¬ë ˆì´ì…˜...")
        
        # ì£¼ìš” ì‚¬ì´íŠ¸ë“¤ì˜ ê°€ìƒ 7ì¼ê°„ ë°ì´í„°
        historical_data = {
            'GGNetwork': [125000, 128000, 132000, 130000, 134304, 135000, 133000],
            'PokerStars': [58000, 57500, 56000, 55000, 55540, 54000, 53500],
            'PokerStars.it': [12000, 11800, 11500, 11200, 11145, 11000, 10800],
            'GGPoker ON': [4200, 4400, 4600, 4500, 4693, 4800, 4750],
            'WPT Global': [2800, 2900, 3100, 3000, 2989, 3200, 3150]
        }
        
        trends = {}
        
        for site, data in historical_data.items():
            # 7ì¼ í‰ê·  ê³„ì‚°
            avg_7days = statistics.mean(data)
            current = data[-2]  # í˜„ì¬ ê°’ (ë§ˆì§€ë§‰ì—ì„œ ë‘ ë²ˆì§¸)
            previous = statistics.mean(data[:-2])  # ì´ì „ í‰ê· 
            
            # ì„±ì¥ë¥  ê³„ì‚°
            growth_rate = ((current - previous) / previous) * 100 if previous > 0 else 0
            
            # íŠ¸ë Œë“œ ë¶„ë¥˜
            trend_type = self.classify_trend(growth_rate)
            
            # ë³€ë™ì„± ê³„ì‚°
            volatility = statistics.stdev(data) if len(data) > 1 else 0
            
            trends[site] = {
                'current_players': current,
                'avg_7days': round(avg_7days),
                'growth_rate': round(growth_rate, 2),
                'trend_type': trend_type,
                'volatility': round(volatility),
                'momentum': self.calculate_momentum(data),
                'forecast_24h': self.forecast_next_24h(data),
                'daily_data': data
            }
            
        return trends
        
    def classify_trend(self, growth_rate):
        """ì„±ì¥ë¥ ì— ë”°ë¥¸ íŠ¸ë Œë“œ ë¶„ë¥˜"""
        if growth_rate >= self.GROWTH_THRESHOLDS['rapid_growth']:
            return 'ê¸‰ì„±ì¥'
        elif growth_rate >= self.GROWTH_THRESHOLDS['growth']:
            return 'ì„±ì¥'
        elif growth_rate >= self.GROWTH_THRESHOLDS['stable']:
            return 'ì•ˆì •'
        elif growth_rate >= self.GROWTH_THRESHOLDS['decline']:
            return 'í•˜ë½'
        else:
            return 'ê¸‰ë½'
            
    def calculate_momentum(self, data):
        """ëª¨ë©˜í…€ ê³„ì‚° (ìµœê·¼ 3ì¼ vs ì´ì „ 4ì¼)"""
        if len(data) < 7:
            return 0
            
        recent_3 = statistics.mean(data[-3:])
        previous_4 = statistics.mean(data[:4])
        
        momentum = ((recent_3 - previous_4) / previous_4) * 100 if previous_4 > 0 else 0
        return round(momentum, 2)
        
    def forecast_next_24h(self, data):
        """ê°„ë‹¨í•œ 24ì‹œê°„ ì˜ˆì¸¡ (ì„ í˜• ì¶”ì„¸ ê¸°ë°˜)"""
        if len(data) < 3:
            return data[-1] if data else 0
            
        # ìµœê·¼ 3ì¼ ë°ì´í„°ë¡œ ì„ í˜• ì¶”ì„¸ ê³„ì‚°
        recent_data = data[-3:]
        trend = (recent_data[-1] - recent_data[0]) / 2
        forecast = recent_data[-1] + trend
        
        return max(0, round(forecast))
        
    def analyze_current_market(self):
        """í˜„ì¬ ì‹œì¥ ìƒí™© ë¶„ì„"""
        logger.info("  ğŸ† í˜„ì¬ ì‹œì¥ ìƒí™© ë¶„ì„...")
        
        try:
            # SQLite í…Œì´ë¸” ì§ì ‘ ì¿¼ë¦¬ (sqlite_integration.pyì—ì„œ ìƒì„±ëœ êµ¬ì¡° ì‚¬ìš©)
            query = """
            SELECT 
                ps.name,
                td.total_players,
                td.cash_players,
                td.tournament_players,
                td.seven_day_average,
                td.rank
            FROM poker_sites ps
            JOIN traffic_data td ON ps.id = td.site_id
            ORDER BY td.total_players DESC
            LIMIT 20
            """
            
            result = self.engine.execute(query)
            sites_data = result.fetchall()
            
            total_players = sum(row[1] for row in sites_data)
            total_cash = sum(row[2] for row in sites_data)
            total_tournaments = sum(row[3] for row in sites_data)
            
            # ì‹œì¥ ì§‘ì¤‘ë„ ê³„ì‚° (ìƒìœ„ 3ê°œ ì‚¬ì´íŠ¸)
            top3_share = sum(row[1] for row in sites_data[:3]) / total_players * 100
            
            # í™œì„± ì‚¬ì´íŠ¸ ìˆ˜ (í”Œë ˆì´ì–´ > 0)
            active_sites = len([row for row in sites_data if row[1] > 0])
            
            market_analysis = {
                'total_players_online': total_players,
                'total_cash_players': total_cash,
                'total_tournament_players': total_tournaments,
                'active_sites_count': active_sites,
                'top3_market_share': round(top3_share, 1),
                'cash_tournament_ratio': round((total_cash / total_tournaments) * 100, 1) if total_tournaments > 0 else 0,
                'market_leader': sites_data[0][0] if sites_data else 'N/A',
                'market_leader_share': round((sites_data[0][1] / total_players) * 100, 1) if sites_data and total_players > 0 else 0
            }
            
            return market_analysis
            
        except Exception as e:
            logger.error(f"ì‹œì¥ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {}
            
    def analyze_news_trends(self):
        """ë‰´ìŠ¤ ê¸°ë°˜ íŠ¸ë Œë“œ ë¶„ì„"""
        logger.info("ğŸ“° ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘...")
        
        try:
            # ë‰´ìŠ¤ ë°ì´í„° ì¿¼ë¦¬
            query = """
            SELECT title, content, category, published_date
            FROM news_items
            ORDER BY scraped_at DESC
            LIMIT 50
            """
            
            result = self.engine.execute(query)
            news_data = result.fetchall()
            
            # í‚¤ì›Œë“œ ë¶„ì„
            keywords_analysis = self.analyze_keywords(news_data)
            
            # ì¹´í…Œê³ ë¦¬ ë¶„ì„
            categories_analysis = self.analyze_categories(news_data)
            
            # ê°ì • ë¶„ì„ (ë‹¨ìˆœ í‚¤ì›Œë“œ ê¸°ë°˜)
            sentiment_analysis = self.analyze_sentiment(news_data)
            
            return {
                'keywords': keywords_analysis,
                'categories': categories_analysis,
                'sentiment': sentiment_analysis,
                'total_articles_analyzed': len(news_data)
            }
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {}
            
    def analyze_keywords(self, news_data):
        """í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„"""
        # ì£¼ìš” í¬ì»¤ ê´€ë ¨ í‚¤ì›Œë“œë“¤
        poker_keywords = [
            'WSOP', 'PokerStars', 'GGPoker', 'GGNetwork', '888poker', 
            'partypoker', 'WPT', 'tournament', 'bracelet', 'high stakes',
            'Main Event', 'World Series', 'EPT', 'online poker'
        ]
        
        keyword_counts = Counter()
        
        for row in news_data:
            title = row[0] or ''
            content = row[1] or ''
            text = (title + ' ' + content).lower()
            
            for keyword in poker_keywords:
                count = text.count(keyword.lower())
                if count > 0:
                    keyword_counts[keyword] += count
                    
        # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ
        top_keywords = dict(keyword_counts.most_common(10))
        
        return top_keywords
        
    def analyze_categories(self, news_data):
        """ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„"""
        category_counts = Counter()
        
        for row in news_data:
            category = row[2] or 'general'
            category_counts[category] += 1
            
        return dict(category_counts)
        
    def analyze_sentiment(self, news_data):
        """ê°ì • ë¶„ì„ (ë‹¨ìˆœ í‚¤ì›Œë“œ ê¸°ë°˜)"""
        positive_words = ['win', 'winner', 'champion', 'success', 'record', 'best', 'good', 'great', 'amazing']
        negative_words = ['lose', 'lost', 'fail', 'worst', 'bad', 'terrible', 'scandal', 'controversy']
        
        positive_count = 0
        negative_count = 0
        neutral_count = 0
        
        for row in news_data:
            title = row[0] or ''
            content = row[1] or ''
            text = (title + ' ' + content).lower()
            
            pos_score = sum(text.count(word) for word in positive_words)
            neg_score = sum(text.count(word) for word in negative_words)
            
            if pos_score > neg_score:
                positive_count += 1
            elif neg_score > pos_score:
                negative_count += 1
            else:
                neutral_count += 1
                
        total = len(news_data)
        
        return {
            'positive': round((positive_count / total) * 100, 1) if total > 0 else 0,
            'negative': round((negative_count / total) * 100, 1) if total > 0 else 0,
            'neutral': round((neutral_count / total) * 100, 1) if total > 0 else 0,
            'sentiment_score': positive_count - negative_count
        }
        
    def detect_anomalies(self, trends_data):
        """ì´ìƒ íŒ¨í„´ ê°ì§€"""
        logger.info("ğŸš¨ ì´ìƒ íŒ¨í„´ ê°ì§€ ì¤‘...")
        
        anomalies = []
        
        for site, data in trends_data.items():
            if isinstance(data, dict):
                growth_rate = data.get('growth_rate', 0)
                volatility = data.get('volatility', 0)
                current_players = data.get('current_players', 0)
                
                # ê¸‰ê²©í•œ ë³€í™” ê°ì§€
                if abs(growth_rate) > 15:
                    anomalies.append({
                        'site': site,
                        'type': 'ê¸‰ê²©í•œ ë³€í™”',
                        'description': f"{growth_rate:+.1f}% ë³€í™”",
                        'severity': 'high' if abs(growth_rate) > 25 else 'medium',
                        'current_players': current_players
                    })
                    
                # ë†’ì€ ë³€ë™ì„± ê°ì§€
                if volatility > current_players * 0.1:  # í‰ê· ì˜ 10% ì´ìƒ ë³€ë™
                    anomalies.append({
                        'site': site,
                        'type': 'ë†’ì€ ë³€ë™ì„±',
                        'description': f"í‘œì¤€í¸ì°¨: {volatility:,.0f}",
                        'severity': 'medium',
                        'current_players': current_players
                    })
                    
        return sorted(anomalies, key=lambda x: x['current_players'], reverse=True)
        
    def generate_broadcast_insights(self, trends_data, news_analysis, anomalies):
        """ë°©ì†¡ìš© ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        logger.info("ğŸ“º ë°©ì†¡ìš© ì¸ì‚¬ì´íŠ¸ ìƒì„± ì¤‘...")
        
        insights = {
            'breaking_news': [],
            'market_highlights': [],
            'trend_alerts': [],
            'talking_points': []
        }
        
        # ë¸Œë ˆì´í‚¹ ë‰´ìŠ¤ ìˆ˜ì¤€ì˜ ë³€í™”
        for anomaly in anomalies:
            if anomaly['severity'] == 'high':
                insights['breaking_news'].append(
                    f"ğŸš¨ {anomaly['site']}ì—ì„œ {anomaly['description']} ê°ì§€!"
                )
                
        # ì‹œì¥ í•˜ì´ë¼ì´íŠ¸
        if 'current_analysis' in trends_data:
            market = trends_data['current_analysis']
            insights['market_highlights'] = [
                f"ì „ì²´ ì˜¨ë¼ì¸ í”Œë ˆì´ì–´: {market.get('total_players_online', 0):,}ëª…",
                f"ì‹œì¥ ë¦¬ë”: {market.get('market_leader', 'N/A')} ({market.get('market_leader_share', 0)}% ì ìœ )",
                f"ìƒìœ„ 3ê°œ ì‚¬ì´íŠ¸ ì ìœ ìœ¨: {market.get('top3_market_share', 0)}%",
                f"ìºì‹œê²Œì„ vs í† ë„ˆë¨¼íŠ¸ ë¹„ìœ¨: {market.get('cash_tournament_ratio', 0)}:100"
            ]
            
        # íŠ¸ë Œë“œ ì•Œë¦¼
        for site, data in trends_data.items():
            if isinstance(data, dict) and data.get('trend_type') in ['ê¸‰ì„±ì¥', 'ê¸‰ë½']:
                insights['trend_alerts'].append(
                    f"{site}: {data['trend_type']} ({data['growth_rate']:+.1f}%)"
                )
                
        # ë°©ì†¡ ì¤‘ ì–¸ê¸‰í•  í¬ì¸íŠ¸
        top_growth = max(
            [(site, data.get('growth_rate', 0)) for site, data in trends_data.items() if isinstance(data, dict)],
            key=lambda x: x[1],
            default=('', 0)
        )
        
        if top_growth[1] > 0:
            insights['talking_points'].append(
                f"ì´ë²ˆ ì£¼ ê°€ì¥ ì£¼ëª©ë°›ëŠ” ì‚¬ì´íŠ¸ëŠ” {top_growth[0]}ì…ë‹ˆë‹¤. {top_growth[1]:+.1f}% ì„±ì¥ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤."
            )
            
        # ë‰´ìŠ¤ ê¸°ë°˜ í¬ì¸íŠ¸
        if news_analysis.get('keywords'):
            top_keyword = max(news_analysis['keywords'].items(), key=lambda x: x[1])
            insights['talking_points'].append(
                f"ìµœê·¼ í¬ì»¤ ë‰´ìŠ¤ì—ì„œ ê°€ì¥ ë§ì´ ì–¸ê¸‰ëœ í‚¤ì›Œë“œëŠ” '{top_keyword[0]}'ì…ë‹ˆë‹¤."
            )
            
        return insights
        
    def save_analysis_results(self, results):
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        output = {
            'analysis_timestamp': datetime.utcnow().isoformat(),
            'analysis_type': 'comprehensive_trend_analysis',
            'results': results
        }
        
        with open('trend_analysis_results.json', 'w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
            
        logger.info("ğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥: trend_analysis_results.json")
        
    def close(self):
        """ì„¸ì…˜ ì¢…ë£Œ"""
        self.session.close()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ“Š í¬ì»¤ íŠ¸ë Œë“œ íŒ¨í„´ ë¶„ì„ ì‹œì‘")
    print("="*50)
    
    analyzer = PokerTrendAnalyzer()
    
    try:
        # 1. íŠ¸ë˜í”½ íŠ¸ë Œë“œ ë¶„ì„
        traffic_trends = analyzer.analyze_traffic_trends()
        
        # 2. ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„
        news_trends = analyzer.analyze_news_trends()
        
        # 3. ì´ìƒ íŒ¨í„´ ê°ì§€
        anomalies = analyzer.detect_anomalies(traffic_trends)
        
        # 4. ë°©ì†¡ìš© ì¸ì‚¬ì´íŠ¸ ìƒì„±
        broadcast_insights = analyzer.generate_broadcast_insights(
            traffic_trends, news_trends, anomalies
        )
        
        # 5. ê²°ê³¼ ì¶œë ¥
        print("\nğŸ† TOP 5 ì‚¬ì´íŠ¸ íŠ¸ë Œë“œ:")
        for i, (site, data) in enumerate(list(traffic_trends.items())[:5]):
            if isinstance(data, dict):
                print(f"  {i+1}. {site}")
                print(f"     í˜„ì¬: {data.get('current_players', 0):,}ëª…")
                print(f"     íŠ¸ë Œë“œ: {data.get('trend_type', 'N/A')} ({data.get('growth_rate', 0):+.1f}%)")
                print(f"     ì˜ˆì¸¡: {data.get('forecast_24h', 0):,}ëª…")
                
        print(f"\nğŸ“° ë‰´ìŠ¤ íŠ¸ë Œë“œ:")
        if news_trends.get('keywords'):
            print("  ì¸ê¸° í‚¤ì›Œë“œ:")
            for keyword, count in list(news_trends['keywords'].items())[:5]:
                print(f"    - {keyword}: {count}íšŒ ì–¸ê¸‰")
                
        print(f"\nğŸš¨ ì´ìƒ íŒ¨í„´:")
        for anomaly in anomalies[:3]:
            print(f"  - {anomaly['site']}: {anomaly['description']}")
            
        print(f"\nğŸ“º ë°©ì†¡ìš© í¬ì¸íŠ¸:")
        for point in broadcast_insights['talking_points']:
            print(f"  â€¢ {point}")
            
        # 6. ê²°ê³¼ ì €ì¥
        results = {
            'traffic_trends': traffic_trends,
            'news_trends': news_trends,
            'anomalies': anomalies,
            'broadcast_insights': broadcast_insights
        }
        
        analyzer.save_analysis_results(results)
        
        print(f"\nâœ… íŠ¸ë Œë“œ ë¶„ì„ ì™„ë£Œ!")
        print(f"ğŸ“Š ë¶„ì„ ê²°ê³¼: trend_analysis_results.json")
        
        return True
        
    except Exception as e:
        logger.error(f"ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        return False
    finally:
        analyzer.close()

if __name__ == "__main__":
    success = main()
    if success:
        print(f"\nğŸš€ ë‹¤ìŒ ë‹¨ê³„: ë‰´ìŠ¤ ë¶„ì„ ê¸°ëŠ¥ ê°œë°œ ë˜ëŠ” ëŒ€ì‹œë³´ë“œ êµ¬í˜„!")
    else:
        print(f"\nğŸ’€ íŠ¸ë Œë“œ ë¶„ì„ ì‹¤íŒ¨ - ë¬¸ì œ í•´ê²° í•„ìš”")