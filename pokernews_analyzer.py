#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PokerNews.com ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„ ë° í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸
"""
import sys
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')

import cloudscraper
from bs4 import BeautifulSoup
import json
import re
from datetime import datetime, timedelta

class PokerNewsAnalyzer:
    def __init__(self):
        self.scraper = cloudscraper.create_scraper()
        
    def analyze_pokernews_structure(self):
        """PokerNews ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„"""
        print("ğŸ” PokerNews.com êµ¬ì¡° ë¶„ì„ ì¤‘...")
        
        urls_to_test = [
            'https://www.pokernews.com',
            'https://www.pokernews.com/news/',
            'https://www.pokernews.com/strategy/',
            'https://www.pokernews.com/tournaments/',
            'https://www.pokernews.com/live/',
        ]
        
        for url in urls_to_test:
            try:
                print(f"\nğŸ“„ ë¶„ì„ ì¤‘: {url}")
                response = self.scraper.get(url, timeout=15)
                
                if response.status_code == 200:
                    print(f"âœ… ì ‘ê·¼ ì„±ê³µ! í¬ê¸°: {len(response.content)} bytes")
                    
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # ê¸°ì‚¬ ê´€ë ¨ ìš”ì†Œë“¤ ì°¾ê¸°
                    self.find_article_elements(soup, url)
                    
                else:
                    print(f"âŒ ì ‘ê·¼ ì‹¤íŒ¨: HTTP {response.status_code}")
                    
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜: {str(e)}")
                
    def find_article_elements(self, soup, url):
        """ê¸°ì‚¬ ê´€ë ¨ ìš”ì†Œë“¤ ì°¾ê¸°"""
        print("  ğŸ” ê¸°ì‚¬ ìš”ì†Œ íƒìƒ‰:")
        
        # 1. article íƒœê·¸
        articles = soup.find_all('article')
        print(f"    â€¢ article íƒœê·¸: {len(articles)}ê°œ")
        
        # 2. ë‹¤ì–‘í•œ ë‰´ìŠ¤ ê´€ë ¨ í´ë˜ìŠ¤ë“¤
        news_classes = [
            'news-item', 'article-item', 'post', 'entry',
            'story', 'content-item', 'news-card', 'article-card',
            'post-item', 'news-list-item'
        ]
        
        for class_name in news_classes:
            elements = soup.find_all(class_=class_name)
            if elements:
                print(f"    â€¢ .{class_name}: {len(elements)}ê°œ")
                
        # 3. ë§í¬ íŒ¨í„´ ë¶„ì„
        links = soup.find_all('a', href=True)
        news_links = []
        pattern = r'/(news|article|story|post)/'
        
        for link in links:
            href = link.get('href', '')
            if re.search(pattern, href) and link.text.strip():
                news_links.append({
                    'url': href,
                    'title': link.text.strip()[:50] + '...' if len(link.text.strip()) > 50 else link.text.strip()
                })
                
        print(f"    â€¢ ë‰´ìŠ¤ ë§í¬ íŒ¨í„´: {len(news_links)}ê°œ")
        
        # ìƒ˜í”Œ ë§í¬ ì¶œë ¥
        if news_links:
            print("    ğŸ“ ìƒ˜í”Œ ë‰´ìŠ¤ ë§í¬:")
            for i, link in enumerate(news_links[:3]):
                print(f"      {i+1}. {link['title']}")
                
        # 4. ì œëª© íƒœê·¸ë“¤
        headings = soup.find_all(['h1', 'h2', 'h3', 'h4'])
        print(f"    â€¢ ì œëª© íƒœê·¸: {len(headings)}ê°œ")
        
        # 5. íŠ¹ì • URLì—ì„œ ë” ìì„¸í•œ ë¶„ì„
        if 'news' in url:
            self.analyze_news_page(soup)
            
    def analyze_news_page(self, soup):
        """ë‰´ìŠ¤ í˜ì´ì§€ ìƒì„¸ ë¶„ì„"""
        print("  ğŸ“° ë‰´ìŠ¤ í˜ì´ì§€ ìƒì„¸ ë¶„ì„:")
        
        # HTML êµ¬ì¡° ì €ì¥
        with open('pokernews_structure.html', 'w', encoding='utf-8') as f:
            f.write(str(soup.prettify()))
        print("    ğŸ’¾ HTML êµ¬ì¡° ì €ì¥: pokernews_structure.html")
        
        # ì ì¬ì  ê¸°ì‚¬ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
        potential_containers = soup.find_all(['div', 'section', 'main'], 
                                           class_=re.compile(r'news|article|post|story|content'))
        
        print(f"    â€¢ ì ì¬ì  ê¸°ì‚¬ ì»¨í…Œì´ë„ˆ: {len(potential_containers)}ê°œ")
        
        for i, container in enumerate(potential_containers[:3]):
            classes = container.get('class', [])
            print(f"      ì»¨í…Œì´ë„ˆ {i+1}: {classes}")
            
    def test_news_extraction(self):
        """ë‰´ìŠ¤ ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ§ª PokerNews ë‰´ìŠ¤ ì¶”ì¶œ í…ŒìŠ¤íŠ¸...")
        
        try:
            response = self.scraper.get('https://www.pokernews.com/news/', timeout=15)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ë°©ë²• 1: article íƒœê·¸ì—ì„œ ì¶”ì¶œ
                articles = self.extract_from_articles(soup)
                if articles:
                    print(f"âœ… ë°©ë²• 1 ì„±ê³µ: article íƒœê·¸ì—ì„œ {len(articles)}ê°œ ì¶”ì¶œ")
                    return articles
                    
                # ë°©ë²• 2: ë§í¬ íŒ¨í„´ìœ¼ë¡œ ì¶”ì¶œ
                articles = self.extract_from_links(soup)
                if articles:
                    print(f"âœ… ë°©ë²• 2 ì„±ê³µ: ë§í¬ íŒ¨í„´ì—ì„œ {len(articles)}ê°œ ì¶”ì¶œ")
                    return articles
                    
                # ë°©ë²• 3: ì¼ë°˜ì ì¸ ì„ íƒìë¡œ ì¶”ì¶œ
                articles = self.extract_from_selectors(soup)
                if articles:
                    print(f"âœ… ë°©ë²• 3 ì„±ê³µ: ì¼ë°˜ ì„ íƒìì—ì„œ {len(articles)}ê°œ ì¶”ì¶œ")
                    return articles
                    
                print("âŒ ëª¨ë“  ì¶”ì¶œ ë°©ë²• ì‹¤íŒ¨")
                
            else:
                print(f"âŒ í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨: HTTP {response.status_code}")
                
        except Exception as e:
            print(f"âŒ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {str(e)}")
            
        return None
        
    def extract_from_articles(self, soup):
        """article íƒœê·¸ì—ì„œ ë‰´ìŠ¤ ì¶”ì¶œ"""
        articles = soup.find_all('article')
        results = []
        
        for article in articles:
            try:
                # ì œëª© ì°¾ê¸°
                title_elem = article.find(['h1', 'h2', 'h3', 'h4'])
                if not title_elem:
                    continue
                    
                title = title_elem.get_text().strip()
                
                # ë§í¬ ì°¾ê¸°
                link_elem = title_elem.find('a') or article.find('a')
                if link_elem:
                    url = link_elem.get('href', '')
                    if not url.startswith('http'):
                        url = 'https://www.pokernews.com' + url
                else:
                    url = ''
                    
                # ë‚ ì§œ ì°¾ê¸°
                date_elem = article.find('time') or article.find(class_=re.compile(r'date|time'))
                date = date_elem.get_text().strip() if date_elem else ''
                
                # ìš”ì•½ ì°¾ê¸°
                summary_elem = article.find('p')
                summary = summary_elem.get_text().strip() if summary_elem else ''
                
                if title and len(title) > 5:
                    results.append({
                        'title': title,
                        'url': url,
                        'date': date,
                        'summary': summary[:200] + '...' if len(summary) > 200 else summary,
                        'source': 'PokerNews'
                    })
                    
            except Exception:
                continue
                
        return results if len(results) > 3 else None
        
    def extract_from_links(self, soup):
        """ë§í¬ íŒ¨í„´ì—ì„œ ë‰´ìŠ¤ ì¶”ì¶œ"""
        links = soup.find_all('a', href=re.compile(r'/(news|article)/'))
        results = []
        seen_urls = set()
        
        for link in links:
            try:
                url = link.get('href', '')
                if not url.startswith('http'):
                    url = 'https://www.pokernews.com' + url
                    
                if url in seen_urls:
                    continue
                seen_urls.add(url)
                
                title = link.get_text().strip()
                if title and len(title) > 10:
                    results.append({
                        'title': title,
                        'url': url,
                        'date': datetime.now().strftime('%Y-%m-%d'),
                        'summary': '',
                        'source': 'PokerNews'
                    })
                    
            except Exception:
                continue
                
        return results[:20] if len(results) > 5 else None
        
    def extract_from_selectors(self, soup):
        """ì¼ë°˜ì ì¸ CSS ì„ íƒìë¡œ ì¶”ì¶œ"""
        selectors = [
            'div[class*="news"] h3 a',
            'div[class*="article"] h2 a',
            'div[class*="post"] h3 a',
            '.content-item a',
            '.story-item a',
            'h2 a[href*="/news/"]',
            'h3 a[href*="/news/"]'
        ]
        
        for selector in selectors:
            try:
                elements = soup.select(selector)
                if len(elements) > 5:
                    results = []
                    for elem in elements[:15]:
                        title = elem.get_text().strip()
                        url = elem.get('href', '')
                        if not url.startswith('http'):
                            url = 'https://www.pokernews.com' + url
                            
                        if title and len(title) > 10:
                            results.append({
                                'title': title,
                                'url': url,
                                'date': datetime.now().strftime('%Y-%m-%d'),
                                'summary': '',
                                'source': 'PokerNews'
                            })
                            
                    if len(results) > 5:
                        print(f"    ì„±ê³µí•œ ì„ íƒì: {selector}")
                        return results
                        
            except Exception:
                continue
                
        return None
        
    def save_sample_data(self, articles):
        """ìƒ˜í”Œ ë°ì´í„° ì €ì¥"""
        if not articles:
            return False
            
        output = {
            'source': 'PokerNews.com',
            'timestamp': datetime.now().isoformat(),
            'total_articles': len(articles),
            'data': articles
        }
        
        with open('pokernews_sample_data.json', 'w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
            
        return True
        
    def display_results(self, articles):
        """ê²°ê³¼ ì¶œë ¥"""
        if not articles:
            print("âŒ ì¶”ì¶œëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        print(f"\nğŸ“° PokerNews ê¸°ì‚¬ {len(articles)}ê°œ ì¶”ì¶œ ì„±ê³µ!")
        print("="*80)
        
        for i, article in enumerate(articles[:10], 1):
            print(f"{i:2d}. {article['title']}")
            if article['date']:
                print(f"    ğŸ“… {article['date']}")
            if article['summary']:
                print(f"    ğŸ“ {article['summary'][:80]}...")
            print(f"    ğŸ”— {article['url']}")
            print()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ PokerNews.com ë¶„ì„ ë° í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    analyzer = PokerNewsAnalyzer()
    
    # 1. ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„
    analyzer.analyze_pokernews_structure()
    
    # 2. ë‰´ìŠ¤ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
    articles = analyzer.test_news_extraction()
    
    if articles:
        # 3. ê²°ê³¼ ì¶œë ¥
        analyzer.display_results(articles)
        
        # 4. ë°ì´í„° ì €ì¥
        success = analyzer.save_sample_data(articles)
        if success:
            print("ğŸ’¾ ìƒ˜í”Œ ë°ì´í„° ì €ì¥: pokernews_sample_data.json")
            
        print(f"\nâœ… PokerNews í¬ë¡¤ë§ ê°€ëŠ¥!")
        print(f"ğŸ“Š ì¶”ì¶œ ê°€ëŠ¥í•œ ê¸°ì‚¬ ìˆ˜: {len(articles)}ê°œ")
        
    else:
        print(f"\nâŒ PokerNews í¬ë¡¤ë§ ì‹¤íŒ¨")
        print("êµ¬ì¡° ë¶„ì„ íŒŒì¼ì„ í™•ì¸í•˜ì—¬ ìˆ˜ë™ ë””ë²„ê¹… í•„ìš”")

if __name__ == "__main__":
    main()