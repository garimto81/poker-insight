#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GG POKER ì§ì›ìš© ë°ì´í„° ì¤‘ì‹¬ ì˜¨ë¼ì¸ í¬ì»¤ ëª¨ë‹ˆí„°ë§ í”Œë«í¼
- 4ê°œ í•µì‹¬ ì§€í‘œ: Players Online, Cash Players, 24h Peak, 7-day Average
- ë‚ ì§œë³„ ì‹œê³„ì—´ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„
- ê¸‰ë³€ ì‹œì ì˜ ë‰´ìŠ¤ ì—°ê´€ì„± ë¶„ì„
- ì¶”ë¡  ë°°ì œ, ìˆœìˆ˜ ë°ì´í„° ê¸°ë°˜ ë¶„ì„
"""
import sys
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')

import json
import logging
import sqlite3
from datetime import datetime, timedelta
from collections import defaultdict, Counter
import statistics

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class GGPokerMonitoringPlatform:
    def __init__(self, db_path='gg_poker_monitoring.db'):
        self.db_path = db_path
        self.setup_database()
        
        # ê¸‰ë³€ ê°ì§€ ì„ê³„ê°’
        self.SIGNIFICANT_CHANGE_THRESHOLD = 15.0  # 15% ë³€í™”
        self.MAJOR_CHANGE_THRESHOLD = 25.0        # 25% ì£¼ìš” ë³€í™”
        self.ANOMALY_THRESHOLD = 50.0             # 50% ì´ìƒì¹˜
        
    def setup_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì„¤ì •"""
        logger.info("ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì„¤ì •...")
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ì¼ì¼ íŠ¸ë˜í”½ ë°ì´í„° í…Œì´ë¸” (ì‹œê³„ì—´)
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS daily_traffic (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                site_name TEXT NOT NULL,
                collection_date DATE NOT NULL,
                collection_time TIME NOT NULL,
                players_online INTEGER NOT NULL,
                cash_players INTEGER NOT NULL,
                peak_24h INTEGER,
                seven_day_avg INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(site_name, collection_date, collection_time)
            )
        ''')
        
        # ë³€í™” ê°ì§€ ë¡œê·¸ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS change_detection (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                site_name TEXT NOT NULL,
                detection_date DATE NOT NULL,
                metric_type TEXT NOT NULL,
                previous_value INTEGER,
                current_value INTEGER,
                change_percentage REAL,
                change_magnitude TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # ë‰´ìŠ¤-ë³€í™” ì—°ê´€ì„± í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS news_correlation (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                change_detection_id INTEGER,
                news_title TEXT,
                news_url TEXT,
                news_date DATE,
                correlation_score REAL,
                correlation_type TEXT,
                analysis_notes TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (change_detection_id) REFERENCES change_detection (id)
            )
        ''')
        
        # ê²½ìŸì‚¬ ë©”íƒ€ë°ì´í„° í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS competitor_metadata (
                site_name TEXT PRIMARY KEY,
                site_url TEXT,
                network_family TEXT,
                market_tier TEXT,
                monitoring_priority TEXT,
                competitor_category TEXT,
                notes TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
        logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì„¤ì • ì™„ë£Œ")
        
    def collect_daily_data(self, site_data_list):
        """ì¼ì¼ ë°ì´í„° ìˆ˜ì§‘"""
        logger.info("ğŸ“ˆ ì¼ì¼ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        collection_date = datetime.now().strftime('%Y-%m-%d')
        collection_time = datetime.now().strftime('%H:%M:%S')
        
        collected_count = 0
        
        for site_data in site_data_list:
            try:
                cursor.execute('''
                    INSERT OR REPLACE INTO daily_traffic 
                    (site_name, collection_date, collection_time, players_online, 
                     cash_players, peak_24h, seven_day_avg)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', (
                    site_data['site_name'],
                    collection_date,
                    collection_time,
                    site_data['players_online'],
                    site_data['cash_players'],
                    site_data.get('peak_24h', None),
                    site_data.get('seven_day_avg', None)
                ))
                
                collected_count += 1
                
            except Exception as e:
                logger.error(f"ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜ - {site_data['site_name']}: {str(e)}")
                
        conn.commit()
        conn.close()
        
        logger.info(f"âœ… {collected_count}ê°œ ì‚¬ì´íŠ¸ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
        return collected_count
        
    def detect_significant_changes(self, target_date=None):
        """ìœ ì˜ë¯¸í•œ ë³€í™” ê°ì§€"""
        if target_date is None:
            target_date = datetime.now().strftime('%Y-%m-%d')
            
        logger.info(f"ğŸ” {target_date} ë³€í™” ê°ì§€ ì‹œì‘...")
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # í˜„ì¬ ë‚ ì§œì™€ ì´ì „ ë‚ ì§œ ë°ì´í„° ë¹„êµ
        query = '''
            WITH current_data AS (
                SELECT site_name, players_online, cash_players, peak_24h, seven_day_avg
                FROM daily_traffic 
                WHERE collection_date = ?
                ORDER BY collection_time DESC
                LIMIT 50
            ),
            previous_data AS (
                SELECT site_name, players_online, cash_players, peak_24h, seven_day_avg
                FROM daily_traffic 
                WHERE collection_date = date(?, '-1 day')
                ORDER BY collection_time DESC
                LIMIT 50
            )
            SELECT 
                c.site_name,
                c.players_online as current_players,
                p.players_online as previous_players,
                c.cash_players as current_cash,
                p.cash_players as previous_cash,
                c.peak_24h as current_peak,
                p.peak_24h as previous_peak,
                c.seven_day_avg as current_7day,
                p.seven_day_avg as previous_7day
            FROM current_data c
            LEFT JOIN previous_data p ON c.site_name = p.site_name
            WHERE p.site_name IS NOT NULL
        '''
        
        cursor.execute(query, (target_date, target_date))
        results = cursor.fetchall()
        
        detected_changes = []
        
        for row in results:
            site_name = row[0]
            changes = self.analyze_site_changes(row)
            
            for change in changes:
                if change['magnitude'] in ['SIGNIFICANT', 'MAJOR', 'ANOMALY']:
                    # ë³€í™” ê°ì§€ ë¡œê·¸ì— ì €ì¥
                    cursor.execute('''
                        INSERT INTO change_detection 
                        (site_name, detection_date, metric_type, previous_value, 
                         current_value, change_percentage, change_magnitude)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        site_name,
                        target_date,
                        change['metric'],
                        change['previous_value'],
                        change['current_value'],
                        change['change_percentage'],
                        change['magnitude']
                    ))
                    
                    change['change_id'] = cursor.lastrowid
                    detected_changes.append(change)
                    
        conn.commit()
        conn.close()
        
        logger.info(f"ğŸš¨ {len(detected_changes)}ê°œ ìœ ì˜ë¯¸í•œ ë³€í™” ê°ì§€")
        return detected_changes
        
    def analyze_site_changes(self, data_row):
        """ì‚¬ì´íŠ¸ë³„ ë³€í™” ë¶„ì„"""
        site_name, curr_players, prev_players, curr_cash, prev_cash, curr_peak, prev_peak, curr_7day, prev_7day = data_row
        
        changes = []
        
        # ê° ì§€í‘œë³„ ë³€í™”ìœ¨ ê³„ì‚°
        metrics = [
            ('players_online', curr_players, prev_players),
            ('cash_players', curr_cash, prev_cash),
            ('peak_24h', curr_peak, prev_peak),
            ('seven_day_avg', curr_7day, prev_7day)
        ]
        
        for metric_name, current, previous in metrics:
            if current is not None and previous is not None and previous > 0:
                change_percentage = ((current - previous) / previous) * 100
                magnitude = self.classify_change_magnitude(abs(change_percentage))
                
                changes.append({
                    'site_name': site_name,
                    'metric': metric_name,
                    'previous_value': previous,
                    'current_value': current,
                    'change_percentage': round(change_percentage, 2),
                    'magnitude': magnitude,
                    'direction': 'INCREASE' if change_percentage > 0 else 'DECREASE'
                })
                
        return changes
        
    def classify_change_magnitude(self, abs_change_percentage):
        """ë³€í™” í¬ê¸° ë¶„ë¥˜"""
        if abs_change_percentage >= self.ANOMALY_THRESHOLD:
            return 'ANOMALY'
        elif abs_change_percentage >= self.MAJOR_CHANGE_THRESHOLD:
            return 'MAJOR'
        elif abs_change_percentage >= self.SIGNIFICANT_CHANGE_THRESHOLD:
            return 'SIGNIFICANT'
        else:
            return 'MINOR'
            
    def generate_time_series_chart_data(self, site_name, days_back=30):
        """ì‹œê³„ì—´ ì°¨íŠ¸ ë°ì´í„° ìƒì„±"""
        logger.info(f"ğŸ“Š {site_name} ì‹œê³„ì—´ ì°¨íŠ¸ ë°ì´í„° ìƒì„± ({days_back}ì¼)")
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ì¼ë³„ ìµœì‹  ë°ì´í„° ì¡°íšŒ
        query = '''
            SELECT 
                collection_date,
                players_online,
                cash_players,
                peak_24h,
                seven_day_avg
            FROM daily_traffic 
            WHERE site_name = ? 
            AND collection_date >= date('now', '-' || ? || ' days')
            GROUP BY collection_date
            HAVING collection_time = MAX(collection_time)
            ORDER BY collection_date
        '''
        
        cursor.execute(query, (site_name, days_back))
        results = cursor.fetchall()
        
        if not results:
            return None
            
        # ì°¨íŠ¸ ë°ì´í„° êµ¬ì„±
        chart_data = {
            'site_name': site_name,
            'period_days': days_back,
            'data_points': len(results),
            'labels': [],  # Xì¶• ë‚ ì§œ
            'datasets': {
                'players_online': {
                    'label': 'Players Online',
                    'data': [],
                    'borderColor': 'rgb(75, 192, 192)',
                    'backgroundColor': 'rgba(75, 192, 192, 0.2)'
                },
                'cash_players': {
                    'label': 'Cash Players',
                    'data': [],
                    'borderColor': 'rgb(255, 99, 132)',
                    'backgroundColor': 'rgba(255, 99, 132, 0.2)'
                },
                'peak_24h': {
                    'label': '24h Peak',
                    'data': [],
                    'borderColor': 'rgb(54, 162, 235)',
                    'backgroundColor': 'rgba(54, 162, 235, 0.2)'
                },
                'seven_day_avg': {
                    'label': '7-day Average',
                    'data': [],
                    'borderColor': 'rgb(255, 206, 86)',
                    'backgroundColor': 'rgba(255, 206, 86, 0.2)'
                }
            },
            'analytics': {}
        }
        
        for row in results:
            chart_data['labels'].append(row[0])
            chart_data['datasets']['players_online']['data'].append(row[1])
            chart_data['datasets']['cash_players']['data'].append(row[2])
            chart_data['datasets']['peak_24h']['data'].append(row[3] if row[3] is not None else 0)
            chart_data['datasets']['seven_day_avg']['data'].append(row[4] if row[4] is not None else 0)
            
        # ê¸°ë³¸ í†µê³„ ê³„ì‚°
        chart_data['analytics'] = self.calculate_chart_analytics(chart_data)
        
        conn.close()
        return chart_data
        
    def calculate_chart_analytics(self, chart_data):
        """ì°¨íŠ¸ ë¶„ì„ ë°ì´í„° ê³„ì‚°"""
        analytics = {}
        
        for metric_key, dataset in chart_data['datasets'].items():
            values = [v for v in dataset['data'] if v is not None and v > 0]
            
            if values:
                analytics[metric_key] = {
                    'current': values[-1] if values else 0,
                    'min': min(values),
                    'max': max(values),
                    'mean': round(statistics.mean(values), 1),
                    'median': round(statistics.median(values), 1),
                    'std_dev': round(statistics.stdev(values), 1) if len(values) > 1 else 0,
                    'trend': self.calculate_trend(values),
                    'volatility': self.calculate_volatility(values),
                    'recent_change_pct': self.calculate_recent_change(values)
                }
            else:
                analytics[metric_key] = {'no_data': True}
                
        return analytics
        
    def calculate_trend(self, values):
        """íŠ¸ë Œë“œ ê³„ì‚°"""
        if len(values) < 3:
            return 'INSUFFICIENT_DATA'
            
        # ìµœê·¼ 7ì¼ vs ì´ì „ ê¸°ê°„ ë¹„êµ
        if len(values) >= 7:
            recent_avg = statistics.mean(values[-7:])
            previous_avg = statistics.mean(values[:-7]) if len(values) > 7 else statistics.mean(values)
            
            if recent_avg > previous_avg * 1.05:
                return 'UPWARD'
            elif recent_avg < previous_avg * 0.95:
                return 'DOWNWARD'
            else:
                return 'STABLE'
        else:
            # ì²«ê°’ vs ë§ˆì§€ë§‰ê°’
            if values[-1] > values[0] * 1.1:
                return 'UPWARD'
            elif values[-1] < values[0] * 0.9:
                return 'DOWNWARD'
            else:
                return 'STABLE'
                
    def calculate_volatility(self, values):
        """ë³€ë™ì„± ê³„ì‚°"""
        if len(values) < 2:
            return 'LOW'
            
        mean_val = statistics.mean(values)
        std_dev = statistics.stdev(values)
        cv = (std_dev / mean_val) * 100 if mean_val > 0 else 0
        
        if cv >= 20:
            return 'HIGH'
        elif cv >= 10:
            return 'MEDIUM'
        else:
            return 'LOW'
            
    def calculate_recent_change(self, values):
        """ìµœê·¼ ë³€í™”ìœ¨ ê³„ì‚°"""
        if len(values) < 2:
            return 0
            
        current = values[-1]
        previous = values[-2]
        
        if previous > 0:
            return round(((current - previous) / previous) * 100, 2)
        return 0
        
    def setup_competitor_monitoring(self):
        """ê²½ìŸì‚¬ ëª¨ë‹ˆí„°ë§ ì„¤ì • (GG POKER í¬í•¨)"""
        logger.info("ğŸ¯ ê²½ìŸì‚¬ ëª¨ë‹ˆí„°ë§ ì„¤ì • (GG POKER í¬í•¨)...")
        
        # GG POKERì™€ ê²½ìŸì‚¬ ë¶„ë¥˜
        monitoring_sites = [
            # GG POKER (ìì‚¬ ë°ì´í„°)
            {'name': 'GGNetwork', 'priority': 'CRITICAL', 'category': 'OWN', 'tier': 'GG_POKER'},
            {'name': 'GGPoker ON', 'priority': 'CRITICAL', 'category': 'OWN', 'tier': 'GG_POKER'},
            
            # Tier 1 ì§ì ‘ ê²½ìŸì‚¬ (ëŒ€í˜• ê¸€ë¡œë²Œ í”Œë«í¼)
            {'name': 'PokerStars', 'priority': 'HIGH', 'category': 'DIRECT', 'tier': 'Tier1'},
            {'name': 'PokerStars Ontario', 'priority': 'HIGH', 'category': 'DIRECT', 'tier': 'Tier1'},
            {'name': 'PokerStars.it', 'priority': 'MEDIUM', 'category': 'DIRECT', 'tier': 'Tier1'},
            
            # Tier 2 ê°„ì ‘ ê²½ìŸì‚¬ (ì¤‘í˜• í”Œë«í¼)
            {'name': '888poker', 'priority': 'MEDIUM', 'category': 'INDIRECT', 'tier': 'Tier2'},
            {'name': 'partypoker', 'priority': 'MEDIUM', 'category': 'INDIRECT', 'tier': 'Tier2'},
            {'name': 'WPT Global', 'priority': 'HIGH', 'category': 'INDIRECT', 'tier': 'Tier2'},
            
            # Tier 3 í‹ˆìƒˆ ê²½ìŸì‚¬ (ë„¤íŠ¸ì›Œí¬/ì§€ì—­ íŠ¹í™”)
            {'name': 'iPoker', 'priority': 'LOW', 'category': 'NICHE', 'tier': 'Tier3'},
            {'name': 'Chico Poker', 'priority': 'LOW', 'category': 'NICHE', 'tier': 'Tier3'},
            {'name': 'Winamax', 'priority': 'LOW', 'category': 'NICHE', 'tier': 'Tier3'},
        ]
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        for site in monitoring_sites:
            network_info = 'GG Network' if site['category'] == 'OWN' else 'Independent'
            notes = f"GG POKER ìì‚¬ ë°ì´í„°" if site['category'] == 'OWN' else f"GG POKER ê²½ìŸì‚¬ ëª¨ë‹ˆí„°ë§ - {site['category']} ê²½ìŸ"
            
            cursor.execute('''
                INSERT OR REPLACE INTO competitor_metadata 
                (site_name, network_family, market_tier, monitoring_priority, 
                 competitor_category, notes)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                site['name'],
                network_info,
                site['tier'],
                site['priority'],
                site['category'],
                notes
            ))
            
        conn.commit()
        conn.close()
        
        logger.info(f"âœ… {len(monitoring_sites)}ê°œ ì‚¬ì´íŠ¸ ëª¨ë‹ˆí„°ë§ ì„¤ì • ì™„ë£Œ (GG POKER í¬í•¨)")
        
    def get_competitor_dashboard_data(self):
        """ê²½ìŸì‚¬ ëŒ€ì‹œë³´ë“œ ë°ì´í„° (GG POKER í¬í•¨)"""
        logger.info("ğŸ“Š ê²½ìŸì‚¬ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„± (GG POKER í¬í•¨)...")
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        query = '''
            SELECT 
                cm.site_name,
                cm.monitoring_priority,
                cm.competitor_category,
                cm.market_tier,
                dt.players_online,
                dt.cash_players,
                dt.peak_24h,
                dt.seven_day_avg,
                dt.collection_date,
                dt.collection_time
            FROM competitor_metadata cm
            LEFT JOIN daily_traffic dt ON cm.site_name = dt.site_name
            WHERE dt.collection_date = (SELECT MAX(collection_date) FROM daily_traffic)
            ORDER BY 
                CASE cm.monitoring_priority 
                    WHEN 'CRITICAL' THEN 0
                    WHEN 'HIGH' THEN 1 
                    WHEN 'MEDIUM' THEN 2 
                    ELSE 3 END,
                dt.players_online DESC
        '''
        
        cursor.execute(query)
        results = cursor.fetchall()
        
        dashboard_data = {
            'generated_at': datetime.now().isoformat(),
            'data_date': datetime.now().strftime('%Y-%m-%d'),
            'gg_poker_data': [],
            'high_priority_competitors': [],
            'medium_priority_competitors': [],
            'low_priority_competitors': [],
            'market_summary': {
                'total_sites': len(results),
                'total_monitored_players': 0,
                'gg_poker_total_players': 0,
                'competitor_total_players': 0,
                'gg_poker_market_share': 0.0
            }
        }
        
        total_players = 0
        
        for row in results:
            site_data = {
                'site_name': row[0],
                'category': row[2],
                'tier': row[3],
                'players_online': row[4] or 0,
                'cash_players': row[5] or 0,
                'peak_24h': row[6] or 0,
                'seven_day_avg': row[7] or 0,
                'cash_ratio': round((row[5] / row[4]) * 100, 1) if row[4] and row[5] else 0,
                'last_updated': f"{row[8]} {row[9]}" if row[8] and row[9] else 'N/A'
            }
            
            total_players += site_data['players_online']
            
            priority = row[1]
            if priority == 'HIGH':
                dashboard_data['high_priority_competitors'].append(site_data)
            elif priority == 'MEDIUM':
                dashboard_data['medium_priority_competitors'].append(site_data)
            else:
                dashboard_data['low_priority_competitors'].append(site_data)
                
        dashboard_data['market_summary']['total_monitored_players'] = total_players
        
        conn.close()
        return dashboard_data
        
    def analyze_news_correlation_for_changes(self, detected_changes):
        """ë³€í™”ì™€ ë‰´ìŠ¤ ì—°ê´€ì„± ë¶„ì„"""
        logger.info("ğŸ“° ë³€í™”-ë‰´ìŠ¤ ì—°ê´€ì„± ë¶„ì„...")
        
        # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ë‰´ìŠ¤ ë°ì´í„°ë² ì´ìŠ¤ë‚˜ APIì—ì„œ ë°ì´í„° ê°€ì ¸ì˜´
        # ì—¬ê¸°ì„œëŠ” ìƒ˜í”Œ ë¶„ì„ ë¡œì§ë§Œ êµ¬í˜„
        
        correlations = []
        
        for change in detected_changes:
            # ë³€í™” ë°œìƒ ì „í›„ 3ì¼ê°„ì˜ ë‰´ìŠ¤ ê²€ìƒ‰
            correlation_analysis = {
                'change_id': change.get('change_id'),
                'site_name': change['site_name'],
                'metric': change['metric'],
                'change_percentage': change['change_percentage'],
                'magnitude': change['magnitude'],
                'potential_news_factors': [],
                'correlation_score': 0.0,
                'analysis_confidence': 'LOW'
            }
            
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì—¬ê¸°ì„œ ë‰´ìŠ¤ API í˜¸ì¶œ ë˜ëŠ” DB ì¿¼ë¦¬
            # ì˜ˆì‹œ: ì£¼ìš” í¬ì»¤ ë‰´ìŠ¤ í‚¤ì›Œë“œ ê²€ìƒ‰
            potential_factors = self.identify_potential_news_factors(change)
            correlation_analysis['potential_news_factors'] = potential_factors
            
            if potential_factors:
                correlation_analysis['correlation_score'] = len(potential_factors) * 0.2
                correlation_analysis['analysis_confidence'] = 'MEDIUM' if len(potential_factors) >= 2 else 'LOW'
                
            correlations.append(correlation_analysis)
            
        return correlations
        
    def identify_potential_news_factors(self, change):
        """ì ì¬ì  ë‰´ìŠ¤ ìš”ì¸ ì‹ë³„"""
        factors = []
        
        site_name = change['site_name']
        magnitude = change['magnitude']
        
        # ê¸‰ë³€ í¬ê¸°ì— ë”°ë¥¸ ê°€ëŠ¥í•œ ìš”ì¸ë“¤
        if magnitude in ['MAJOR', 'ANOMALY']:
            factors.extend([
                'Major tournament announcement',
                'Platform update or maintenance',
                'Promotional campaign launch',
                'Regulatory news',
                'Partnership announcement'
            ])
        elif magnitude == 'SIGNIFICANT':
            factors.extend([
                'Weekly tournament series',
                'Bonus promotion',
                'Software update',
                'Market news'
            ])
            
        # ì‚¬ì´íŠ¸ë³„ íŠ¹í™” ìš”ì¸
        if 'PokerStars' in site_name:
            factors.extend(['SCOOP/WCOOP event', 'EPT tournament', 'Sunday Million special'])
        elif 'GG' in site_name:
            factors.extend(['WSOP satellite', 'GG Masters series', 'Bounty tournament'])
        elif 'WPT' in site_name:
            factors.extend(['WPT500 series', 'World Poker Tour event'])
            
        return factors[:3]  # ìƒìœ„ 3ê°œ ìš”ì¸ë§Œ ë°˜í™˜
        
    def export_monitoring_report(self, date_range_days=7):
        """ëª¨ë‹ˆí„°ë§ ë¦¬í¬íŠ¸ ë‚´ë³´ë‚´ê¸°"""
        logger.info(f"ğŸ“‹ ëª¨ë‹ˆí„°ë§ ë¦¬í¬íŠ¸ ìƒì„± ({date_range_days}ì¼ê°„)")
        
        report = {
            'report_metadata': {
                'generated_at': datetime.now().isoformat(),
                'report_type': 'gg_poker_monitoring_report',
                'analysis_period_days': date_range_days,
                'focus': 'competitor_analysis_and_change_detection'
            },
            'executive_summary': {},
            'competitor_analysis': {},
            'change_detection_summary': {},
            'time_series_highlights': {},
            'recommendations': []
        }
        
        # ê²½ìŸì‚¬ ëŒ€ì‹œë³´ë“œ ë°ì´í„°
        competitor_data = self.get_competitor_dashboard_data()
        report['competitor_analysis'] = competitor_data
        
        # ìµœê·¼ ë³€í™” ê°ì§€ ìš”ì•½
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT 
                COUNT(*) as total_changes,
                COUNT(CASE WHEN change_magnitude = 'ANOMALY' THEN 1 END) as anomaly_count,
                COUNT(CASE WHEN change_magnitude = 'MAJOR' THEN 1 END) as major_count,
                COUNT(CASE WHEN change_magnitude = 'SIGNIFICANT' THEN 1 END) as significant_count
            FROM change_detection 
            WHERE detection_date >= date('now', '-' || ? || ' days')
        ''', (date_range_days,))
        
        change_summary = cursor.fetchone()
        if change_summary:
            report['change_detection_summary'] = {
                'total_changes': change_summary[0],
                'anomaly_changes': change_summary[1],
                'major_changes': change_summary[2],
                'significant_changes': change_summary[3],
                'analysis_period': f'{date_range_days} days'
            }
            
        # ê²½ì˜ì§„ ìš”ì•½
        report['executive_summary'] = {
            'monitored_competitors': competitor_data['market_summary']['total_competitors'],
            'total_competitor_players': competitor_data['market_summary']['total_monitored_players'],
            'high_priority_competitors': len(competitor_data['high_priority_competitors']),
            'significant_changes_detected': change_summary[0] if change_summary else 0,
            'data_quality': 'GOOD',
            'monitoring_status': 'ACTIVE'
        }
        
        # ê¶Œê³ ì‚¬í•­
        if change_summary and change_summary[1] > 0:  # ANOMALYê°€ ìˆëŠ” ê²½ìš°
            report['recommendations'].append({
                'priority': 'HIGH',
                'recommendation': f'{change_summary[1]}ê°œ ì´ìƒ ì§•í›„ ê°ì§€, ì¦‰ì‹œ ìƒì„¸ ë¶„ì„ í•„ìš”',
                'action': 'INVESTIGATE'
            })
            
        # ë¦¬í¬íŠ¸ ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'gg_poker_monitoring_report_{timestamp}.json'
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
            
        # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìš”ì•½ë„ ìƒì„±
        summary_filename = f'monitoring_summary_{timestamp}.txt'
        self.save_text_summary(report, summary_filename)
        
        conn.close()
        
        logger.info(f"ğŸ“Š ë¦¬í¬íŠ¸ ì €ì¥: {filename}")
        logger.info(f"ğŸ“„ ìš”ì•½ ì €ì¥: {summary_filename}")
        
        return filename, summary_filename
        
    def save_text_summary(self, report, filename):
        """í…ìŠ¤íŠ¸ ìš”ì•½ ì €ì¥"""
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("ğŸ¯ GG POKER ê²½ìŸì‚¬ ëª¨ë‹ˆí„°ë§ ë¦¬í¬íŠ¸ ìš”ì•½\n")
            f.write(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„')}\n")
            f.write("=" * 80 + "\n\n")
            
            # ê²½ì˜ì§„ ìš”ì•½
            summary = report['executive_summary']
            f.write("ğŸ“Š í•µì‹¬ ìš”ì•½\n")
            f.write("-" * 40 + "\n")
            f.write(f"ëª¨ë‹ˆí„°ë§ ê²½ìŸì‚¬: {summary['monitored_competitors']}ê°œ\n")
            f.write(f"ê²½ìŸì‚¬ ì´ í”Œë ˆì´ì–´: {summary['total_competitor_players']:,}ëª…\n")
            f.write(f"ê³ ìš°ì„ ìˆœìœ„ ê²½ìŸì‚¬: {summary['high_priority_competitors']}ê°œ\n")
            f.write(f"ê°ì§€ëœ ìœ ì˜ë¯¸í•œ ë³€í™”: {summary['significant_changes_detected']}ê±´\n\n")
            
            # ê³ ìš°ì„ ìˆœìœ„ ê²½ìŸì‚¬ í˜„í™©
            competitor_data = report['competitor_analysis']
            if competitor_data['high_priority_competitors']:
                f.write("ğŸ† ê³ ìš°ì„ ìˆœìœ„ ê²½ìŸì‚¬ í˜„í™©\n")
                f.write("-" * 40 + "\n")
                for comp in competitor_data['high_priority_competitors']:
                    f.write(f"{comp['site_name']}: {comp['players_online']:,}ëª… ")
                    f.write(f"(ìºì‹œ {comp['cash_ratio']}%, í”¼í¬ {comp['peak_24h']:,}ëª…)\n")
                f.write("\n")
                
            # ë³€í™” ê°ì§€ ìš”ì•½
            change_summary = report.get('change_detection_summary', {})
            if change_summary.get('total_changes', 0) > 0:
                f.write("ğŸš¨ ë³€í™” ê°ì§€ ìš”ì•½\n")
                f.write("-" * 40 + "\n")
                f.write(f"ì´ ë³€í™”: {change_summary['total_changes']}ê±´\n")
                f.write(f"ì´ìƒ ì§•í›„ (ANOMALY): {change_summary['anomaly_changes']}ê±´\n")
                f.write(f"ì£¼ìš” ë³€í™” (MAJOR): {change_summary['major_changes']}ê±´\n")
                f.write(f"ìœ ì˜ë¯¸í•œ ë³€í™” (SIGNIFICANT): {change_summary['significant_changes']}ê±´\n\n")
                
            # ê¶Œê³ ì‚¬í•­
            if report['recommendations']:
                f.write("ğŸ’¡ ê¶Œê³ ì‚¬í•­\n")
                f.write("-" * 40 + "\n")
                for rec in report['recommendations']:
                    f.write(f"[{rec['priority']}] {rec['recommendation']}\n")
                    f.write(f"ì¡°ì¹˜: {rec['action']}\n\n")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ GG POKER ì§ì›ìš© ë°ì´í„° ì¤‘ì‹¬ ëª¨ë‹ˆí„°ë§ í”Œë«í¼")
    print("=" * 70)
    print("ğŸ“Š 4ê°œ í•µì‹¬ ì§€í‘œ: Players Online, Cash Players, 24h Peak, 7-day Avg")
    print("ğŸ“ˆ ë‚ ì§œë³„ ì‹œê³„ì—´ ì°¨íŠ¸ ë¶„ì„")
    print("ğŸš¨ ê¸‰ë³€ ì‹œì  ìë™ ê°ì§€ + ë‰´ìŠ¤ ì—°ê´€ì„± ë¶„ì„")
    print("ğŸ† ì‹¤ì‹œê°„ ê²½ìŸì‚¬ ëª¨ë‹ˆí„°ë§")
    print("=" * 70)
    
    platform = GGPokerMonitoringPlatform()
    
    try:
        # 1. ê²½ìŸì‚¬ ëª¨ë‹ˆí„°ë§ ì„¤ì •
        print("\nğŸ”§ ê²½ìŸì‚¬ ëª¨ë‹ˆí„°ë§ ì„¤ì •...")
        platform.setup_competitor_monitoring()
        
        # 2. ìƒ˜í”Œ ë°ì´í„° ìˆ˜ì§‘ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” í¬ë¡¤ëŸ¬ì—ì„œ ë§¤ì¼ í˜¸ì¶œ)
        print("\nğŸ“ˆ ìƒ˜í”Œ ì¼ì¼ ë°ì´í„° ìˆ˜ì§‘...")
        sample_data = [
            # 4ê°œ í•µì‹¬ ì§€í‘œ ìˆ˜ì§‘
            {'site_name': 'PokerStars', 'players_online': 55540, 'cash_players': 1397, 'peak_24h': 62000, 'seven_day_avg': 58000},
            {'site_name': 'PokerStars Ontario', 'players_online': 55540, 'cash_players': 0, 'peak_24h': 60000, 'seven_day_avg': 55000},
            {'site_name': 'WPT Global', 'players_online': 2989, 'cash_players': 1596, 'peak_24h': 3500, 'seven_day_avg': 2800},
            {'site_name': '888poker', 'players_online': 1850, 'cash_players': 420, 'peak_24h': 2100, 'seven_day_avg': 1900},
            {'site_name': 'Chico Poker', 'players_online': 2253, 'cash_players': 671, 'peak_24h': 2500, 'seven_day_avg': 2100},
        ]
        
        collected = platform.collect_daily_data(sample_data)
        
        # 3. ê²½ìŸì‚¬ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„±
        print("\nğŸ“Š ê²½ìŸì‚¬ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„±...")
        dashboard_data = platform.get_competitor_dashboard_data()
        
        print(f"\nâœ… ëŒ€ì‹œë³´ë“œ ìš”ì•½:")
        print(f"ëª¨ë‹ˆí„°ë§ ì¤‘ì¸ ê²½ìŸì‚¬: {dashboard_data['market_summary']['total_competitors']}ê°œ")
        print(f"ì´ ê²½ìŸì‚¬ í”Œë ˆì´ì–´: {dashboard_data['market_summary']['total_monitored_players']:,}ëª…")
        
        print(f"\nğŸ† ê³ ìš°ì„ ìˆœìœ„ ê²½ìŸì‚¬:")
        for comp in dashboard_data['high_priority_competitors']:
            print(f"  â€¢ {comp['site_name']}: {comp['players_online']:,}ëª… (ìºì‹œ {comp['cash_ratio']}%)")
            
        # 4. ì‹œê³„ì—´ ì°¨íŠ¸ ë°ì´í„° ìƒì„± ìƒ˜í”Œ
        print("\nğŸ“ˆ ì‹œê³„ì—´ ì°¨íŠ¸ ë°ì´í„° ìƒì„± ìƒ˜í”Œ...")
        for site_name in ['PokerStars', 'WPT Global']:
            chart_data = platform.generate_time_series_chart_data(site_name, 7)
            if chart_data:
                analytics = chart_data['analytics']
                players_data = analytics.get('players_online', {})
                if 'no_data' not in players_data:
                    print(f"  {site_name}: í˜„ì¬ {players_data['current']:,}ëª…, íŠ¸ë Œë“œ {players_data['trend']}")
                    print(f"    ì°¨íŠ¸ ë°ì´í„° í¬ì¸íŠ¸: {chart_data['data_points']}ê°œ")
                    
        # 5. ëª¨ë‹ˆí„°ë§ ë¦¬í¬íŠ¸ ìƒì„±
        print("\nğŸ“‹ ëª¨ë‹ˆí„°ë§ ë¦¬í¬íŠ¸ ìƒì„±...")
        report_file, summary_file = platform.export_monitoring_report(7)
        
        print(f"\nğŸ¯ GG POKER ëª¨ë‹ˆí„°ë§ í”Œë«í¼ ì™„ì„±!")
        print(f"ğŸ“Š ìƒì„¸ ë¦¬í¬íŠ¸: {report_file}")
        print(f"ğŸ“„ ìš”ì•½ ë¦¬í¬íŠ¸: {summary_file}")
        print(f"")
        print(f"ğŸš€ í•µì‹¬ ê¸°ëŠ¥:")
        print(f"  âœ… 4ê°œ ì§€í‘œ ì¼ì¼ ìˆ˜ì§‘ (Players Online, Cash Players, 24h Peak, 7-day Avg)")
        print(f"  âœ… ë‚ ì§œë³„ ì‹œê³„ì—´ ì°¨íŠ¸ ë°ì´í„° ìƒì„±")
        print(f"  âœ… ê¸‰ë³€ ì‹œì  ìë™ ê°ì§€ (15%/25%/50% ì„ê³„ê°’)")
        print(f"  âœ… ë‰´ìŠ¤ ì—°ê´€ì„± ë¶„ì„ í”„ë ˆì„ì›Œí¬")
        print(f"  âœ… ê²½ìŸì‚¬ ìš°ì„ ìˆœìœ„ë³„ ëª¨ë‹ˆí„°ë§")
        print(f"  âœ… ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ ë°ì´í„°")
        print(f"  âœ… ìë™ ë¦¬í¬íŠ¸ ìƒì„±")
        
        return True
        
    except Exception as e:
        logger.error(f"í”Œë«í¼ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
        return False

if __name__ == "__main__":
    success = main()
    if success:
        print(f"\nğŸ‰ GG POKERìš© ë°ì´í„° ì¤‘ì‹¬ ëª¨ë‹ˆí„°ë§ í”Œë«í¼ ì™„ì„±!")
        print(f"ğŸ“ˆ ë§¤ì¼ 4ê°œ ì§€í‘œ ìˆ˜ì§‘ â†’ Xì¶• ë‚ ì§œ ì°¨íŠ¸ ë¶„ì„ â†’ ê¸‰ë³€ ê°ì§€ â†’ ë‰´ìŠ¤ ì—°ê´€ì„± â†’ ì •í™•í•œ ë°ì´í„° ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸")
    else:
        print(f"\nğŸ’€ í”Œë«í¼ ì„¤ì • ì‹¤íŒ¨ - ë¬¸ì œ í•´ê²° í•„ìš”")